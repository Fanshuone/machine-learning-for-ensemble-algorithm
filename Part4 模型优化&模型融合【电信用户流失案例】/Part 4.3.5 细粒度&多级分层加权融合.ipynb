{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d781943d-db12-4745-90fa-1b8685612d2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> 【Kaggle】Telco Customer Churn 电信用户流失预测案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082b43b-8599-485e-9f19-330be34cade2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bffda-da03-40b9-b318-b06592e72b73",
   "metadata": {},
   "source": [
    "## <font face=\"仿宋\">第四部分导读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bce3c-5383-4af0-b7c8-f4d0d33b524a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">在案例的第二、三部分中，我们详细介绍了关于特征工程的各项技术，特征工程技术按照大类来分可以分为数据预处理、特征衍生、特征筛选三部分，其中特征预处理的目的是为了将数据集整理、清洗到可以建模的程度，具体技术包括缺失值处理、异常值处理、数据重编码等，是建模之前必须对数据进行的处理和操作；而特征衍生和特征筛选则更像是一类优化手段，能够帮助模型突破当前数据集建模的效果上界。并且我们在第二部分完整详细的介绍机器学习可解释性模型的训练、优化和解释方法，也就是逻辑回归和决策树模型。并且此前我们也一直以这两种算法为主，来进行各个部分的模型测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc64a85-6dfe-439a-bc97-810e76b72467",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而第四部分，我们将开始介绍集成学习的训练和优化的实战技巧，尽管从可解释性角度来说，集成学习的可解释性并不如逻辑回归和决策树，但在大多数建模场景下，集成学习都将获得一个更好的预测结果，这也是目前效果优先的建模场景下最常使用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7dff9-3d0d-4b17-a0da-198f843544db",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">总的来说，本部分内容只有一个目标，那就是借助各类优化方法，抵达每个主流集成学习的效果上界。换而言之，本部分我们将围绕单模优化策略展开详细的探讨，涉及到的具体集成学习包括随机森林、XGBoost、LightGBM、和CatBoost等目前最主流的集成学习算法，而具体的优化策略则包括超参数优化器的使用、特征衍生和筛选方法的使用、单模型自融合方法的使用，这些优化方法也是截至目前，提升单模效果最前沿、最有效、同时也是最复杂的方法。其中有很多较为艰深的理论，也有很多是经验之谈，但无论如何，我们希望能够围绕当前数据集，让每个集成学习算法优化到极限。值得注意的是，在这个过程中，我们会将此前介绍的特征衍生和特征筛选视作是一种模型优化方法，衍生和筛选的效果，一律以模型的最终结果来进行评定。而围绕集成学习进行海量特征衍生和筛选，也才是特征衍生和筛选技术能发挥巨大价值的主战场。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa91604-5ba3-40e6-9343-d5bb28e61f30",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而在抵达了单模的极限后，我们就会进入到下一阶段，也就是模型融合阶段。需要知道的是，只有单模的效果到达了极限，进一步的多模型融合、甚至多层融合，才是有意义的，才是有效果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737201b-6d4f-4897-bbab-6d9e67e6dc26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a109b3f-cec0-4b61-8f7d-b459003a9e3d",
   "metadata": {},
   "source": [
    "# <center>Part 4.集成算法的训练与优化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb2efdab-fee2-45c1-9eec-0b4dda3257c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8289998-a353-45bf-be6f-eb46bdaba477",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后执行Part 1中的数据清洗相关工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a53a72f0-a169-46aa-878b-f22f32e30a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "tcc = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# 标注连续/离散字段\n",
    "# 离散字段\n",
    "category_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "                'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "                'PaymentMethod']\n",
    "\n",
    "# 连续字段\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    " \n",
    "# 标签\n",
    "target = 'Churn'\n",
    "\n",
    "# ID列\n",
    "ID_col = 'customerID'\n",
    "\n",
    "# 验证是否划分能完全\n",
    "assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]\n",
    "\n",
    "# 连续字段转化\n",
    "tcc['TotalCharges']= tcc['TotalCharges'].apply(lambda x: x if x!= ' ' else np.nan).astype(float)\n",
    "tcc['MonthlyCharges'] = tcc['MonthlyCharges'].astype(float)\n",
    "\n",
    "# 缺失值填补\n",
    "tcc['TotalCharges'] = tcc['TotalCharges'].fillna(0)\n",
    "\n",
    "# 标签值手动转化 \n",
    "tcc['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "tcc['Churn'].replace(to_replace='No',  value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87ba14ab-daea-4f05-8f1f-f4d4da68dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tcc.drop(columns=[ID_col, target]).copy()\n",
    "labels = tcc['Churn'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e829f45-bca3-43c9-ae3f-196a808410c5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时，创建自然编码后的数据集以及经过时序特征衍生的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebfb1c94-eb39-4d34-8a30-0de0af48498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train, test = train_test_split(tcc, random_state=22)\n",
    "\n",
    "X_train = train.drop(columns=[ID_col, target]).copy()\n",
    "X_test = test.drop(columns=[ID_col, target]).copy()\n",
    "\n",
    "y_train = train['Churn'].copy()\n",
    "y_test = test['Churn'].copy()\n",
    "\n",
    "X_train_seq = pd.DataFrame()\n",
    "X_test_seq = pd.DataFrame()\n",
    "\n",
    "# 年份衍生\n",
    "X_train_seq['tenure_year'] = ((72 - X_train['tenure']) // 12) + 2014\n",
    "X_test_seq['tenure_year'] = ((72 - X_test['tenure']) // 12) + 2014\n",
    "\n",
    "# 月份衍生\n",
    "X_train_seq['tenure_month'] = (72 - X_train['tenure']) % 12 + 1\n",
    "X_test_seq['tenure_month'] = (72 - X_test['tenure']) % 12 + 1\n",
    "\n",
    "# 季度衍生\n",
    "X_train_seq['tenure_quarter'] = ((X_train_seq['tenure_month']-1) // 3) + 1\n",
    "X_test_seq['tenure_quarter'] = ((X_test_seq['tenure_month']-1) // 3) + 1\n",
    "\n",
    "# 独热编码\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_train_seq)\n",
    "\n",
    "seq_new = list(X_train_seq.columns)\n",
    "\n",
    "# 创建带有列名称的独热编码之后的df\n",
    "X_train_seq = pd.DataFrame(enc.transform(X_train_seq).toarray(), \n",
    "                           columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "X_test_seq = pd.DataFrame(enc.transform(X_test_seq).toarray(), \n",
    "                          columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "# 调整index\n",
    "X_train_seq.index = X_train.index\n",
    "X_test_seq.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fe260bf-01e0-4c19-9a30-6062f6618c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(X_train[category_cols])\n",
    "\n",
    "X_train_OE = pd.DataFrame(ord_enc.transform(X_train[category_cols]), columns=category_cols)\n",
    "X_train_OE.index = X_train.index\n",
    "X_train_OE = pd.concat([X_train_OE, X_train[numeric_cols]], axis=1)\n",
    "\n",
    "X_test_OE = pd.DataFrame(ord_enc.transform(X_test[category_cols]), columns=category_cols)\n",
    "X_test_OE.index = X_test.index\n",
    "X_test_OE = pd.concat([X_test_OE, X_test[numeric_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d224b-d197-41bf-b440-aecfb80ef34f",
   "metadata": {},
   "source": [
    "然后是模型融合部分所需的第三方库、准备的数据以及训练好的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cbf1089-42bb-48ba-ac61-5906974dbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本节新增第三方库\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3777e790-ea1c-4abd-a08b-47714028a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifier_threshold(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimators, voting='hard', weights=None, thr=0.5):\n",
    "        self.estimators = estimators\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "        self.thr = thr\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        VC = VotingClassifier(estimators = self.estimators, \n",
    "                              voting = self.voting, \n",
    "                              weights = self.weights)\n",
    "        \n",
    "        VC.fit(X, y)\n",
    "        self.clf = VC\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        if self.voting == 'soft':\n",
    "            res_proba = self.clf.predict_proba(X)\n",
    "        else:\n",
    "            res_proba = None\n",
    "        return res_proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.voting == 'soft':\n",
    "            res = (self.clf.predict_proba(X)[:, 1] >= self.thr) * 1\n",
    "        else:\n",
    "            res = self.clf.predict(X)\n",
    "        return res\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        acc = accuracy_score(self.predict(X), y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34b46207-735a-4524-a9a9-419565e019a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化KFold评估器\n",
    "kf = KFold(n_splits=5, random_state=12, shuffle=True)\n",
    "\n",
    "# 重置训练集和测试集的index\n",
    "X_train_OE = X_train_OE.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "train_part_index_l = []\n",
    "eval_index_l = []\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train_OE, y_train):\n",
    "    train_part_index_l.append(train_part_index)\n",
    "    eval_index_l.append(eval_index)\n",
    "    \n",
    "# 训练集特征\n",
    "X_train1 = X_train_OE.loc[train_part_index_l[0]]\n",
    "X_train2 = X_train_OE.loc[train_part_index_l[1]]\n",
    "X_train3 = X_train_OE.loc[train_part_index_l[2]]\n",
    "X_train4 = X_train_OE.loc[train_part_index_l[3]]\n",
    "X_train5 = X_train_OE.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集特征\n",
    "X_eval1 = X_train_OE.loc[eval_index_l[0]]\n",
    "X_eval2 = X_train_OE.loc[eval_index_l[1]]\n",
    "X_eval3 = X_train_OE.loc[eval_index_l[2]]\n",
    "X_eval4 = X_train_OE.loc[eval_index_l[3]]\n",
    "X_eval5 = X_train_OE.loc[eval_index_l[4]]\n",
    "\n",
    "# 训练集标签\n",
    "y_train1 = y_train.loc[train_part_index_l[0]]\n",
    "y_train2 = y_train.loc[train_part_index_l[1]]\n",
    "y_train3 = y_train.loc[train_part_index_l[2]]\n",
    "y_train4 = y_train.loc[train_part_index_l[3]]\n",
    "y_train5 = y_train.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集标签\n",
    "y_eval1 = y_train.loc[eval_index_l[0]]\n",
    "y_eval2 = y_train.loc[eval_index_l[1]]\n",
    "y_eval3 = y_train.loc[eval_index_l[2]]\n",
    "y_eval4 = y_train.loc[eval_index_l[3]]\n",
    "y_eval5 = y_train.loc[eval_index_l[4]]\n",
    "\n",
    "train_set = [(X_train1, y_train1), \n",
    "             (X_train2, y_train2), \n",
    "             (X_train3, y_train3), \n",
    "             (X_train4, y_train4), \n",
    "             (X_train5, y_train5)]\n",
    "\n",
    "eval_set = [(X_eval1, y_eval1), \n",
    "            (X_eval2, y_eval2), \n",
    "            (X_eval3, y_eval3), \n",
    "            (X_eval4, y_eval4), \n",
    "            (X_eval5, y_eval5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64e2f134-cfdf-445a-a612-f5c44f04fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型组\n",
    "grid_RF_1 = load('grid_RF_1.joblib') \n",
    "grid_RF_2 = load('grid_RF_2.joblib') \n",
    "grid_RF_3 = load('grid_RF_3.joblib') \n",
    "grid_RF_4 = load('grid_RF_4.joblib') \n",
    "grid_RF_5 = load('grid_RF_5.joblib') \n",
    "\n",
    "RF_1 = grid_RF_1.best_estimator_\n",
    "RF_2 = grid_RF_2.best_estimator_\n",
    "RF_3 = grid_RF_3.best_estimator_\n",
    "RF_4 = grid_RF_4.best_estimator_\n",
    "RF_5 = grid_RF_5.best_estimator_\n",
    "\n",
    "RF_l = [RF_1, RF_2, RF_3, RF_4, RF_5]\n",
    "\n",
    "# 决策树模型组\n",
    "grid_tree_1 = load('grid_tree_1.joblib')\n",
    "grid_tree_2 = load('grid_tree_2.joblib')\n",
    "grid_tree_3 = load('grid_tree_3.joblib')\n",
    "grid_tree_4 = load('grid_tree_4.joblib')\n",
    "grid_tree_5 = load('grid_tree_5.joblib')\n",
    "\n",
    "tree_1 = grid_tree_1.best_estimator_\n",
    "tree_2 = grid_tree_2.best_estimator_\n",
    "tree_3 = grid_tree_3.best_estimator_\n",
    "tree_4 = grid_tree_4.best_estimator_\n",
    "tree_5 = grid_tree_5.best_estimator_\n",
    "\n",
    "tree_l = [tree_1, tree_2, tree_3, tree_4, tree_5]\n",
    "\n",
    "# 逻辑回归模型组\n",
    "grid_lr_1 = load('grid_lr_1.joblib')\n",
    "grid_lr_2 = load('grid_lr_2.joblib')\n",
    "grid_lr_3 = load('grid_lr_3.joblib')\n",
    "grid_lr_4 = load('grid_lr_4.joblib')\n",
    "grid_lr_5 = load('grid_lr_5.joblib')\n",
    "\n",
    "lr_1 = grid_lr_1.best_estimator_\n",
    "lr_2 = grid_lr_2.best_estimator_\n",
    "lr_3 = grid_lr_3.best_estimator_\n",
    "lr_4 = grid_lr_4.best_estimator_\n",
    "lr_5 = grid_lr_5.best_estimator_\n",
    "\n",
    "lr_l = [lr_1, lr_2, lr_3, lr_4, lr_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b24300a8-194b-4bfb-b5ed-a7ea7de58fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1_predict_proba_RF = pd.Series(RF_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_RF = pd.Series(RF_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_RF = pd.Series(RF_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_RF = pd.Series(RF_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_RF = pd.Series(RF_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_RF = pd.concat([eval1_predict_proba_RF, \n",
    "                                   eval2_predict_proba_RF, \n",
    "                                   eval3_predict_proba_RF, \n",
    "                                   eval4_predict_proba_RF, \n",
    "                                   eval5_predict_proba_RF]).sort_index()\n",
    "\n",
    "eval1_predict_proba_tree = pd.Series(tree_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_tree = pd.Series(tree_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_tree = pd.Series(tree_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_tree = pd.Series(tree_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_tree = pd.Series(tree_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_tree = pd.concat([eval1_predict_proba_tree, \n",
    "                                     eval2_predict_proba_tree, \n",
    "                                     eval3_predict_proba_tree, \n",
    "                                     eval4_predict_proba_tree, \n",
    "                                     eval5_predict_proba_tree]).sort_index()\n",
    "\n",
    "eval1_predict_proba_lr = pd.Series(lr_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_lr = pd.Series(lr_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_lr = pd.Series(lr_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_lr = pd.Series(lr_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_lr = pd.Series(lr_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_lr = pd.concat([eval1_predict_proba_lr, \n",
    "                                   eval2_predict_proba_lr, \n",
    "                                   eval3_predict_proba_lr, \n",
    "                                   eval4_predict_proba_lr, \n",
    "                                   eval5_predict_proba_lr]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6a2bad6-effb-46fc-9f0f-d78420fdefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_proba_RF = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF.append(RF_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_RF = np.array(test_predict_proba_RF)\n",
    "test_predict_proba_RF = test_predict_proba_RF.mean(0)\n",
    "\n",
    "test_predict_proba_tree = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree.append(tree_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_tree = np.array(test_predict_proba_tree)\n",
    "test_predict_proba_tree = test_predict_proba_tree.mean(0)\n",
    "\n",
    "test_predict_proba_lr = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr.append(lr_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_lr = np.array(test_predict_proba_lr)\n",
    "test_predict_proba_lr = test_predict_proba_lr.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451b201-be5e-4500-af54-2ec4ce788856",
   "metadata": {},
   "source": [
    "## <center>Ch.3 模型融合基础方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd9a3f-46f3-47ed-ab80-2b6c0fa6ff8d",
   "metadata": {},
   "source": [
    "## 六、交叉训练权重搜索策略评价与改进方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839df7a-67c1-4010-ae80-a7195875058a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;尽管上述流程已经能够稳定获得一个还不错的融合结果，但实际上这个加权软投票的过程，还是有进一步优化的可能性的。这里我们先对上述流程进行复盘，然后再进一步讨论后续优化的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ec3d6-43f3-496b-88a4-ce75448c16b2",
   "metadata": {},
   "source": [
    "### 1.交叉训练权重搜索方案评价"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792b7e8-a70b-4afa-9e76-86a9d4ac2374",
   "metadata": {},
   "source": [
    "- 搜索空间裁剪不再可行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d162ff-e6c1-4632-bd2b-69f0a3a6ed5e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，相信在熟悉了经验法+搜索空间裁剪过程之后，肯定有同学会觉得，上述流程是否也能通过裁剪搜索空间提高结果的泛化能力呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c94c55-1633-45b2-8f53-049a98389f99",
   "metadata": {},
   "source": [
    "&emsp;&emsp;答案是否定的。其根本原因就在于此时我们只剩下一个权重判别依据——验证集的准确率。在TPE强大的搜索能力下，我们不太可能手动“试”出一个比TPE更好的验证集准确率结果，而不能找到依据说明经验结果更好，就无法据此进一步裁剪搜索空间。需要知道的是，在交叉训练模型之前的经验法，实际上是通过训练集上的准确率判断一个更好的经验结果，而TPE搜索过程则以验证集平均准确率作为搜索依据，正是二者选取依据的不同，才创造了“训练集上准确率比验证集平均准确率更有效”的可能性，而有了这个可能性之后，以训练集准确率作为依据的经验法裁剪搜索空间才是有效的。而在交叉训练之后，训练集的准确率实际上就是验证集的准确率（因为验证集能“拼”出一个训练集）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8570d15-3577-4afc-b9f1-13157fc45d18",
   "metadata": {},
   "source": [
    "> 这里稍微转牛角尖的进行讨论下，哪怕在交叉训练的情况下，每个模型仍然还是有那80%的数据是用于训练的，能否每个模型在自身训练数据集上进行预测然后求平均，最终算出训练集的准确率呢？可以，但没必要。在训练集、验证集信息完全隔离的情况下，不相信验证集的结果而相信训练集的结果，并不是一个有利于提升模型泛化能力的方向。<center><img src=\"https://s2.loli.net/2022/05/24/6rvmpInWyJcwiF1.png\" alt=\"image-20220524210942174\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a010d68-8f4f-4ba6-b956-737d31fc2711",
   "metadata": {},
   "source": [
    "- 基于交叉训练的权重搜索效果极限"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d675c1-cc2c-4c19-80fb-7c9836080ecd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但是，如果是站在“上帝视角”，直接带入测试集进行权重搜索，其实是能获得一个更好的权重搜索结果的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51d8da3c-1999-4f02-ba53-7aec262a19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'thr': hp.uniform(\"thr\", 0.4, 0.6), \n",
    "                'weight1': hp.uniform(\"weight1\",0,1),\n",
    "                'weight2': hp.uniform(\"weight2\",0,1),\n",
    "                'weight3': hp.uniform(\"weight3\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a976feca-3b26-406c-bbc0-7b2126e6615c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params):\n",
    "    thr = params['thr']\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    \n",
    "    weights_sum = weight1 + weight2 + weight3\n",
    "\n",
    "    predict_probo_weight = (test_predict_proba_lr * weight1 + \n",
    "                            test_predict_proba_tree * weight2 + \n",
    "                            test_predict_proba_RF * weight3) / weights_sum\n",
    "\n",
    "    res_weight = (predict_probo_weight >= thr) * 1\n",
    "\n",
    "    eval_score = accuracy_score(res_weight, y_test)\n",
    "    \n",
    "    return -eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95083498-f900-4da5-b568-e56a1e52b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate = np.random.default_rng(17))    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce31f898-60ed-44b2-88a3-a62a571a7cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5000/5000 [01:40<00:00, 49.98trial/s, best loss: -0.8006814310051107]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8282eef4-b2cb-4891-8192-fed16166220d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thr': 0.46824033457779035,\n",
       " 'weight1': 9.187915210945308e-05,\n",
       " 'weight2': 0.0003131011546776627,\n",
       " 'weight3': 0.7000333202399632}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82708b39-efee-4b78-aadc-ac7bf760dda1",
   "metadata": {},
   "source": [
    "能看出，如果带入上述权重，实际上是能在测试集上达到80%准确率的。因此如果我们据此缩小搜索空间，或许能获得一个更好的结果的。但是在实际建模过程中，如果测试集完全未知，我们并没有任何理由去裁剪搜索空间（裁剪之后会让唯一的评估指标——验证集准确率下降）。因此，即使该流程的效果极限是测试集80%的准确率，上述流程最佳实践结果也只能达到79.72%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17296c-b6b5-4caf-ab81-a2cce43f677c",
   "metadata": {},
   "source": [
    "### 2.基于交叉训练的权重搜索优化方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674dd60-6676-4100-8a75-38d045479ff2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们继续讨论交叉训练权重搜索策略的改进方案。这里需要注意，其实交叉训练权重搜索策略其实已经属于加权融合过程中效果非常好且结果非常稳定的一种策略，该策略能够很好的利用TPE优化器的性能，同时也能很好的抑制过拟合问题，是模型融合过程中必须要尝试的一类方法，甚至很多时候会比后续要讨论的Stacking、Blending等方法效果更好。但这里我们仍然要探讨进一步优化方案，不仅仅是为大家的模型融合“武器库”提供更多“枪支弹药”，更多的是帮助大家“打开脑洞”。就像本节开篇所言，模型融合的实际应用是非常灵活的，生搬硬套可能不足以和别人拉开差距。而如何才能做到因地制宜、活学活用，很多时候就需要更多思想上的碰撞。这里介绍的优化方法都是交叉训练权重搜索方法的升级版，但就本数据集而言不一定能够有更好的结果，但一方面这些方法背后的思想值得借鉴，其二在后续案例和其他数据集上，这些方法或许能获得一个更好结果；基于此，我们还是非常有必要详细介绍这些方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceef36c-e4e8-4a09-b3fe-d6c1e7fe317a",
   "metadata": {},
   "source": [
    "#### 2.1 细粒度权重搜索方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5616ef9-f83b-4239-9bf4-ca9af9c566a3",
   "metadata": {},
   "source": [
    "- 方案介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892a65f-18a2-4a7c-aa8e-899e25b071d4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总的来说，上述流程还可以有至少两个优化的方向，首先，我们完全可以把每个模型看成是独立的模型，独立的在验证集上进行权重搜索，独立的参与测试集的预测，由此获得更灵活的验证集结果表现，借此提升融合效果。如下图所示：例如lr_1、tree_1和RF_1模型，都是在Part 1-4数据集上进行的训练模型，这些三个模型就可以围绕Part 5数据集进行加权融合，分别训练得到三个模型不同的权重，类似的其他模型也可以按照此方法进行训练，最终得到每个模型单独的权重，然后在预测的过程中，每个模型单独对测试集进行预测，然后通过加权的方式输出最终预测结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77ae09-bcb4-4855-b5d4-366f2afcdc16",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2022/05/24/8mIKPwJOGoZRcL2.png\" alt=\"image-20220524232358976\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d7db1-c984-4c06-9c7f-382cf6b7cc14",
   "metadata": {},
   "source": [
    "- 方案评价"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d104f-87d6-440e-9177-d2750b26e0f4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;很明显，该方案通过给每个模型单独分配权重，能够大幅提升测试集上的效果上限，但如此规模的（连续变量）超参数对TPE的搜索过程会造成一定的压力，尽管上限较高，同时验证集仍然可信，但TPE却不一定能搜索得到哪怕是验证集上的最优解，因此实际效果并不一定比原始策略更好。但是可以判断的是，如果能够妥善修改交叉验证折数（如降低折数），或者数据一致性较好，则该方法能取得一个不错的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0a726-a618-4735-b120-35fd226852f2",
   "metadata": {},
   "source": [
    "#### 2.2 多级分层加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e385bf-5eff-43f9-9829-bb51fe4fc909",
   "metadata": {},
   "source": [
    "- 方案介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb0da9-793b-406f-9735-4b2fee750a56",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而第二种方法则是分层融合，我们可以考虑将每一组模型简单看成是一个数据集上训练得到的5个模型（就像第一小节中在完整数据集上训练的逻辑回归、决策树和随机森林），然后组内进行加权融合，通过对训练集的预测得到每个模型的权重（也就是VotingClassifier+cross_val_score过程），例如RF_l组内加权融合过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9a387-ca0b-4051-8ea9-5ee38c14d2c0",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/05/25/HErYdURczIotaZW.png\" alt=\"image-20220525010307470\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f1077-2b04-462c-8668-3901c984f937",
   "metadata": {},
   "source": [
    "然后，当每一组内都分别训练得到一组权重，并且分别得到了3个不同的训练集上的加权预测结果后，再参考上一小节的内容，计算组间权重："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb67f9e-1ab8-460d-899c-b7a4221013f9",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2022/05/25/lpJdSjZtenRYP7C.png\" alt=\"image-20220525010429071\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc81401-27c7-4c72-afaf-ad35eddf1c50",
   "metadata": {},
   "source": [
    "能够发现，这其实是一种分层级的加权融合过程，我们可以将其命名为多级分层加权融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088e366-ab3b-48ee-999c-b76f95927308",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，在实际围绕测试集的预测过程中，也是先进行每个模型的预测，然后再根据一级权重得到每一组的预测结果，然后再根据组间权重算出最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6119a1e-2e4b-4aee-b162-c45dc1b6a387",
   "metadata": {},
   "source": [
    "- 方案评价"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d797fca-e05f-457d-9a6c-4a10b2eb0118",
   "metadata": {},
   "source": [
    "&emsp;&emsp;细心的同学一定发现，其实多级分层融合最终的目的也是为了让每个评估器拥有独立的权重，这点和细粒度权重搜索方案类似，而所不同的是，多级分层融合的组内融合过程会泄露一部分验证集信息（同一个模型在全部数据上进行交叉验证），而换来的却是每一次搜索的高效。换而言之，两种方案的对比就是，细粒度搜索融合效果上限更高、但不容易达到，瓶颈在于超参数优化器，而多级分层搜索上限较低，但更容易达到。可以说两种方法各有优劣。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a29914-b481-4468-ab0a-76e6a3fe0217",
   "metadata": {},
   "source": [
    "> 此外，还可以考虑的优化方向则是从模型训练角度入手，把模型的超参数和权重超参数放在一个机器学习流中进行联合调参。不过这类方法的实践需要更多的基础知识——即模型多样性对融合效果的影响。这部分内容我们将在下个阶段进行探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a412e5-7314-4b23-8936-11a094e5eb05",
   "metadata": {},
   "source": [
    "## 七、细粒度加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb4aee-2608-425e-b3cc-75fad0b9e865",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来让我们开始尝试多级加权融合的过程。该方法同样并不深奥，但较为繁琐，主要比较繁琐的环节就是众多权重参数的设置。我们可以通过如下方式一次性搜索全部参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b31bfc67-81f9-495b-b854-eb3d8f495f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'thr': hp.uniform(\"thr\", 0.4, 0.6), \n",
    "                'weight_lr1': hp.uniform(\"weight_lr1\",0,1),\n",
    "                'weight_lr2': hp.uniform(\"weight_lr2\",0,1),\n",
    "                'weight_lr3': hp.uniform(\"weight_lr3\",0,1), \n",
    "                'weight_lr4': hp.uniform(\"weight_lr4\",0,1), \n",
    "                'weight_lr5': hp.uniform(\"weight_lr5\",0,1), \n",
    "                'weight_tree1': hp.uniform(\"weight_tree1\",0,1),\n",
    "                'weight_tree2': hp.uniform(\"weight_tree2\",0,1),\n",
    "                'weight_tree3': hp.uniform(\"weight_tree3\",0,1), \n",
    "                'weight_tree4': hp.uniform(\"weight_tree4\",0,1), \n",
    "                'weight_tree5': hp.uniform(\"weight_tree5\",0,1), \n",
    "                'weight_RF1': hp.uniform(\"weight_RF1\",0,1),\n",
    "                'weight_RF2': hp.uniform(\"weight_RF2\",0,1),\n",
    "                'weight_RF3': hp.uniform(\"weight_RF3\",0,1), \n",
    "                'weight_RF4': hp.uniform(\"weight_RF4\",0,1), \n",
    "                'weight_RF5': hp.uniform(\"weight_RF5\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f923b61-e6fc-48a2-bb93-eac42000d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params):\n",
    "    thr = params['thr']\n",
    "    weight_lr1 = params['weight_lr1']\n",
    "    weight_lr2 = params['weight_lr2']\n",
    "    weight_lr3 = params['weight_lr3']\n",
    "    weight_lr4 = params['weight_lr4']\n",
    "    weight_lr5 = params['weight_lr5']\n",
    "    \n",
    "    weight_tree1 = params['weight_tree1']\n",
    "    weight_tree2 = params['weight_tree2']\n",
    "    weight_tree3 = params['weight_tree3']\n",
    "    weight_tree4 = params['weight_tree4']\n",
    "    weight_tree5 = params['weight_tree5']\n",
    "    \n",
    "    weight_RF1 = params['weight_RF1']\n",
    "    weight_RF2 = params['weight_RF2']\n",
    "    weight_RF3 = params['weight_RF3']\n",
    "    weight_RF4 = params['weight_RF4']\n",
    "    weight_RF5 = params['weight_RF5']\n",
    "    \n",
    "    eval1_predict_proba_weight = (pd.Series(lr_1.predict_proba(X_eval1)[:, 1], index=X_eval1.index) * weight_lr1 + \n",
    "                                  pd.Series(tree_1.predict_proba(X_eval1)[:, 1], index=X_eval1.index) * weight_tree1 + \n",
    "                                  pd.Series(RF_1.predict_proba(X_eval1)[:, 1], index=X_eval1.index) * weight_RF1) / (weight_lr1 + weight_tree1 + weight_RF1)\n",
    "\n",
    "    eval2_predict_proba_weight = (pd.Series(lr_2.predict_proba(X_eval2)[:, 1], index=X_eval2.index) * weight_lr2 + \n",
    "                                  pd.Series(tree_2.predict_proba(X_eval2)[:, 1], index=X_eval2.index) * weight_tree2 + \n",
    "                                  pd.Series(RF_2.predict_proba(X_eval2)[:, 1], index=X_eval2.index) * weight_RF2) / (weight_lr2 + weight_tree2 + weight_RF2)    \n",
    "    \n",
    "    eval3_predict_proba_weight = (pd.Series(lr_3.predict_proba(X_eval3)[:, 1], index=X_eval3.index) * weight_lr3 + \n",
    "                                  pd.Series(tree_3.predict_proba(X_eval3)[:, 1], index=X_eval3.index) * weight_tree3 + \n",
    "                                  pd.Series(RF_3.predict_proba(X_eval3)[:, 1], index=X_eval3.index) * weight_RF3) / (weight_lr3 + weight_tree3 + weight_RF3)    \n",
    "    \n",
    "    eval4_predict_proba_weight = (pd.Series(lr_4.predict_proba(X_eval4)[:, 1], index=X_eval4.index) * weight_lr4 + \n",
    "                                  pd.Series(tree_4.predict_proba(X_eval4)[:, 1], index=X_eval4.index) * weight_tree4 + \n",
    "                                  pd.Series(RF_4.predict_proba(X_eval4)[:, 1], index=X_eval4.index) * weight_RF4) / (weight_lr4 + weight_tree4 + weight_RF4)    \n",
    "    \n",
    "    eval5_predict_proba_weight = (pd.Series(lr_5.predict_proba(X_eval5)[:, 1], index=X_eval5.index) * weight_lr5 + \n",
    "                                  pd.Series(tree_5.predict_proba(X_eval5)[:, 1], index=X_eval5.index) * weight_tree5 + \n",
    "                                  pd.Series(RF_5.predict_proba(X_eval5)[:, 1], index=X_eval5.index) * weight_RF5) / (weight_lr5 + weight_tree5 + weight_RF5)        \n",
    "    \n",
    "    eval_predict_proba_weight = pd.concat([eval1_predict_proba_weight,\n",
    "                                           eval2_predict_proba_weight, \n",
    "                                           eval3_predict_proba_weight, \n",
    "                                           eval4_predict_proba_weight, \n",
    "                                           eval5_predict_proba_weight]).sort_index()\n",
    "    \n",
    "    eval_predict = (eval_predict_proba_weight >= thr) * 1\n",
    "    \n",
    "    eval_acc = accuracy_score(eval_predict, y_train)\n",
    "    \n",
    "    return -eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b523fead-643b-41b3-9570-2d54c149674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate = np.random.default_rng(2))    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5f3d90a-bc1a-403a-a850-0a5aaf03d75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5000/5000 [10:42<00:00,  7.78trial/s, best loss: -0.8241196516471033]\n"
     ]
    }
   ],
   "source": [
    "best_params = param_hyperopt_weight(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3fec3bc-2a78-4dee-baca-3ece4abcf758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muti_weight_test_acc(params):\n",
    "    thr = params['thr']\n",
    "    weight_lr1 = params['weight_lr1']\n",
    "    weight_lr2 = params['weight_lr2']\n",
    "    weight_lr3 = params['weight_lr3']\n",
    "    weight_lr4 = params['weight_lr4']\n",
    "    weight_lr5 = params['weight_lr5']\n",
    "    \n",
    "    weight_lr_l = np.array([weight_lr1, weight_lr2, weight_lr3, weight_lr4, weight_lr5])\n",
    "    weight_lr_sum = weight_lr_l.sum()\n",
    "    \n",
    "    weight_tree1 = params['weight_tree1']\n",
    "    weight_tree2 = params['weight_tree2']\n",
    "    weight_tree3 = params['weight_tree3']\n",
    "    weight_tree4 = params['weight_tree4']\n",
    "    weight_tree5 = params['weight_tree5']\n",
    "    \n",
    "    weight_tree_l = np.array([weight_tree1, weight_tree2, weight_tree3, weight_tree4, weight_tree5])\n",
    "    weight_tree_sum = weight_tree_l.sum()\n",
    "    \n",
    "    weight_RF1 = params['weight_RF1']\n",
    "    weight_RF2 = params['weight_RF2']\n",
    "    weight_RF3 = params['weight_RF3']\n",
    "    weight_RF4 = params['weight_RF4']\n",
    "    weight_RF5 = params['weight_RF5']\n",
    "    \n",
    "    weight_RF_l = np.array([weight_RF1, weight_RF2, weight_RF3, weight_RF4, weight_RF5])\n",
    "    weight_RF_sum = weight_RF_l.sum()\n",
    "    \n",
    "    test_predict_proba = (lr_1.predict_proba(X_test_OE)[:, 1] * weight_lr1 + \n",
    "                          lr_2.predict_proba(X_test_OE)[:, 1] * weight_lr2 + \n",
    "                          lr_3.predict_proba(X_test_OE)[:, 1] * weight_lr3 + \n",
    "                          lr_4.predict_proba(X_test_OE)[:, 1] * weight_lr4 + \n",
    "                          lr_5.predict_proba(X_test_OE)[:, 1] * weight_lr5 + \n",
    "                          tree_1.predict_proba(X_test_OE)[:, 1] * weight_tree1 + \n",
    "                          tree_2.predict_proba(X_test_OE)[:, 1] * weight_tree2 + \n",
    "                          tree_3.predict_proba(X_test_OE)[:, 1] * weight_tree3 + \n",
    "                          tree_4.predict_proba(X_test_OE)[:, 1] * weight_tree4 + \n",
    "                          tree_5.predict_proba(X_test_OE)[:, 1] * weight_tree5 + \n",
    "                          RF_1.predict_proba(X_test_OE)[:, 1] * weight_RF1 + \n",
    "                          RF_2.predict_proba(X_test_OE)[:, 1] * weight_RF2 + \n",
    "                          RF_3.predict_proba(X_test_OE)[:, 1] * weight_RF3 + \n",
    "                          RF_4.predict_proba(X_test_OE)[:, 1] * weight_RF4 + \n",
    "                          RF_5.predict_proba(X_test_OE)[:, 1] * weight_RF5) / (weight_lr_sum + \n",
    "                                                                              weight_tree_sum + \n",
    "                                                                              weight_RF_sum)\n",
    "                          \n",
    "                \n",
    "    \n",
    "    test_predict = (test_predict_proba >= thr) * 1\n",
    "    \n",
    "    test_acc = accuracy_score(test_predict, y_test)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f356ecab-45af-4ac9-b35a-707080be2071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7915956842703009"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muti_weight_test_acc(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e50063-c69b-4066-9958-a4503edbbebe",
   "metadata": {},
   "source": [
    "能够发现，当我们迭代5000次时，测试集准确率为0.79159，接下来我们可以继续尝试增加迭代次数，以提升搜索精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c09ba-9cc5-4a64-ac4b-07f3adccb694",
   "metadata": {},
   "source": [
    "## 八、多级分层加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8002a1-c229-4595-a50a-7f8ea9e98db1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来继续第二种优化思路，即多级分层加权融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53852ba-aee6-4a91-a7d0-382872bf00f1",
   "metadata": {},
   "source": [
    "### 1.组内加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef563d-8b44-4cb9-a89a-1cdbb85d5022",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是组内的加权融合过程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a73132-fd1a-4e3b-a026-0d973c703a7a",
   "metadata": {},
   "source": [
    "- 随机森林模型组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b46a150d-0f2c-47dc-8322-e279ab7d1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_RF = [('RF_1',RF_1), ('RF_2',RF_2), ('RF_3',RF_3), ('RF_4',RF_4), ('RF_5',RF_5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "40fce8d7-3a86-4f4f-851f-da328eb65f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'thr': hp.uniform(\"thr\", 0.4, 0.6), \n",
    "                'weight1': hp.uniform(\"weight1\",0,1),\n",
    "                'weight2': hp.uniform(\"weight2\",0,1),\n",
    "                'weight3': hp.uniform(\"weight3\",0,1), \n",
    "                'weight4': hp.uniform(\"weight4\",0,1), \n",
    "                'weight5': hp.uniform(\"weight5\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a93ed05b-c09c-45c5-b9f0-23b72d331856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params):\n",
    "    thr = params['thr']\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    weight4 = params['weight4']\n",
    "    weight5 = params['weight5']\n",
    "    \n",
    "    weights = [weight1, weight2, weight3, weight4, weight5]\n",
    "    \n",
    "    # 创建带阈值的平均法评估器\n",
    "    VC_weight_search = VotingClassifier_threshold(estimators=estimators_RF, \n",
    "                                                  weights=weights,\n",
    "                                                  voting='soft', \n",
    "                                                  thr=thr)\n",
    "\n",
    "    # 输出验证集上的平均得分\n",
    "    val_score = cross_val_score(VC_weight_search, \n",
    "                                X_train_OE, \n",
    "                                y_train, \n",
    "                                scoring='accuracy', \n",
    "                                n_jobs=15,\n",
    "                                cv=5).mean()\n",
    "    \n",
    "    return -val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2bda8f70-618d-47ca-887d-b81705e1f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate = np.random.default_rng(2))    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c0132ec8-687b-4b5c-bffc-597a7f37f775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 500/500 [05:35<00:00,  1.49trial/s, best loss: -0.8084048264097934]\n"
     ]
    }
   ],
   "source": [
    "best_params = param_hyperopt_weight(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d4dd1-bc64-44a0-8aab-51b0ac9643c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2df408eb-939b-4aed-b9b9-5c5e917cf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_extract(best_params):\n",
    "    thr = best_params['thr']\n",
    "    weight1 = best_params['weight1']\n",
    "    weight2 = best_params['weight2']\n",
    "    weight3 = best_params['weight3']\n",
    "    weight4 = best_params['weight4']\n",
    "    weight5 = best_params['weight5']\n",
    "\n",
    "    weights_sum = (weight1 + weight2 + weight3 + weight4 + weight5)\n",
    "\n",
    "    weight1 = weight1 / weights_sum\n",
    "    weight2 = weight2 / weights_sum\n",
    "    weight3 = weight3 / weights_sum\n",
    "    weight4 = weight4 / weights_sum\n",
    "    weight5 = weight5 / weights_sum\n",
    "\n",
    "    weights = [weight1, weight2, weight3, weight4, weight5]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "885093cb-a082-45d1-af34-e049a707f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_weights = weights_extract(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe8d05-5997-4eef-a90c-e5ab10117ab1",
   "metadata": {},
   "source": [
    "然后输出验证集（训练集）上加权融合预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "761514d1-82ac-4630-a5f1-c54624de9fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03881102, 0.54438575, 0.12338471, ..., 0.5642814 , 0.03396568,\n",
       "       0.01746549])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_predict_proba_RF = 0\n",
    "\n",
    "for i in range(5):\n",
    "    eval_predict_proba_RF += (RF_l[i].predict_proba(X_train_OE)[:, 1]) * RF_weights[i]\n",
    "\n",
    "eval_predict_proba_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a262b0-0ec7-4bda-ac0e-673b0fe3f743",
   "metadata": {},
   "source": [
    "以及测试集上预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "86336474-9903-4b76-8b14-207fb2580794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03111997, 0.30563842, 0.01820175, ..., 0.14348214, 0.53063581,\n",
       "       0.1075506 ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_proba_RF = 0\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF += (RF_l[i].predict_proba(X_test_OE)[:, 1]) * RF_weights[i]\n",
    "\n",
    "test_predict_proba_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cb391-34ce-49d5-886f-6d073d3e659a",
   "metadata": {},
   "source": [
    "- 逻辑回归模型组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f74ac30e-1548-4a2e-b05f-4c154d4f4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_lr = [('lr_1',lr_1), ('lr_2',lr_2), ('lr_3',lr_3), ('lr_4',lr_4), ('lr_5',lr_5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e5e31de8-e9e9-464f-ae61-b6eff0c44910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'thr': hp.uniform(\"thr\", 0.4, 0.6), \n",
    "                'weight1': hp.uniform(\"weight1\",0,1),\n",
    "                'weight2': hp.uniform(\"weight2\",0,1),\n",
    "                'weight3': hp.uniform(\"weight3\",0,1), \n",
    "                'weight4': hp.uniform(\"weight4\",0,1), \n",
    "                'weight5': hp.uniform(\"weight5\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "784e75d5-afb6-4140-8ec9-e445c6f009d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params):\n",
    "    thr = params['thr']\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    weight4 = params['weight4']\n",
    "    weight5 = params['weight5']\n",
    "    \n",
    "    weights = [weight1, weight2, weight3, weight4, weight5]\n",
    "    \n",
    "    # 创建带阈值的平均法评估器\n",
    "    VC_weight_search = VotingClassifier_threshold(estimators=estimators_lr, \n",
    "                                                  weights=weights,\n",
    "                                                  voting='soft', \n",
    "                                                  thr=thr)\n",
    "\n",
    "    # 输出验证集上的平均得分\n",
    "    val_score = cross_val_score(VC_weight_search, \n",
    "                                X_train_OE, \n",
    "                                y_train, \n",
    "                                scoring='accuracy', \n",
    "                                n_jobs=15,\n",
    "                                cv=5).mean()\n",
    "    \n",
    "    return -val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d4865b62-86f3-4357-bf98-a647004fd0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate = np.random.default_rng(17))    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "06c0bb71-5813-4551-b5f4-0f46f8741687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 300/300 [01:44<00:00,  2.87trial/s, best loss: -0.8114340543562397]\n"
     ]
    }
   ],
   "source": [
    "best_params = param_hyperopt_weight(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8fbf489c-128b-4cd6-b487-b2684657c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_weights = weights_extract(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250874fb-e427-4215-a796-41231e5c058b",
   "metadata": {},
   "source": [
    "然后输出验证集（训练集）上加权融合预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ce6158ee-accf-497a-a5a1-732be45a345e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01318283, 0.55771798, 0.14512801, ..., 0.68283328, 0.05042271,\n",
       "       0.00420453])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_predict_proba_lr = 0\n",
    "\n",
    "for i in range(5):\n",
    "    eval_predict_proba_lr += (lr_l[i].predict_proba(X_train_OE)[:, 1]) * lr_weights[i]\n",
    "\n",
    "eval_predict_proba_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc424e1-9de3-445c-81e3-ccf9a9620d69",
   "metadata": {},
   "source": [
    "以及测试集上预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3fe68274-1cd7-433b-aefb-9fa51ed580bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04505546, 0.23169159, 0.0052967 , ..., 0.12388625, 0.50385759,\n",
       "       0.0633811 ])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_proba_lr = 0\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr += (lr_l[i].predict_proba(X_test_OE)[:, 1]) * lr_weights[i]\n",
    "\n",
    "test_predict_proba_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fab9b-248d-415a-96af-e558b4ed75e5",
   "metadata": {},
   "source": [
    "- 决策树模型组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5cda9eb8-fbc4-4dae-b119-80656144b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_tree = [('tree_1',tree_1), ('tree_2',tree_2), ('tree_3',tree_3), ('tree_4',tree_4), ('tree_5',tree_5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dfa2f8e5-c908-4119-87d8-c4b7cbe8b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'thr': hp.uniform(\"thr\", 0.4, 0.6), \n",
    "                'weight1': hp.uniform(\"weight1\",0,1),\n",
    "                'weight2': hp.uniform(\"weight2\",0,1),\n",
    "                'weight3': hp.uniform(\"weight3\",0,1), \n",
    "                'weight4': hp.uniform(\"weight4\",0,1), \n",
    "                'weight5': hp.uniform(\"weight5\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "719948fd-2f0d-49f7-9e6f-6656361d26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params):\n",
    "    thr = params['thr']\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    weight4 = params['weight4']\n",
    "    weight5 = params['weight5']\n",
    "    \n",
    "    weights = [weight1, weight2, weight3, weight4, weight5]\n",
    "    \n",
    "    # 创建带阈值的平均法评估器\n",
    "    VC_weight_search = VotingClassifier_threshold(estimators=estimators_tree, \n",
    "                                                  weights=weights,\n",
    "                                                  voting='soft', \n",
    "                                                  thr=thr)\n",
    "\n",
    "    # 输出验证集上的平均得分\n",
    "    val_score = cross_val_score(VC_weight_search, \n",
    "                                X_train_OE, \n",
    "                                y_train, \n",
    "                                scoring='accuracy', \n",
    "                                n_jobs=15,\n",
    "                                cv=5).mean()\n",
    "    \n",
    "    return -val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6fff3264-f8c2-43fa-90eb-ad1af395f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate = np.random.default_rng(17))    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "db22c51b-b040-40ca-9c8c-5b029cc565b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 300/300 [00:13<00:00, 22.13trial/s, best loss: -0.797234167598406]\n"
     ]
    }
   ],
   "source": [
    "best_params = param_hyperopt_weight(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2badf4df-654e-4b68-bd79-0655420e1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_weights = weights_extract(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d3c60-6b36-4c6f-9e67-1112512522bb",
   "metadata": {},
   "source": [
    "然后输出验证集（训练集）上加权融合预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "db07439a-c0f2-41df-bc70-aaf413e8633e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04365352, 0.74609312, 0.16049057, ..., 0.39059284, 0.04624722,\n",
       "       0.04365352])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_predict_proba_tree = 0\n",
    "\n",
    "for i in range(5):\n",
    "    eval_predict_proba_tree += (tree_l[i].predict_proba(X_train_OE)[:, 1]) * tree_weights[i]\n",
    "\n",
    "eval_predict_proba_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b74052-1fa4-4010-bd12-ec74aef70045",
   "metadata": {},
   "source": [
    "以及测试集上预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fd8f2185-74eb-4138-95ea-a10376327d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04365352, 0.1870317 , 0.04365352, ..., 0.1870317 , 0.45025595,\n",
       "       0.17253686])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_proba_tree = 0\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree += (tree_l[i].predict_proba(X_test_OE)[:, 1]) * tree_weights[i]\n",
    "\n",
    "test_predict_proba_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e96fba-0531-4eb2-89f8-8c8af5e121ba",
   "metadata": {},
   "source": [
    "### 2.组间融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997a383-f5b0-4573-bcfc-4b3d24be7a96",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行执行组间融合，该部分操作和上一小节的流程非常类似，代码可以直接复用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "25116c07-32a7-4564-8ae6-6e4b94e6de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'thr': hp.uniform(\"thr\", 0.4, 0.6), \n",
    "                'weight1': hp.uniform(\"weight1\",0,1),\n",
    "                'weight2': hp.uniform(\"weight2\",0,1),\n",
    "                'weight3': hp.uniform(\"weight3\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18a6acf7-5c00-42b1-98ca-6d34371de7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params):\n",
    "    thr = params['thr']\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    \n",
    "    weights_sum = weight1 + weight2 + weight3\n",
    "\n",
    "    predict_probo_weight = (eval_predict_proba_lr * weight1 + \n",
    "                            eval_predict_proba_tree * weight2 + \n",
    "                            eval_predict_proba_RF * weight3) / weights_sum\n",
    "\n",
    "    res_weight = (predict_probo_weight >= thr) * 1\n",
    "\n",
    "    eval_score = accuracy_score(res_weight, y_train)\n",
    "    \n",
    "    return -eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "93ab7ea0-041c-40d4-9ac5-5d6d9322be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate = np.random.default_rng(2))    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b6d466f2-c371-4546-bd7c-80b4e04d008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5000/5000 [01:32<00:00, 54.34trial/s, best loss: -0.8354789852328663]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c470b701-9091-4548-b85e-106eb746e99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thr': 0.455650120218397,\n",
       " 'weight1': 0.018974448598142818,\n",
       " 'weight2': 0.0013087603673773525,\n",
       " 'weight3': 0.8149604936936505}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a593ed-08bd-4e5d-a1a7-296cd3f3ef70",
   "metadata": {},
   "source": [
    "然后以相同权重带入测试集进行加权计算，结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "40946447-23ba-4907-a021-4f6b94b253ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(params_best):\n",
    "    thr = params_best['thr']\n",
    "    weight1 = params_best['weight1']\n",
    "    weight2 = params_best['weight2']\n",
    "    weight3 = params_best['weight3']\n",
    "\n",
    "    weights_sum = weight1 + weight2 + weight3\n",
    "\n",
    "    test_predict_proba = (((test_predict_proba_lr * weight1 + \n",
    "                            test_predict_proba_tree * weight2 + \n",
    "                            test_predict_proba_RF * weight3) / weights_sum) >= thr) * 1\n",
    "\n",
    "    print(accuracy_score(test_predict_proba, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "55e893a6-4516-4003-a64d-0ff0c7762d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7961385576377058\n"
     ]
    }
   ],
   "source": [
    "test_acc(params_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
