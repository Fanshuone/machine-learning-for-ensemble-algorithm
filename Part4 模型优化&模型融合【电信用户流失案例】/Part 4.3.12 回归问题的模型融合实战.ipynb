{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb88ccd-8d82-4430-824c-8b364d177406",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> 【Kaggle】Telco Customer Churn 电信用户流失预测案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3675b67-eb57-4f58-a55b-d868803d2fd0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ea78f-054b-40a2-80ed-9144d8d4215c",
   "metadata": {},
   "source": [
    "## <font face=\"仿宋\">第四部分导读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1e57e-21de-4922-9a79-000e9e8bf5ff",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">在案例的第二、三部分中，我们详细介绍了关于特征工程的各项技术，特征工程技术按照大类来分可以分为数据预处理、特征衍生、特征筛选三部分，其中特征预处理的目的是为了将数据集整理、清洗到可以建模的程度，具体技术包括缺失值处理、异常值处理、数据重编码等，是建模之前必须对数据进行的处理和操作；而特征衍生和特征筛选则更像是一类优化手段，能够帮助模型突破当前数据集建模的效果上界。并且我们在第二部分完整详细的介绍机器学习可解释性模型的训练、优化和解释方法，也就是逻辑回归和决策树模型。并且此前我们也一直以这两种算法为主，来进行各个部分的模型测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e59e90-a2f2-4f52-8c81-1fdfc44fb8bc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而第四部分，我们将开始介绍集成学习的训练和优化的实战技巧，尽管从可解释性角度来说，集成学习的可解释性并不如逻辑回归和决策树，但在大多数建模场景下，集成学习都将获得一个更好的预测结果，这也是目前效果优先的建模场景下最常使用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fa489-3498-4264-9cd2-7a74fac9c191",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">总的来说，本部分内容只有一个目标，那就是借助各类优化方法，抵达每个主流集成学习的效果上界。换而言之，本部分我们将围绕单模优化策略展开详细的探讨，涉及到的具体集成学习包括随机森林、XGBoost、LightGBM、和CatBoost等目前最主流的集成学习算法，而具体的优化策略则包括超参数优化器的使用、特征衍生和筛选方法的使用、单模型自融合方法的使用，这些优化方法也是截至目前，提升单模效果最前沿、最有效、同时也是最复杂的方法。其中有很多较为艰深的理论，也有很多是经验之谈，但无论如何，我们希望能够围绕当前数据集，让每个集成学习算法优化到极限。值得注意的是，在这个过程中，我们会将此前介绍的特征衍生和特征筛选视作是一种模型优化方法，衍生和筛选的效果，一律以模型的最终结果来进行评定。而围绕集成学习进行海量特征衍生和筛选，也才是特征衍生和筛选技术能发挥巨大价值的主战场。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc521449-1b3c-4bb5-bd28-9d1218296d49",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而在抵达了单模的极限后，我们就会进入到下一阶段，也就是模型融合阶段。需要知道的是，只有单模的效果到达了极限，进一步的多模型融合、甚至多层融合，才是有意义的，才是有效果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646bc4d-32e4-4f87-a3ca-d1029a04c010",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e1beb-0ec5-4584-a28f-b49dc126e8ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center>Part 4.集成算法的训练与优化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0634a53-0e6f-43ef-a6db-7818e09c322c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# 导入模型融合模块\n",
    "import manual_ensemble as me\n",
    "from manual_ensemble import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138e9e3-0ec1-4a57-8d35-4c4cd1d488db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center>Ch.3.12 回归问题的模型融合方法实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46989c2c-d3e5-493c-ad42-7ab56dddcbab",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在训练得到了三个回归模型之后，接下来我们考虑对其结果进行融合。正如此前所说，回归类问题的融合方法和分类问题的融合方法并没有本质上的区别，从大类上来划分都是基础结合器法（平均&加权平均融合）和学习器结合器法（Stacking&Blending）两大类方法，两类融合方法的区别仅仅在于具体使用上的区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab989f7-ff73-48bc-ada7-0ff4833211c4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们首先探讨在如何在回归问题中使用这些融合方法，并在实践过程中总结回归问题模型融合的注意事项、并围绕此前定义的模型融合函数工具进行修改，使得其能够适用于回归问题的融合场景中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5e3b1-e8fc-496e-982d-fb60fd2274c1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先先按照上一小节的方法，对数据和已经保存好的模型进行导入："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae475012-dc72-4310-b187-eba40bd7345a",
   "metadata": {},
   "source": [
    "- 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d1aeb5-7b91-488d-a9f3-7f5747126284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# 加载数据\n",
    "cal_housing = fetch_california_housing()\n",
    "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "y = cal_housing.target\n",
    "\n",
    "# 划分训练集&测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# 重置index\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfbe61c-5bd0-41cb-985e-611f324f3f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.6134</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.560729</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>3.141700</td>\n",
       "      <td>37.93</td>\n",
       "      <td>-121.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.3578</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.455598</td>\n",
       "      <td>1.007722</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>4.131274</td>\n",
       "      <td>32.80</td>\n",
       "      <td>-117.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5111</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.716747</td>\n",
       "      <td>1.037905</td>\n",
       "      <td>3903.0</td>\n",
       "      <td>2.689869</td>\n",
       "      <td>34.25</td>\n",
       "      <td>-118.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.1124</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.623188</td>\n",
       "      <td>1.019324</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>2.785024</td>\n",
       "      <td>34.11</td>\n",
       "      <td>-118.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2957</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.627832</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>3.443366</td>\n",
       "      <td>37.25</td>\n",
       "      <td>-121.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  6.6134       4.0  6.560729   0.939271      1552.0  3.141700     37.93   \n",
       "1  2.3578      41.0  5.455598   1.007722      1070.0  4.131274     32.80   \n",
       "2  5.5111      16.0  5.716747   1.037905      3903.0  2.689869     34.25   \n",
       "3  8.1124      52.0  6.623188   1.019324      1153.0  2.785024     34.11   \n",
       "4  6.2957      25.0  6.627832   1.008091      2128.0  3.443366     37.25   \n",
       "\n",
       "   Longitude  \n",
       "0    -121.97  \n",
       "1    -117.15  \n",
       "2    -118.61  \n",
       "3    -118.14  \n",
       "4    -121.81  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb75afb-5fc6-4683-9f6a-b8f313a53709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e716273-85d0-46ff-b7a3-7553a4465c5d",
   "metadata": {},
   "source": [
    "- 模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70e9052-77dc-44e2-b1e2-df0d0eb005d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg = load('./models/RF_reg.joblib')\n",
    "ET_reg = load('./models/ET_reg.joblib')\n",
    "GBR_reg = load('./models/GBR_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a9f456-7765-4e26-b30f-2c4c10026288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03228467081263358, 0.23113640374016972)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg.predict(X_train), y_train), mean_squared_error(RF_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13895e7-4936-4d59-a6d9-146170462afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3687030793344334e-07, 0.22259777187847052)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_reg.predict(X_train), y_train), mean_squared_error(ET_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957e9f4d-7d5a-488a-882f-99b97cce1dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02536793820105726, 0.19804404020292288)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_reg.predict(X_train), y_train), mean_squared_error(GBR_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68563a64-c3da-4241-ba01-84a02dc209d4",
   "metadata": {},
   "source": [
    "同时，为了方便对比后续融合效果，我们将单模MSE指标总结如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b2cde-46dc-48de-ba75-add23342a223",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <center>0.1980 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f0ce3-c8be-4eac-8e80-1c8a8d83f89a",
   "metadata": {},
   "source": [
    "## 十二、回归问题的模型融合方法实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55563b6-d574-450d-9b16-a192476e6798",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.基础结合器法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee1e82-dd65-4f35-adeb-7b7458b76b7e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是基础接合器法的尝试，也就是平均法&投票法。不同于分类问题可以围绕类别预测结果和概率预测结果进行融合，回归问题只有一种结果——标签的数值预测结果，因此融合时只有平均法和加权平均法，并没有硬投票方法。这里我们先输出三个模型在训练集和测试集上预测结果，方便后续进行手动融合操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99940d5b-3a87-47db-85ed-aee5c6d13292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.0558111 , 1.48645396, 2.75037489, ..., 1.22637157, 2.01426659,\n",
       "       2.01472055])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prediction_RF = RF_reg.predict(X_train)\n",
    "train_prediction_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bcd1e0e-b655-4d54-8f2c-bfef82ae663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集上的预测结果\n",
    "train_prediction_RF = RF_reg.predict(X_train)\n",
    "train_prediction_ET = ET_reg.predict(X_train)\n",
    "train_prediction_GBR = GBR_reg.predict(X_train)\n",
    "\n",
    "# 测试集上的预测结果\n",
    "test_prediction_RF = RF_reg.predict(X_test)\n",
    "test_prediction_ET = ET_reg.predict(X_test)\n",
    "test_prediction_GBR = GBR_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c6f17-72a4-48c0-ba7c-6e68b9f7f611",
   "metadata": {},
   "source": [
    "#### 1.1 平均法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2e905-6454-4e57-acf8-014c769a405e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后先看平均融合法，该方法的实践过程非常简单，只需要对三个模型的输出结果进行均值计算即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "99883273-038e-43bd-92b0-83a5a93abf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_soft_train = np.mean([train_prediction_RF, train_prediction_ET, train_prediction_GBR], axis=0)\n",
    "Voting_soft_test = np.mean([test_prediction_RF, test_prediction_ET, test_prediction_GBR], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "64c45843-f606-46c3-9430-96baee36117a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.06245493, 1.53372996, 2.72675295, ..., 1.25156342, 2.00781961,\n",
       "       1.95747529])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Voting_soft_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4f1bda51-c06a-4ad7-a39f-293bbcbb1832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010958893356275578, 0.2049222750790844)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(Voting_soft_train, y_train), mean_squared_error(Voting_soft_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c99e8-aec1-4dfd-a2ea-a6df7ddac327",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <center>0.1980 |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3736fb-563a-42f4-b421-5917547bc009",
   "metadata": {},
   "source": [
    "能够发现，（一如既往的）平均融合结果并不如最好的单模效果好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e28f60-3e6c-4b83-bd96-9c9e1d85850f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，平均法融合也可以调用sklearn中的VotingRegressor进行融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9862e0-e7cc-49fb-bb29-0a9ac76efa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b9fe368d-26c4-4319-8113-e52a8677c65a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mVotingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Prediction voting regressor for unfitted estimators.\n",
       "\n",
       "A voting regressor is an ensemble meta-estimator that fits several base\n",
       "regressors, each on the whole dataset. Then it averages the individual\n",
       "predictions to form a final prediction.\n",
       "\n",
       "Read more in the :ref:`User Guide <voting_regressor>`.\n",
       "\n",
       ".. versionadded:: 0.21\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimators : list of (str, estimator) tuples\n",
       "    Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n",
       "    of those original estimators that will be stored in the class attribute\n",
       "    ``self.estimators_``. An estimator can be set to ``'drop'`` using\n",
       "    ``set_params``.\n",
       "\n",
       "    .. versionchanged:: 0.21\n",
       "        ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n",
       "        support was removed in 0.24.\n",
       "\n",
       "weights : array-like of shape (n_regressors,), default=None\n",
       "    Sequence of weights (`float` or `int`) to weight the occurrences of\n",
       "    predicted values before averaging. Uses uniform weights if `None`.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel for ``fit``.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    If True, the time elapsed while fitting will be printed as it\n",
       "    is completed.\n",
       "\n",
       "    .. versionadded:: 0.23\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "estimators_ : list of regressors\n",
       "    The collection of fitted sub-estimators as defined in ``estimators``\n",
       "    that are not 'drop'.\n",
       "\n",
       "named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
       "    Attribute to access any fitted sub-estimators by name.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`. Only defined if the\n",
       "    underlying regressor exposes such an attribute when fit.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Only defined if the\n",
       "    underlying estimators expose such an attribute when fit.\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "VotingClassifier : Soft Voting/Majority Rule classifier.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.linear_model import LinearRegression\n",
       ">>> from sklearn.ensemble import RandomForestRegressor\n",
       ">>> from sklearn.ensemble import VotingRegressor\n",
       ">>> r1 = LinearRegression()\n",
       ">>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n",
       ">>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n",
       ">>> y = np.array([2, 6, 12, 20, 30, 42])\n",
       ">>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n",
       ">>> print(er.fit(X, y).predict(X))\n",
       "[ 3.3  5.7 11.8 19.7 28.  40.3]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VotingRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27d4ee-f1f0-459b-b081-7773f5fbd901",
   "metadata": {},
   "source": [
    "这里能发现，sklearn中回归问题的平均法评估器仍然属于Voting大类，不过相比分类问题的VotingClassifier评估器的参数，VotingRegressor简单很多，除了必要的estimators之外，需要我们关注的就只剩下weights参数，而该参数的功能和VotingClassifier中的weights参数一样，用于调整各评估器的权重，用于加权平均法的融合场景中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b3fcc-7d09-4c10-91aa-0b614fb72993",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们创建estimator对象类型，并借助VotingRegressor进行平均融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0430753b-586d-4573-a6f7-b200f9a379cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('RF', RF_reg), ('ET', ET_reg), ('GBR', GBR_reg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1c3017ea-223d-483a-b31d-6c318924f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR = VotingRegressor(estimators).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "59de458b-5ac6-4a76-b8bc-bb0ad6897e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010958893356275578, 0.2049222750790844)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(VR.predict(X_train), y_train), mean_squared_error(VR.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c3544-7d2e-46c6-8032-cfc7b3fcf977",
   "metadata": {},
   "source": [
    "能够发现和手动实现结果完全一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb124d-2e58-47bf-9c21-d8a3c3c6ee81",
   "metadata": {},
   "source": [
    "#### 1.2 基于经验的加权平均法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b418d4-004d-4f86-ae98-aeffecaf63de",
   "metadata": {},
   "source": [
    "&emsp;&emsp;诚然在大多数情况下平均法无法获得更好的结果，但加权平均就所有不同了。通过不同评估器的灵活的权重设置，很多时候我们都能够通过加权平均的方法获得一个更好的结果，甚至有些情况下加权平均的效果会超越学习结合器的融合效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb436b-2329-46fb-9b66-28e3c9c09ece",
   "metadata": {},
   "source": [
    "- 权重设计策略回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddfd01-7069-4b42-8964-2276624b1618",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们先回顾下此前介绍的加权平均中的权重设计策略。一般来说我们首先权重设计可以分为经验法和超参数搜索法两种方法，所谓经验法，指的是通过不断的尝试，试出一组还不错的权重组合；而所谓的超参数搜索法，则指的是把权重看成是超参数，通过灵活高效的TPE搜索来确定一组最佳参数。而具体经验法的实践过程中又可以细分为倍数梯度权重和指数梯度权重两种方法，而超参数搜索法则可以在具体实践过程中借助经验法找的权重组合进行搜索空间裁剪，从而帮助超参数搜索法更加快速、精准的找到最佳权重组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc6237-c6d2-4059-a75b-a9915536e5d0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;需要注意的是，上述全部方法均可直接应用于回归问题中，并且回归类问题往往由于数值表现更加丰富，稍微调整下数值可能就会得到一个更好的结果，因此各类融合方法将会有更大的尝试空间和更好的融合效果。这里我们按照经验法、超参数搜索法的顺序进行尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6b1b5-0e52-4fcd-a599-d712f959d8af",
   "metadata": {},
   "source": [
    "- 经验法确定权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b3819-0381-4ea2-bdcf-999b23106f5f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在经验法确定权重的过程中，首先值得尝试的就是按照倍数进行权重设置——也就是根据模型效果倒序排序的序号作为权重进行设置，例如此处三个模型，GBDT效果好于极端随机树好于随机森林，则我们可以给这三个模型分别分配3、2、1的权重进行融合："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e8e0b-c8fe-42a8-a248-5f7d242952b8",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <center>0.1980 |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39eb3f6-4b14-4224-a468-c2a6ebd5083f",
   "metadata": {},
   "source": [
    "具体实现过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "28e84b7b-d853-4aa2-8eab-5a1cd0c84269",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_weight1_train = (train_prediction_RF * 1 + \n",
    "                        train_prediction_ET * 2 + \n",
    "                        train_prediction_GBR * 3) / (1 + 2 + 3)\n",
    "Voting_weight1_test = (test_prediction_RF * 1 + \n",
    "                       test_prediction_ET * 2 + \n",
    "                       test_prediction_GBR * 3) / (1 + 2 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c9936db8-afbe-45d8-aca9-08a8c4a346b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01065539056373778, 0.19896041813115542)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(Voting_weight1_train, y_train), mean_squared_error(Voting_weight1_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb5e98-c4bb-45e0-b852-6e844fab1067",
   "metadata": {},
   "source": [
    "能够发现，融合结果较平均融合结果更好，不过仍然没有超过单模最好效果，而且训练集得分和测试集得分方向并不一致："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d131b5-d34d-46ac-aee9-7eb72efa5cd4",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <center>0.1980 |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8acd0ba-204d-4677-86ef-d226e127e3a8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;既然倍数梯度的权重设置效果不佳，接下来我们继续尝试指数梯度的权重设置方法，即三个模型的权重分配为1、10和100，具体实现过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "144f3efb-dcd5-4439-b0ac-137c6d2ef987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_weight2_train = (train_prediction_RF * 1 + \n",
    "                        train_prediction_ET * 10 + \n",
    "                        train_prediction_GBR * 100) / (1 + 10 + 100)\n",
    "Voting_weight2_test = (test_prediction_RF * 1 + \n",
    "                       test_prediction_ET * 10 + \n",
    "                       test_prediction_GBR * 100) / (1 + 10 + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e9e5c8f7-3a73-4c0f-be9d-3af871b197ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02092639703571113, 0.1963037475747155)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(Voting_weight2_train, y_train), mean_squared_error(Voting_weight2_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9075e3-06c5-481f-af37-f4be8f76f033",
   "metadata": {},
   "source": [
    "能够发现，指数梯度权重分配方案融合得到了目前为主最好的一个结果，并且超越了单模最佳评分："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2977e3-7eb0-4389-9502-bc42df14b11f",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <font color=\"red\"><center>**0.1980** |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |\n",
    "| <center>指数梯度权重 | <center>0.0209 | <font color=\"red\"><center>**0.1963** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345d44f-8978-4652-a629-f3b8d69fed89",
   "metadata": {},
   "source": [
    "当然，一旦当我们确定了经验法的某种效果有效之后，我们就可以继续在1：10：100的基础权重设置之上进行微调，例如我们可以设置权重为5：10：100，得到结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b110d12b-0ac2-4215-9f20-397cfef2b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_weight3_train = (train_prediction_RF * 5 + \n",
    "                        train_prediction_ET * 10 + \n",
    "                        train_prediction_GBR * 100) / (5 + 10 + 100)\n",
    "Voting_weight3_test = (test_prediction_RF * 5 + \n",
    "                       test_prediction_ET * 10 + \n",
    "                       test_prediction_GBR * 100) / (5 + 10 + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b14b4068-e2b5-434c-9820-30e5821e2bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020792380614160604, 0.19613799284676225)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(Voting_weight3_train, y_train), mean_squared_error(Voting_weight3_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7e23c-646a-4213-9c31-930067a8c53d",
   "metadata": {},
   "source": [
    "能够发现，效果有了更进一步的提升。这里可以继续手动尝试，不过一种更合理高效的方法，是基于1：10：100的基础权重进行超参数空间设计、然后借助TPE搜索得到一个更好的权重组合。这一方案我们稍后会进行具体的尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23f3a8-e57c-40c7-80e8-bdb2e3919036",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，除了手动实现外，我们也可以借助VotingRegressor快速实现加权平均融合。首先是倍数梯度权重分配融合过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1ec7b1d0-69cd-4942-8627-7b8311d00086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RF',\n",
       "  RandomForestRegressor(max_features=2, max_samples=0.9990956320729892,\n",
       "                        n_estimators=619, random_state=12)),\n",
       " ('ET',\n",
       "  ExtraTreesRegressor(max_depth=39, max_features=0.6, n_estimators=516,\n",
       "                      random_state=12)),\n",
       " ('GBR',\n",
       "  GradientBoostingRegressor(learning_rate=0.07948277691156985, max_depth=7,\n",
       "                            n_estimators=499, random_state=12,\n",
       "                            subsample=0.9053472864274166))]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b1a84e22-a3f0-45a9-a3c5-33b659315214",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR_weight1 = VotingRegressor(estimators, weights=[1, 2, 3]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c1ef1520-6eb8-4b36-be66-322a14bd0fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01065539056373778, 0.19896041813115542)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(VR_weight1.predict(X_train), y_train), mean_squared_error(VR_weight1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "97728ffd-0fa1-41d0-9ce0-2c53c5b775a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.06341202, 1.52727695, 2.71233779, ..., 1.24522128, 2.01855007,\n",
       "       1.95797275])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR_weight1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb8f65-99c6-4e22-a9ba-224c4d040ad1",
   "metadata": {},
   "source": [
    "其次是指数梯度权重分配过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ead4b65a-5509-4fbd-8638-4bc136d42606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02092639703571113, 0.1963037475747155)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR_weight2 = VotingRegressor(estimators, weights=[1, 10, 100]).fit(X_train, y_train)\n",
    "mean_squared_error(VR_weight2.predict(X_train), y_train), mean_squared_error(VR_weight2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "13e901fe-bbbe-4594-9f56-91103ce12b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020792380614160604, 0.19613799284676225)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR_weight3 = VotingRegressor(estimators, weights=[5, 10, 100]).fit(X_train, y_train)\n",
    "mean_squared_error(VR_weight3.predict(X_train), y_train), mean_squared_error(VR_weight3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf6bf7-5669-44bc-8d5f-28217e7d4f34",
   "metadata": {},
   "source": [
    "和手动实现过程完全一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6068024-b66c-4d4c-abdb-9942b0733730",
   "metadata": {},
   "source": [
    "> 这里需要注意，尽管在课程中回归和分类问题的经验法加权平均融合过程中，都是指数梯度权重设计结果好于倍数梯度权重设计结果，但在实际建模过程中，这两种方法都是值得尝试的方法，并不一定哪种方法就一定好于另一种方法。事实上，正是由于两组实验中都是模型结果差异较大，这才导致指数权重策略好于倍数权重策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16394ae9-febb-472a-844b-42b408fde254",
   "metadata": {},
   "source": [
    "#### 1.3 基于TPE搜索的加权平均法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f03e3-06e8-48a0-8585-1810ed6daa40",
   "metadata": {},
   "source": [
    "- 基于TPE的权重搜索策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a162ed-ed3e-436a-b9dc-8fd1adfd039e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来继续尝试基于TPE的权重搜索策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a6f56-d810-4841-9fbd-cc5c2c9ff44e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在此前的分类问题融合过程中，基于TPE权重搜索的加权平均融合在实际执行过程中容易出现过拟合问题，也就是会出现训练集上得分上升、但测试集上得分反而下降的情况。为了解决这个问题，我们提出了三种解决方案，其一是用交叉验证的融合结果代替原始融合结果，也就是借助更有可信度的验证集的平均得分代替训练集上得分，进行权重的筛选；其二则是借助经验法得到的权重组合，对搜索空间进行裁剪，此举不仅能加快迭代速度，更能帮助搜索过程跳出“伪”最优解陷阱；其三则是在模型训练阶段就进行交叉训练，然后用验证集拼接而成的训练集（train_oof）代替原始训练集，并以train_oof的融合结果作为超参数筛选依据，进行权重筛选，此时由于train_oof数据都是间接得出，因此该数据集上的融合结果可信度更高，最终帮助搜索过程提高权重结果的泛化能力。接下来我们逐个方法进行尝试："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f672eb-9558-4df2-9513-72fca54cad4e",
   "metadata": {},
   "source": [
    "- 方案一：基于交叉验证的TPE搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee2bf5-187d-49a4-8ecb-21638af626fe",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们先尝试基于交叉验证的TPE搜索，此处需要注意使用验证集的平均得分作为目标函数的输出，具体搜索过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0a30dcff-3f93-4206-bb6d-483755222ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'weight1': hp.uniform(\"weight1\",0,1),\n",
    "                'weight2': hp.uniform(\"weight2\",0,1),\n",
    "                'weight3': hp.uniform(\"weight3\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a0b32ada-82ce-4ade-8dd5-51ae7fe17cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params, train=True):\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    \n",
    "    weights = [weight1, weight2, weight3]\n",
    "    \n",
    "    VR_weight_search = VotingRegressor(estimators, weights=weights)\n",
    "\n",
    "    if train == True:\n",
    "        val_score = cross_val_score(VR_weight_search, \n",
    "                                    X_train, \n",
    "                                    y_train, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=15,\n",
    "                                    cv=5).mean()\n",
    "        res = -val_score\n",
    "    else:\n",
    "        VR_weight_search = VotingRegressor(estimators, \n",
    "                                           weights=weights).fit(X_train, y_train)\n",
    "        res = VR_weight_search\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7a39138d-ab15-409c-8229-58950f1a5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a9aae-11e7-4d92-bc90-1dc0ce6a8d00",
   "metadata": {},
   "source": [
    "首先尝试迭代50次。这里由于需要重复训练模型，因此整体搜索时间较长。最终得到搜索结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c39ae817-8174-4dc4-9e2f-48a4157776d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 50/50 [30:02<00:00, 36.06s/trial, best loss: 0.2064834684512073]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "166147ae-b107-42e8-9069-5f17aa589cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.11106741979888705,\n",
       " 'weight2': 0.20768131703488266,\n",
       " 'weight3': 0.9143087708368308}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ff3acc25-c408-44a1-99b0-ff7753513761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016947070903517353, 0.19576421177473802)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR = hyperopt_objective_weight(params_best, train=False)\n",
    "mean_squared_error(VR.predict(X_train), y_train), mean_squared_error(VR.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4a9ea-5086-409b-94a3-5dc79ad972e4",
   "metadata": {},
   "source": [
    "能够发现，相比指数梯度权重设置，此时训练集和测试集的MSE均有不同成都下降，这也说明我们得到了一组更优权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f8375-9336-47dd-860b-37f627838524",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <font color=\"red\"><center>**0.1980** |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |\n",
    "| <center>指数梯度权重 | <center>0.0209 | <font color=\"red\"><center>**0.1963** |\n",
    "| <center>基于交叉验证的TPE搜索（50次迭代） | <center>0.0169 | <font color=\"red\"><center>**0.1957** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795a06d-98f7-4de8-a758-ab3fd2ca42a6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而这里需要重点关注的是，对比此前的分类问题实验，回归问题中的基于TPE搜索的权重设计方案似乎过拟合问题并没有那么严重。该结论可以从以下两点可以看出，其一是对比指数梯度权重策略，TPE搜索结果呈现出训练集和测试集得分同步变化的趋势；其二则是最终我们搜索得到的权重组合，和经验法试出来的指数梯度权重组合较为接近，这也说明不同方法其实在逼近相类似的最优解，间接能够判断在当前场景下，基于TPE的权重搜索导致的过拟合问题并没有之前分类问题是严重那么严重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8180f9b4-5551-4048-a7c7-6ade301e0e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.11106741979888705,\n",
       " 'weight2': 0.20768131703488266,\n",
       " 'weight3': 0.9143087708368308}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990e849-65d1-4eb3-a302-d2ddefd1a026",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而这里过拟合倾向被抑制，也可以从交叉验证平均结果、训练集预测结果和测试集预测结果三组结果的对比中看出，通过引入交叉验证方法，能够得到更具有泛化能力的评估结果。这里我们可以进一步手动验证结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "29d25e69-2a9e-4755-a00e-6c2eef0586bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VR_weight1 = VotingRegressor(estimators, weights=[1, 2, 3]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "58194346-79e8-4077-ba24-b03b76682e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01065539056373778, 0.19896041813115542)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(VR_weight1.predict(X_train), y_train), mean_squared_error(VR_weight1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9cffe7f3-20b3-4454-8534-8095686f80f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02092639703571113, 0.1963037475747155)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR_weight2 = VotingRegressor(estimators, weights=[1, 10, 100]).fit(X_train, y_train)\n",
    "mean_squared_error(VR_weight2.predict(X_train), y_train), mean_squared_error(VR_weight2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "45ed0b1c-d1e1-4b48-8302-517475f68db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020792380614160604, 0.19613799284676225)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR_weight3 = VotingRegressor(estimators, weights=[5, 10, 100]).fit(X_train, y_train)\n",
    "mean_squared_error(VR_weight3.predict(X_train), y_train), mean_squared_error(VR_weight3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea5cde-85b5-4409-9799-723b4dd09a4d",
   "metadata": {},
   "source": [
    "引入交叉验验证后："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5fef126a-120d-4899-be05-21830aee8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score_weight1 = cross_val_score(VR_weight1, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f4bd7181-57aa-416a-bfc3-c0df8f3fd357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20975354476853605"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_score_weight1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "93e08943-db52-4fed-99d4-4fc55f6becd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score_weight2 = cross_val_score(VR_weight2, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "842f92fe-b438-4ca3-8a2a-5800d5817f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20719220099122068"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_score_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c04dcaf7-9d44-43f3-b284-bdd51f47cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score_weight3 = cross_val_score(VR_weight3, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8465ddda-1a6e-4af8-9244-8062909fa6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20697517984665487"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_score_weight3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdec54-e6f4-412d-bc46-82b06727ccd8",
   "metadata": {},
   "source": [
    "| 权重分配 | 训练集得分 | 验证集平均得分 | 测试集平均得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| <center>1：2：3 | <center>0.0106 | <center>0.2097 | <center>0.1989 |\n",
    "| <center>1：10：100 | <center>0.0209 | <center>0.2071 | <center>0.1963 |\n",
    "| <center>5：10：100 | <center>0.0207 | <center>0.2069 | <center>0.1961 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34245320-c81c-474c-b5de-9109124288b0",
   "metadata": {},
   "source": [
    "能够看出，本实验中，验证集的平均得分不仅和测试集评分更为接近，并且保持了同步变化趋势，这也进一步验证了验证集的有效性，并且这也让我们更加确信TPE权重搜索方法对于该数据的建模有效性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7af49-d1dc-48f8-92d1-7c37334007a0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过为何分类问题中存在的过拟合问题，到回归问题中就得到了好转？这里其实需要注意的是，模型融合中某方法是否存在过拟合问题，其实也是因数据而异的。我们这里可以把TPE权重搜索融合也看成是一种“算法”，一种算法针对不同的数据集会有不同的模型表现，有时过拟合而有时候泛化能力较强，都属于正常情况。并且需要实际尝试，才能得到最终结论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff8559-ae29-42ae-a84c-0a119c79cda3",
   "metadata": {},
   "source": [
    "> 抑制过拟合和提升泛化能力其实是一件事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de838ba1-9c0f-4a52-8059-25971b40ce93",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此时，在判断权重搜索融合能得确保泛化能力的情况下，我们可以进一步进一步尝试提升迭代次数。此处我们尝试迭代100次，最终结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a879d6fe-57e9-445a-9446-93cc6a8a0a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [50:59<00:00, 30.60s/trial, best loss: 0.20605715906734842]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9384424e-c696-4930-baf4-fac4b935dd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.0024817595519138385,\n",
       " 'weight2': 0.289661638124085,\n",
       " 'weight3': 0.8367511006808657}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9740e80-3c4c-4879-96e5-5cc191dc8d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.014009603224766298, 0.19532482892782888)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR = hyperopt_objective_weight(params_best, train=False)\n",
    "mean_squared_error(VR.predict(X_train), y_train), mean_squared_error(VR.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617480af-7414-4beb-88ee-c5aa490f5c2c",
   "metadata": {},
   "source": [
    "能够发现，效果有更进一步的提升："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8100dcb-2ce7-4c08-843c-4f45cdbfd590",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <font color=\"red\"><center>**0.1980** |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |\n",
    "| <center>指数梯度权重 | <center>0.0209 | <font color=\"red\"><center>**0.1963** |\n",
    "| <center>基于交叉验证的TPE搜索（50次迭代） | <center>0.0169 | <center>0.1957 |\n",
    "| <center>基于交叉验证的TPE搜索（100次迭代） | <center>0.0140 | <font color=\"red\"><center>**0.1953** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdb361-1a0a-47e0-876d-2315eec00816",
   "metadata": {},
   "source": [
    "- 方案二：基于搜索空间裁剪的TPE权重搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc245b-58b5-420e-93cb-8b85b2ca99ee",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在分类问题的融合过程，我们曾讨论到可以通过裁剪搜索空间来抑制过拟合，实际上除此功能外，搜索空间的裁剪也能够提高搜索效率、提高泛化能力。此案例中尽管TPE搜索融合并没有太多的过拟合倾向，但我们仍然可以通过搜索空间裁剪来提高融合结果。同样我们还是根据经验法探索的1：10：100权重来进行空间裁剪，具体空间裁剪数值如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "12e3ddf4-4dac-417e-aeb5-49906e216d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'weight1': hp.uniform(\"weight1\",0,0.05),\n",
    "                'weight2': hp.uniform(\"weight2\",0.05,0.1),\n",
    "                'weight3': hp.uniform(\"weight3\",0.5,1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ef2b8-c584-4bc6-bf11-60c65af3e231",
   "metadata": {},
   "source": [
    "然后尝试进行TPE搜索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1fe794fd-545d-41c6-b90a-46c9e216337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [30:11<00:00, 36.23s/trial, best loss: 0.20640153459947724]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c536cead-10a8-4899-bed1-9441c342cb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.024057728548545276,\n",
       " 'weight2': 0.09590853047157558,\n",
       " 'weight3': 0.5172496557013706}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7df73866-c570-4467-98e4-39ff10fd0dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0180191974133278, 0.19562547355816728)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VR = hyperopt_objective_weight(params_best, train=False)\n",
    "mean_squared_error(VR.predict(X_train), y_train), mean_squared_error(VR.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a539c-ad4c-4c99-afd6-de85bd7b59e9",
   "metadata": {},
   "source": [
    "能够发现，对比裁剪前的50次迭代结果，最终结果有进一步的提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0163861-f4a9-4d33-b1a0-2cd4f94af5ab",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <font color=\"red\"><center>**0.1980** |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |\n",
    "| <center>指数梯度权重 | <center>0.0209 | <font color=\"red\"><center>**0.1963** |\n",
    "| <center>基于交叉验证的TPE搜索（50次迭代） | <center>0.0169 | <center>0.1957 |\n",
    "| <center>基于交叉验证的TPE搜索（100次迭代） | <center>0.0140 | <font color=\"red\"><center>**0.1953** |\n",
    "| <center>空间裁剪的TPE搜索（50次迭代） | <center>0.0180 | <center>0.1956 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2f62c-2626-441a-8c83-7ef82b57ded6",
   "metadata": {},
   "source": [
    "这里同学们可以课后自行尝试基于空间裁剪的100次迭代效果，基本也能逼近基于交叉验证的搜索效果。不难看出，通过上述实践，也再次验证了权重搜索融合方法及其搜索空间裁剪策略的有效性。而0.1953也是目前我们获得的测试集上最佳评分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f63189-a431-46fe-accc-dbc3c6c4040b",
   "metadata": {},
   "source": [
    "- 方案三：基于交叉训练的TPE权重搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3736ca-b36d-4057-a467-3039509b6676",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们继续尝试基于交叉训练的TPE搜索。和搜索空间裁剪法类似，提出交叉训练方法的初衷是为了解决过拟合问题，不过在一些非过拟合的场景中，交叉训练仍然能够更进一步的提高融合效果。并且由于交叉训练是不用每次都反复训练模型、只需要在train_oof上反复搜索即可，因此每次搜索的耗时更短，能够支持海量次数的迭代。而当本身TPE权重搜索融合的过程并不会严重过拟合，海量迭代就能够逼近理论上的效果上上限，本实例就是非常好的一次验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937016f-7c1b-4ab8-b9e0-c9b06f9e4a1a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，交叉训练过程还是借助此前定义的train_cross函数，并在函数内添加回归问题的train_oof数据集创建过程，实现过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8117b0bf-530d-4dc7-9941-2660a43c16c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_cross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mblending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Stacking融合过程一级学习器交叉训练函数\n",
       "\n",
       ":param X_train: 训练集特征\n",
       ":param y_train: 训练集标签\n",
       ":param X_test: 测试集特征\n",
       ":param estimators: 一级学习器，由(名称,评估器)组成的列表\n",
       ":param n_splits: 交叉训练折数\n",
       ":param test_size: blending过程留出集占比\n",
       ":param random_state: 随机数种子\n",
       ":param blending: 是否进行blending融合\n",
       "\n",
       ":return：交叉训练后创建oof训练数据和测试集平均预测结果，同时包含特征和标签，标签在最后一列\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\work\\jupyter\\telco\\正式课程\\manual_ensemble.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cross?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94d6a2b6-e6c0-45ce-a890-4bf4553d440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross(X_train, \n",
    "                y_train, \n",
    "                X_test, \n",
    "                estimators, \n",
    "                test_size = 0.2, \n",
    "                n_splits = 5, \n",
    "                random_state = 12, \n",
    "                blending = False, \n",
    "                regress = False):\n",
    "    \"\"\"\n",
    "    Stacking融合过程一级学习器交叉训练函数\n",
    "    \n",
    "    :param X_train: 训练集特征\n",
    "    :param y_train: 训练集标签\n",
    "    :param X_test: 测试集特征\n",
    "    :param estimators: 一级学习器，由(名称,评估器)组成的列表\n",
    "    :param n_splits: 交叉训练折数\n",
    "    :param test_size: blending过程留出集占比，blending参数为False时无用\n",
    "    :param random_state: 随机数种子\n",
    "    :param blending: 是否进行blending融合\n",
    "    :param regress: 是否进行回归类问题融合\n",
    "    \n",
    "    :return：交叉训练后创建oof训练数据和测试集平均预测结果，同时包含特征和标签，标签在最后一列\n",
    "    \"\"\"    \n",
    "    # 创建一级评估器输出的训练集预测结果和测试集预测结果数据集\n",
    "    if type(y_train) == np.ndarray:\n",
    "        y_train = pd.Series(y_train)\n",
    "    \n",
    "    if blending == True:\n",
    "        X, X1, y, y1 = train_test_split(X_train, y_train, test_size=test_size, random_state=random_state)\n",
    "        m = X1.shape[0]\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        X1 = X1.reset_index(drop=True)\n",
    "        y1 = y1.reset_index(drop=True)\n",
    "    else:\n",
    "        m = X_train.shape[0]\n",
    "        X = X_train.reset_index(drop=True)\n",
    "        y = y_train.reset_index(drop=True)\n",
    "    \n",
    "    n = len(estimators)\n",
    "    m_test = X_test.shape[0]\n",
    "    \n",
    "    columns = []\n",
    "    for estimator in estimators:\n",
    "        columns.append(estimator[0] + '_oof')\n",
    "    \n",
    "    train_oof = pd.DataFrame(np.zeros((m, n)), columns=columns)\n",
    "    \n",
    "    columns = []\n",
    "    for estimator in estimators:\n",
    "        columns.append(estimator[0] + '_predict')\n",
    "    \n",
    "    test_predict = pd.DataFrame(np.zeros((m_test, n)), columns=columns)\n",
    "    \n",
    "    # 实例化重复交叉验证评估器\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # 执行交叉训练\n",
    "    for estimator in estimators:\n",
    "        model = estimator[1]\n",
    "        oof_colName = estimator[0] + '_oof'\n",
    "        predict_colName = estimator[0] + '_predict'\n",
    "        \n",
    "        for train_part_index, eval_index in kf.split(X, y):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X.loc[train_part_index]\n",
    "            y_train_part = y.loc[train_part_index]\n",
    "            model.fit(X_train_part, y_train_part)\n",
    "            \n",
    "            # 如果是回归问题\n",
    "            if regress == True:\n",
    "                if blending == True:\n",
    "                    # 在留出集上进行预测并求均值\n",
    "                    train_oof[oof_colName] += model.predict(X1) / n_splits\n",
    "                    # 在测试集上进行预测并求均值\n",
    "                    test_predict[predict_colName] += model.predict(X_test) / n_splits\n",
    "                else:\n",
    "                    # 在验证集上进行验证\n",
    "                    X_eval_part = X.loc[eval_index]\n",
    "                    # 将验证集上预测结果拼接入oof数据集\n",
    "                    train_oof[oof_colName].loc[eval_index] = model.predict(X_eval_part)\n",
    "                    # 将测试集上预测结果填入predict数据集\n",
    "                    test_predict[predict_colName] += model.predict(X_test) / n_splits\n",
    "            \n",
    "            # 如果是分类问题\n",
    "            else:\n",
    "                if blending == True:\n",
    "                    # 在留出集上进行预测并求均值\n",
    "                    train_oof[oof_colName] += model.predict_proba(X1)[:, 1] / n_splits\n",
    "                    # 在测试集上进行预测并求均值\n",
    "                    test_predict[predict_colName] += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "                else:\n",
    "                    # 在验证集上进行验证\n",
    "                    X_eval_part = X.loc[eval_index]\n",
    "                    # 将验证集上预测结果拼接入oof数据集\n",
    "                    train_oof[oof_colName].loc[eval_index] = model.predict_proba(X_eval_part)[:, 1]\n",
    "                    # 将测试集上预测结果填入predict数据集\n",
    "                    test_predict[predict_colName] += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    \n",
    "    # 添加标签列\n",
    "    if blending == True:\n",
    "        train_oof[y1.name] = y1\n",
    "    else:\n",
    "        train_oof[y.name] = y\n",
    "        \n",
    "    return train_oof, test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c9a44-b123-48fb-bb1d-93668dc7c579",
   "metadata": {},
   "source": [
    "改写过程中我们只添加了回归问题判别语句以及回归问题时的处理代码，回归问题的train_oof和test_predict数据集的创建过程和分类问题基本一致，只需要区分回归问题是用.predict输出预测结果，而分类问题则是.predict_proba输出结果并需要切片1类数据预测概率。其他并无区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdd992-ec81-4ba9-9610-f36ef5be9761",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来测试函数效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78fd431f-4140-43f3-9328-bc6329261c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof, test_predict = train_cross(X_train, y_train, X_test, estimators, regress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c48aa421-2431-4277-9ead-2d5f36ce3bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_oof</th>\n",
       "      <th>ET_oof</th>\n",
       "      <th>GBR_oof</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.910792</td>\n",
       "      <td>2.806845</td>\n",
       "      <td>3.010536</td>\n",
       "      <td>3.07000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.122422</td>\n",
       "      <td>1.356957</td>\n",
       "      <td>1.299047</td>\n",
       "      <td>1.66700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.748327</td>\n",
       "      <td>2.771082</td>\n",
       "      <td>2.520479</td>\n",
       "      <td>2.76600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.495247</td>\n",
       "      <td>4.514102</td>\n",
       "      <td>4.789784</td>\n",
       "      <td>5.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.743010</td>\n",
       "      <td>2.723145</td>\n",
       "      <td>2.453989</td>\n",
       "      <td>2.51800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>2.241706</td>\n",
       "      <td>2.312674</td>\n",
       "      <td>2.191293</td>\n",
       "      <td>1.91300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>1.402586</td>\n",
       "      <td>1.442147</td>\n",
       "      <td>1.425944</td>\n",
       "      <td>1.61800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>1.097334</td>\n",
       "      <td>1.105217</td>\n",
       "      <td>1.088715</td>\n",
       "      <td>1.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>2.166915</td>\n",
       "      <td>2.142442</td>\n",
       "      <td>2.269993</td>\n",
       "      <td>1.92900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>2.227121</td>\n",
       "      <td>2.197091</td>\n",
       "      <td>1.963705</td>\n",
       "      <td>1.84000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RF_oof    ET_oof   GBR_oof     None\n",
       "0      2.910792  2.806845  3.010536  3.07000\n",
       "1      1.122422  1.356957  1.299047  1.66700\n",
       "2      2.748327  2.771082  2.520479  2.76600\n",
       "3      4.495247  4.514102  4.789784  5.00001\n",
       "4      2.743010  2.723145  2.453989  2.51800\n",
       "...         ...       ...       ...      ...\n",
       "16507  2.241706  2.312674  2.191293  1.91300\n",
       "16508  1.402586  1.442147  1.425944  1.61800\n",
       "16509  1.097334  1.105217  1.088715  1.34000\n",
       "16510  2.166915  2.142442  2.269993  1.92900\n",
       "16511  2.227121  2.197091  1.963705  1.84000\n",
       "\n",
       "[16512 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92879161-9e9f-49d8-bda9-55e4b09717bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_predict</th>\n",
       "      <th>ET_predict</th>\n",
       "      <th>GBR_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.995458</td>\n",
       "      <td>2.160231</td>\n",
       "      <td>2.073867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.142218</td>\n",
       "      <td>1.961178</td>\n",
       "      <td>2.063859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.096183</td>\n",
       "      <td>2.206849</td>\n",
       "      <td>2.074315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.544865</td>\n",
       "      <td>1.594343</td>\n",
       "      <td>1.440555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164448</td>\n",
       "      <td>1.159081</td>\n",
       "      <td>1.009008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>2.744251</td>\n",
       "      <td>2.657388</td>\n",
       "      <td>2.597423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4124</th>\n",
       "      <td>3.054298</td>\n",
       "      <td>2.946849</td>\n",
       "      <td>2.727207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>0.837954</td>\n",
       "      <td>0.890181</td>\n",
       "      <td>0.851294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>3.904305</td>\n",
       "      <td>3.897062</td>\n",
       "      <td>4.076651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>2.052269</td>\n",
       "      <td>2.037865</td>\n",
       "      <td>1.961599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RF_predict  ET_predict  GBR_predict\n",
       "0       1.995458    2.160231     2.073867\n",
       "1       2.142218    1.961178     2.063859\n",
       "2       2.096183    2.206849     2.074315\n",
       "3       1.544865    1.594343     1.440555\n",
       "4       1.164448    1.159081     1.009008\n",
       "...          ...         ...          ...\n",
       "4123    2.744251    2.657388     2.597423\n",
       "4124    3.054298    2.946849     2.727207\n",
       "4125    0.837954    0.890181     0.851294\n",
       "4126    3.904305    3.897062     4.076651\n",
       "4127    2.052269    2.037865     1.961599\n",
       "\n",
       "[4128 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70856f07-aad5-44f0-bd16-ebe446cd717c",
   "metadata": {},
   "source": [
    "这里需要注意，train_oof最后一列名称为None的列是数据集标签，正常情况标签名是通过y.name来进行提取，但此处输入函数中的y是array，虽然经过了函数内部的Series的转换，但没有列名称，因此结果为None："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6598e35-b9be-4a6b-979c-809dcc6fb169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.07 , 1.667, 2.766, ..., 1.34 , 1.929, 1.84 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2437625d-408e-4a36-9568-fb3e1a264ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e402d62-608a-4aa3-b711-a6e91fe6af58",
   "metadata": {},
   "source": [
    "这里我们可以手动添加最后一列的列名称："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afa0ea10-7a66-440c-a9f7-49ebd461e1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedHouseVal']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_housing.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1408674b-a518-4efc-bf91-846c8d29f248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MedHouseVal'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_housing.target_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b13f1f88-14ca-48ce-972a-867cc647a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof.columns = ['RF_oof', 'ET_oof', 'GBR_oof', 'MedHouseVal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "832874d9-2f3b-4e74-95cc-73901df5c41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_oof</th>\n",
       "      <th>ET_oof</th>\n",
       "      <th>GBR_oof</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.910792</td>\n",
       "      <td>2.806845</td>\n",
       "      <td>3.010536</td>\n",
       "      <td>3.07000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.122422</td>\n",
       "      <td>1.356957</td>\n",
       "      <td>1.299047</td>\n",
       "      <td>1.66700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.748327</td>\n",
       "      <td>2.771082</td>\n",
       "      <td>2.520479</td>\n",
       "      <td>2.76600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.495247</td>\n",
       "      <td>4.514102</td>\n",
       "      <td>4.789784</td>\n",
       "      <td>5.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.743010</td>\n",
       "      <td>2.723145</td>\n",
       "      <td>2.453989</td>\n",
       "      <td>2.51800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RF_oof    ET_oof   GBR_oof  MedHouseVal\n",
       "0  2.910792  2.806845  3.010536      3.07000\n",
       "1  1.122422  1.356957  1.299047      1.66700\n",
       "2  2.748327  2.771082  2.520479      2.76600\n",
       "3  4.495247  4.514102  4.789784      5.00001\n",
       "4  2.743010  2.723145  2.453989      2.51800"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec2f33-2c44-4fe8-95d8-0f5e8f08a546",
   "metadata": {},
   "source": [
    "或者也可以在输入y的时候先将其转化为带有名称的Series。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215e4a0-e902-4628-a253-819ef1383952",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在得到了train_oof和test_predict之后，我们即可对其进行加权平均融合。这里我们先简单尝试平均融合结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5ada69ba-15d1-42a6-999c-c536b4ea8d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.909391\n",
       "1        1.259475\n",
       "2        2.679963\n",
       "3        4.599711\n",
       "4        2.640048\n",
       "           ...   \n",
       "16507    2.248558\n",
       "16508    1.423559\n",
       "16509    1.097089\n",
       "16510    2.193117\n",
       "16511    2.129306\n",
       "Length: 16512, dtype: float64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_VR_mean = train_oof.iloc[:, :3].mean(1)\n",
    "train_VR_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a3747eb9-d813-45c1-9a77-5fd58c1a16b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2.076519\n",
       "1       2.055752\n",
       "2       2.125782\n",
       "3       1.526588\n",
       "4       1.110846\n",
       "          ...   \n",
       "4123    2.666354\n",
       "4124    2.909452\n",
       "4125    0.859810\n",
       "4126    3.959339\n",
       "4127    2.017245\n",
       "Length: 4128, dtype: float64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_VR_mean = test_predict.iloc[:, :3].mean(1)\n",
    "test_VR_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e5b0ac87-ddbd-4455-874d-32523fb04a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21608936247690866, 0.20849425035096503)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(train_VR_mean, y_train), mean_squared_error(test_VR_mean, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8975a4c-93f2-4afd-bb73-9725972a1123",
   "metadata": {},
   "source": [
    "能够发现，交叉训练本身并不能提升简单平均融合的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8997a7-ea22-4e45-8571-f6ed0f2e84f4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们继续考虑进行TPE权重搜索融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d8987c14-c78e-4536-aa05-63a1ad5b87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'weight1': hp.uniform(\"weight1\",0,1),\n",
    "                'weight2': hp.uniform(\"weight2\",0,1),\n",
    "                'weight3': hp.uniform(\"weight3\",0,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "34840dea-4c86-4cfb-8e7f-408d2b3cdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params, train=True):\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    \n",
    "    weights = np.array([weight1, weight2, weight3])\n",
    "    \n",
    "    if train == True:\n",
    "        res_train = (train_oof.iloc[:, :3] * weights).sum(1) / weights.sum()\n",
    "        MSE_res = mean_squared_error(res_train, y_train)\n",
    "        res = MSE_res\n",
    "    else:\n",
    "        res = weights\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8492a-e562-4fd9-b4cb-fa4d9112d6d1",
   "metadata": {},
   "source": [
    "需要注意的是，目标函数里面的加权过程用到了数组广播，具体广播相乘计算过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dbff0ac8-ea2d-41cc-ac75-69fe26ebc71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.910792\n",
       "1        1.122422\n",
       "2        2.748327\n",
       "3        4.495247\n",
       "4        2.743010\n",
       "           ...   \n",
       "16507    2.241706\n",
       "16508    1.402586\n",
       "16509    1.097334\n",
       "16510    2.166915\n",
       "16511    2.227121\n",
       "Name: RF_oof, Length: 16512, dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "64d46164-7da8-44e5-8839-7cc5d4ea0864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_oof</th>\n",
       "      <th>ET_oof</th>\n",
       "      <th>GBR_oof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.910792</td>\n",
       "      <td>28.068451</td>\n",
       "      <td>301.053632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.122422</td>\n",
       "      <td>13.569574</td>\n",
       "      <td>129.904728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.748327</td>\n",
       "      <td>27.710817</td>\n",
       "      <td>252.047942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.495247</td>\n",
       "      <td>45.141018</td>\n",
       "      <td>478.978426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.743010</td>\n",
       "      <td>27.231453</td>\n",
       "      <td>245.398855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>2.241706</td>\n",
       "      <td>23.126736</td>\n",
       "      <td>219.129305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>1.402586</td>\n",
       "      <td>14.421474</td>\n",
       "      <td>142.594369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>1.097334</td>\n",
       "      <td>11.052171</td>\n",
       "      <td>108.871537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>2.166915</td>\n",
       "      <td>21.424420</td>\n",
       "      <td>226.999336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>2.227121</td>\n",
       "      <td>21.970914</td>\n",
       "      <td>196.370511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RF_oof     ET_oof     GBR_oof\n",
       "0      2.910792  28.068451  301.053632\n",
       "1      1.122422  13.569574  129.904728\n",
       "2      2.748327  27.710817  252.047942\n",
       "3      4.495247  45.141018  478.978426\n",
       "4      2.743010  27.231453  245.398855\n",
       "...         ...        ...         ...\n",
       "16507  2.241706  23.126736  219.129305\n",
       "16508  1.402586  14.421474  142.594369\n",
       "16509  1.097334  11.052171  108.871537\n",
       "16510  2.166915  21.424420  226.999336\n",
       "16511  2.227121  21.970914  196.370511\n",
       "\n",
       "[16512 rows x 3 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof.iloc[:, :3] * np.array([1, 10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "78a31ce6-5f3f-421f-8e50-d377da595ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        332.032875\n",
       "1        144.596723\n",
       "2        282.507086\n",
       "3        528.614692\n",
       "4        275.373318\n",
       "            ...    \n",
       "16507    244.497747\n",
       "16508    158.418430\n",
       "16509    121.021042\n",
       "16510    250.590670\n",
       "16511    220.568547\n",
       "Length: 16512, dtype: float64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_oof.iloc[:, :3] * np.array([1, 10, 100])).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "59094db4-a866-4597-a922-534f534ea55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 10, 100]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb200e7-1c3f-4e91-9ed7-34e8639dca76",
   "metadata": {},
   "source": [
    "然后定义优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "9d25c627-9abc-47f9-8b97-b3eee09e33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ef527-04ed-4d8d-ad91-31ce4e5aded6",
   "metadata": {},
   "source": [
    "接下来测试进行搜索，由于不用反复训练模型、每次迭代只是数值计算，因此可以考虑设置更大的迭代次数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fb21baf4-9443-45cd-a357-98efa8546245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 200/200 [00:01<00:00, 153.74trial/s, best loss: 0.2066999070373698]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6d1429e9-652c-49ee-ad58-d36692dc2926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.03132637208709112,\n",
       " 'weight2': 0.09982461974413209,\n",
       " 'weight3': 0.5003967731059809}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1edf1c6c-1b3f-4b94-a78a-18a59720b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03132637, 0.09982462, 0.50039677])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weights = hyperopt_objective_weight(params_best, train=False)\n",
    "best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6c13d6fd-c861-4051-bfb3-061ce890a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = (test_predict.iloc[:, :3] * best_weights).sum(1) / best_weights.sum()\n",
    "MSE_res = mean_squared_error(res_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "65511821-c77b-4b8b-a2f3-335187cb2c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19333050577750574"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba132c8-97c3-469e-8938-13ca28203212",
   "metadata": {},
   "source": [
    "迭代200次仅用时1s，并且得到了一组截止目前最好的融合结果。接下来继续尝试增加迭代次数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1e9afe8d-6021-49ee-8ef7-0b175975536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 2000/2000 [00:29<00:00, 68.78trial/s, best loss: 0.20668595198010906]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "596f3647-10d8-461a-a4c3-10feef300a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.019872909249536313,\n",
       " 'weight2': 0.09999748042937995,\n",
       " 'weight3': 0.5000702673971477}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "51fa858b-5d0e-420f-902a-da497814e1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01987291, 0.09999748, 0.50007027])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weights = hyperopt_objective_weight(params_best, train=False)\n",
    "best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6138c43b-aefe-4b34-83ff-0bfe57d65b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = (test_predict.iloc[:, :3] * best_weights).sum(1) / best_weights.sum()\n",
    "MSE_res = mean_squared_error(res_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8083260e-2b02-4937-9885-98e6c73adb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1930182288329917"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018efaf0-071f-42ae-b2a8-372c44a1f37c",
   "metadata": {},
   "source": [
    "能够发现，迭代2000次后效果有了更进一步提升，接下来继续增加迭代次数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0ae5643c-8d63-4726-a807-2105a49b71f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 10000/10000 [09:20<00:00, 17.83trial/s, best loss: 0.20668588719715994]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d5b4e41d-967d-403e-a29d-c2f6bedf54b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.01933442915160935,\n",
       " 'weight2': 0.09999448846226298,\n",
       " 'weight3': 0.5000126791510763}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2cd92711-5034-4aa2-81f5-0228b37abd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01933443, 0.09999449, 0.50001268])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weights = hyperopt_objective_weight(params_best, train=False)\n",
    "best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ca5afaa8-adad-4662-9eb5-f712a3454ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = (test_predict.iloc[:, :3] * best_weights).sum(1) / best_weights.sum()\n",
    "MSE_res = mean_squared_error(res_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "3aa14e1b-d513-4697-bd4f-76e34f565753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19300368584356492"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43a113-31f6-4092-adc1-f5413716d4de",
   "metadata": {},
   "source": [
    "伴随着迭代次数提升，融合效果效果不断提升、并且验证集和测试集始终保持同步变化，这也说明方法的在当前数据集上的表现非常稳健："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537480d-c147-478b-a849-f65fec2cea96",
   "metadata": {},
   "source": [
    "| 迭代次数 | train_oof得分 | 测试集平均得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>200 | <center>0.2066999070373698 | <center>0.19333 |\n",
    "| <center>2000 | <center>0.20668595198010906 | <center>0.19301 |\n",
    "| <center>10000 | <center>0.20668588719715994 | <center>0.19300 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb4939-cf32-4f56-9646-8b528a426cfa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，哪怕是交叉训练后的TPE权重搜索，我们也是可以进行搜索空间裁剪的，以进一步提升迭代效率和预测结果。还是按照类似的裁剪数值，TPE搜索效果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "02ba1f7e-50bd-4a86-a6fd-573b701e2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数空间\n",
    "params_space = {'weight1': hp.uniform(\"weight1\",0,0.01),\n",
    "                'weight2': hp.uniform(\"weight2\",0.05,0.1),\n",
    "                'weight3': hp.uniform(\"weight3\",0.5,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "cc17276e-928a-48e6-b09a-bfccf784b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 2000/2000 [00:29<00:00, 68.43trial/s, best loss: 0.20669530153582757]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b18e6a3c-6236-4c9e-8a46-7ada5a4ae28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 0.009995130515269924,\n",
       " 'weight2': 0.09999831798285891,\n",
       " 'weight3': 0.5001811581252161}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f581d36d-aa30-4efd-88a5-158ee0d102be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00999513, 0.09999832, 0.50018116])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weights = hyperopt_objective_weight(params_best, train=False)\n",
    "best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ba09c387-f916-44cc-9e75-09e5d629e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = (test_predict.iloc[:, :3] * best_weights).sum(1) / best_weights.sum()\n",
    "MSE_res = mean_squared_error(res_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "123a13b6-8027-4682-9423-21b9cd564575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19274993739359678"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa6e48-97bd-4ebf-b44e-754e24ab4dc0",
   "metadata": {},
   "source": [
    "能够发现效果有了更进一步的提升。至此我们也获得了加权平均融合最好的一组融合结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf3f83-689b-4999-af73-d9fcf39f5ae4",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <font color=\"red\"><center>**0.1980** |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |\n",
    "| <center>指数梯度权重 | <center>0.0209 | <font color=\"red\"><center>**0.1963** |\n",
    "| <center>基于交叉验证的TPE搜索（50次迭代） | <center>0.0169 | <center>0.1957 |\n",
    "| <center>基于交叉验证的TPE搜索（100次迭代） | <center>0.0140 | <font color=\"red\"><center>**0.1953** |\n",
    "| <center>空间裁剪的TPE搜索（50次迭代） | <center>0.0180 | <center>0.1956 |\n",
    "| <center>交叉训练的TPE搜索（200次迭代） | <center>0.2066 | <center>0.19333 |\n",
    "| <center>交叉训练的TPE搜索（2000次迭代） | <center>0.2066 | <center>0.19301 |\n",
    "| <center>交叉训练的TPE搜索（10000次迭代） | <center>0.2066 | <center>0.19300 |\n",
    "| <center>交叉训练+空间裁剪TPE搜索（2000次迭代） |<center>0.2066 | <font color=\"red\"><center>**0.1927** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a81cd0-ad92-4fa2-8226-b879dbd211c3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然和分类问题类似，我们可以类似的把测试集带入进行搜索，测试加权平均融合效果上限："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "19aa805f-95f3-4393-82e0-bac07eae0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标函数\n",
    "def hyperopt_objective_weight(params, train=True):\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    \n",
    "    weights = np.array([weight1, weight2, weight3])\n",
    "    \n",
    "    if train == True:\n",
    "        res_test = (test_predict.iloc[:, :3] * weights).sum(1) / weights.sum()\n",
    "        MSE_res = mean_squared_error(res_test, y_test)\n",
    "        res = MSE_res\n",
    "    else:\n",
    "        res = weights\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d12f617c-e05d-4dc2-9d69-58f794fd38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "def param_hyperopt_weight(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_objective_weight,\n",
    "                       space = params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "001cf63d-2b57-4d1d-9470-c40ceedb4835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 1000/1000 [00:09<00:00, 102.55trial/s, best loss: 0.19170078526754766]\n"
     ]
    }
   ],
   "source": [
    "params_best = param_hyperopt_weight(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c28bc034-d7e3-4710-9147-8c624ec61147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight1': 1.2340960509984558e-06,\n",
       " 'weight2': 0.050004127069823705,\n",
       " 'weight3': 0.9989348939544079}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352843f-a68d-486a-a048-d30e8ea6d031",
   "metadata": {},
   "source": [
    "能够发现测试集加权平均融合的效果上界是0.1917，和训练得到的0.1927非常接近，也说明此前的加权融合取得了不错的效果。当然需要再次强调，一般情况测试集并没有标签，因此这里的效果上线测试只是为了课程为了验证方法效果，并不是一般建模手段。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387b28a-b90e-419f-8d96-ae4d4622de93",
   "metadata": {},
   "source": [
    "### 2.学习结合器法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c532024-8887-4b55-87f2-0926761f657e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们继续尝试学习结合器法进行模型融合——也就是Stacking与Blending方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b9cff-3d9a-478b-a1e4-70445c965867",
   "metadata": {},
   "source": [
    "#### 2.1 Stacking融合法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20c4a3-e7ee-43d0-8606-f2f08f3e98d2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是Stacking模型融合方法，先快速回顾Stacking融合原理如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa90a8-485a-49b8-9fd5-acfc3fc92951",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221103152633511.png\" alt=\"image-20221103152633511\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0c2bf-2498-4d85-b5c7-8f4bd3c227ae",
   "metadata": {},
   "source": [
    "并且由于此前我们已经得到了train_oof数据集和test_predict，这里我们直接使用train_oof训练元学习器、然后在test_predict上进行预测即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0710f5f-48d0-4ccb-bdfb-fe97f577665c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_oof</th>\n",
       "      <th>ET_oof</th>\n",
       "      <th>GBR_oof</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.910792</td>\n",
       "      <td>2.806845</td>\n",
       "      <td>3.010536</td>\n",
       "      <td>3.07000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.122422</td>\n",
       "      <td>1.356957</td>\n",
       "      <td>1.299047</td>\n",
       "      <td>1.66700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.748327</td>\n",
       "      <td>2.771082</td>\n",
       "      <td>2.520479</td>\n",
       "      <td>2.76600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.495247</td>\n",
       "      <td>4.514102</td>\n",
       "      <td>4.789784</td>\n",
       "      <td>5.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.743010</td>\n",
       "      <td>2.723145</td>\n",
       "      <td>2.453989</td>\n",
       "      <td>2.51800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>2.241706</td>\n",
       "      <td>2.312674</td>\n",
       "      <td>2.191293</td>\n",
       "      <td>1.91300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>1.402586</td>\n",
       "      <td>1.442147</td>\n",
       "      <td>1.425944</td>\n",
       "      <td>1.61800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>1.097334</td>\n",
       "      <td>1.105217</td>\n",
       "      <td>1.088715</td>\n",
       "      <td>1.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>2.166915</td>\n",
       "      <td>2.142442</td>\n",
       "      <td>2.269993</td>\n",
       "      <td>1.92900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>2.227121</td>\n",
       "      <td>2.197091</td>\n",
       "      <td>1.963705</td>\n",
       "      <td>1.84000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RF_oof    ET_oof   GBR_oof  MedHouseVal\n",
       "0      2.910792  2.806845  3.010536      3.07000\n",
       "1      1.122422  1.356957  1.299047      1.66700\n",
       "2      2.748327  2.771082  2.520479      2.76600\n",
       "3      4.495247  4.514102  4.789784      5.00001\n",
       "4      2.743010  2.723145  2.453989      2.51800\n",
       "...         ...       ...       ...          ...\n",
       "16507  2.241706  2.312674  2.191293      1.91300\n",
       "16508  1.402586  1.442147  1.425944      1.61800\n",
       "16509  1.097334  1.105217  1.088715      1.34000\n",
       "16510  2.166915  2.142442  2.269993      1.92900\n",
       "16511  2.227121  2.197091  1.963705      1.84000\n",
       "\n",
       "[16512 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7431d9f-da20-4b8d-8e53-5982b5489c18",
   "metadata": {},
   "source": [
    "- 回归融合问题的元学习器选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0cd0b-4a9e-4d16-b306-b4bb5a0b7330",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其实无论是回归问题还是分类问题、无论是Stacking还是Blending，元学习器的选择都是类似的——为避免过拟合问题，元学习器往往需要选择模型结构简单、预测效力一般的模型。之前的分类问题讲解中，我们重点推荐逻辑回归模型或者结构非常简单的树模型作为元学习器，而在回归问题Stacking融合中，则往往使用线性回归模型或者贝叶斯回归作为元学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76461a-26bf-449d-81a3-b186df314b22",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们先进行一次深度尝试，大范围测试不同类型的回归模型作为元学习器的效果，再就元学习器模型选择问题进行分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af4b51c6-f4b9-410a-ba13-ec7cd0e34ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train-MSE: 0.205622, Test-MSE: 0.192784\n",
      "The results of Ridge-final:\n",
      "Train-MSE: 0.205622, Test-MSE: 0.192803\n",
      "The results of Lasso-final:\n",
      "Train-MSE: 1.109248, Test-MSE: 1.150578\n",
      "The results of ElasticNet-final:\n",
      "Train-MSE: 0.549212, Test-MSE: 0.561142\n",
      "The results of BayesianRidge-final:\n",
      "Train-MSE: 0.205622, Test-MSE: 0.192801\n",
      "The results of SVR-final:\n",
      "Train-MSE: 0.205412, Test-MSE: 0.194418\n",
      "The results of tree_reg-final:\n",
      "Train-MSE: 0.000000, Test-MSE: 0.377318\n",
      "The results of Bagging-final:\n",
      "Train-MSE: 0.042989, Test-MSE: 0.224572\n",
      "The results of RF-final:\n",
      "Train-MSE: 0.031436, Test-MSE: 0.209634\n",
      "The results of AdaBoost-final:\n",
      "Train-MSE: 0.280633, Test-MSE: 0.272866\n",
      "The results of GBR-final:\n",
      "Train-MSE: 0.190360, Test-MSE: 0.194770\n",
      "The results of XGB-final:\n",
      "Train-MSE: 0.114721, Test-MSE: 0.207962\n"
     ]
    }
   ],
   "source": [
    "# 线性回归\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_reg = LinearRegression().fit(train_oof.iloc[:, :3], y_train)\n",
    "lr_train_prediction = lr_reg.predict(train_oof.iloc[:, :3])\n",
    "lr_test_prediction = lr_reg.predict(test_predict)\n",
    "print('The results of LR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(lr_train_prediction, y_train), \n",
    "                                       mean_squared_error(lr_test_prediction, y_test)))\n",
    "\n",
    "# 岭回归\n",
    "from sklearn.linear_model import Ridge\n",
    "Ridge_reg = Ridge().fit(train_oof.iloc[:, :3], y_train)\n",
    "Ridge_train_prediction = Ridge_reg.predict(train_oof.iloc[:, :3])\n",
    "Ridge_test_prediction = Ridge_reg.predict(test_predict)\n",
    "print('The results of Ridge-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Ridge_train_prediction, y_train), \n",
    "                                       mean_squared_error(Ridge_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# LASSO\n",
    "from sklearn.linear_model import Lasso\n",
    "Lasso_reg = Lasso().fit(train_oof.iloc[:, :3], y_train)\n",
    "Lasso_train_prediction = Lasso_reg.predict(train_oof.iloc[:, :3])\n",
    "Lasso_test_prediction = Lasso_reg.predict(test_predict)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Lasso_train_prediction, y_train), \n",
    "                                       mean_squared_error(Lasso_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# 弹性网\n",
    "from sklearn.linear_model import ElasticNet\n",
    "ElasticNet_reg = ElasticNet().fit(train_oof.iloc[:, :3], y_train)\n",
    "ElasticNet_train_prediction = ElasticNet_reg.predict(train_oof.iloc[:, :3])\n",
    "ElasticNet_test_prediction = ElasticNet_reg.predict(test_predict)\n",
    "print('The results of ElasticNet-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(ElasticNet_train_prediction, y_train), \n",
    "                                       mean_squared_error(ElasticNet_test_prediction, y_test)))\n",
    "\n",
    "# 贝叶斯回归\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "BayesianRidge_reg = BayesianRidge().fit(train_oof.iloc[:, :3], y_train)\n",
    "BayesianRidge_train_prediction = BayesianRidge_reg.predict(train_oof.iloc[:, :3])\n",
    "BayesianRidge_test_prediction = BayesianRidge_reg.predict(test_predict)\n",
    "print('The results of BayesianRidge-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(BayesianRidge_train_prediction, y_train), \n",
    "                                       mean_squared_error(BayesianRidge_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "SVR_reg = SVR().fit(train_oof.iloc[:, :3], y_train)\n",
    "SVR_train_prediction = SVR_reg.predict(train_oof.iloc[:, :3])\n",
    "SVR_test_prediction = SVR_reg.predict(test_predict)\n",
    "print('The results of SVR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(SVR_train_prediction, y_train), \n",
    "                                       mean_squared_error(SVR_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# 决策树回归\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "tree_train_prediction = tree_reg.predict(train_oof.iloc[:, :3])\n",
    "tree_test_prediction = tree_reg.predict(test_predict)\n",
    "print('The results of tree_reg-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(tree_train_prediction, y_train), \n",
    "                                       mean_squared_error(tree_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# Bagging\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging_reg = BaggingRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "bagging_train_prediction = bagging_reg.predict(train_oof.iloc[:, :3])\n",
    "bagging_test_prediction = bagging_reg.predict(test_predict)\n",
    "print('The results of Bagging-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(bagging_train_prediction, y_train), \n",
    "                                       mean_squared_error(bagging_test_prediction, y_test)))\n",
    "\n",
    "# 随机森林\n",
    "RFR = RandomForestRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "RFR_train_prediction = RFR.predict(train_oof.iloc[:, :3])\n",
    "RFR_test_prediction = RFR.predict(test_predict)\n",
    "print('The results of RF-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(RFR_train_prediction, y_train), \n",
    "                                       mean_squared_error(RFR_test_prediction, y_test)))\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ABR = AdaBoostRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "ABR_train_prediction = ABR.predict(train_oof.iloc[:, :3])\n",
    "ABR_test_prediction = ABR.predict(test_predict)\n",
    "print('The results of AdaBoost-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(ABR_train_prediction, y_train), \n",
    "                                       mean_squared_error(ABR_test_prediction, y_test)))\n",
    "\n",
    "# GBDT\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBR = GradientBoostingRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "GBR_train_prediction = GBR.predict(train_oof.iloc[:, :3])\n",
    "GBR_test_prediction = GBR.predict(test_predict)\n",
    "print('The results of GBR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(GBR_train_prediction, y_train), \n",
    "                                       mean_squared_error(GBR_test_prediction, y_test)))\n",
    "\n",
    "# XGB\n",
    "from xgboost import XGBRegressor\n",
    "XGB = XGBRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "XGB_train_prediction = XGB.predict(train_oof.iloc[:, :3])\n",
    "XGB_test_prediction = XGB.predict(test_predict)\n",
    "print('The results of XGB-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(XGB_train_prediction, y_train), \n",
    "                                       mean_squared_error(XGB_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ba132-3d98-432d-b517-12a4397bb34c",
   "metadata": {},
   "source": [
    "| 元学习器 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>线性回归 | <center>0.205622 | <center>0.192784 |\n",
    "| <center>贝叶斯回归 | <center>0.205622 | <center>0.192801 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a5a97-c895-42e6-b0d1-66c854654b3a",
   "metadata": {},
   "source": [
    "这里我们选取了线性回归方程家族模型，包括线性回归、岭回归、LASSO和弹性网，以及选择了贝叶斯回归、支持向量机、决策树模型，同时也带入了集成学习进行计算，能够非常明显的发现，线性回归和贝叶斯回归效果最好，而其他模型，除了弹性网和LASSO出现了欠拟合问题外，其他模型均出现了不同程度的过拟合问题。这里针对不同模型作为元学习器的不同表现进行分析："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034ff26-4cad-4480-b050-2186ab840db6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是关于岭回归、LASSO和弹性网三个模型，由于这三个模型在损失函数中增加了扰动项用于解决线性回归中系数矩阵不可逆导致无法求解的问题，因此面对简单数据集（此时不存在共线性问题），建模结果与预测精度并不如原始的线性回归模型，这也就是为何在这里这三个模型会表现出欠拟合的主要问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda55951-95c3-4228-9fa5-04f4e3659b9d",
   "metadata": {},
   "source": [
    "> 岭回归、LASSO和弹性网中的扰动项，从机器学习的角度进行解释其实就是结构风险惩罚项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d7145-a523-472a-aa85-fb2bbd4a2c05",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对比四个模型的损失函数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51c846-3c29-4308-92ae-7632efc64620",
   "metadata": {},
   "source": [
    "$$线性回归：\\min_{w} || X w - y||_2^2$$\n",
    "$$岭回归：\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "$$LASSO：\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$$\n",
    "$$弹性网：\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d343d-43a2-4f77-b3ca-412137b90a8d",
   "metadata": {},
   "source": [
    "而要消除这三个模型的过拟合问题也很简单，只需要手动调整超参数、以减弱扰动项对损失函数造成的影响即可，例如岭回归中我们逐渐减少$\\alpha$的取值，则能够逐渐提升模型预测效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9089c857-1414-4383-9504-cd2e1a1f8b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train-MSE: 0.205622, Test-MSE: 0.192803\n"
     ]
    }
   ],
   "source": [
    "# 岭回归\n",
    "from sklearn.linear_model import Ridge\n",
    "Ridge_reg = Ridge().fit(train_oof.iloc[:, :3], y_train)\n",
    "Ridge_train_prediction = Ridge_reg.predict(train_oof.iloc[:, :3])\n",
    "Ridge_test_prediction = Ridge_reg.predict(test_predict)\n",
    "print('The results of LR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Ridge_train_prediction, y_train), \n",
    "                                       mean_squared_error(Ridge_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05132459-eebe-4af6-81d2-788e49f5d620",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear least squares with l2 regularization.\n",
       "\n",
       "Minimizes the objective function::\n",
       "\n",
       "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
       "\n",
       "This model solves a regression model where the loss function is\n",
       "the linear least squares function and regularization is given by\n",
       "the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
       "This estimator has built-in support for multi-variate regression\n",
       "(i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
       "\n",
       "Read more in the :ref:`User Guide <ridge_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
       "    Regularization strength; must be a positive float. Regularization\n",
       "    improves the conditioning of the problem and reduces the variance of\n",
       "    the estimates. Larger values specify stronger regularization.\n",
       "    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
       "    :class:`~sklearn.linear_model.LogisticRegression` or\n",
       "    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
       "    assumed to be specific to the targets. Hence they must correspond in\n",
       "    number.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to fit the intercept for this model. If set\n",
       "    to false, no intercept will be used in calculations\n",
       "    (i.e. ``X`` and ``y`` are expected to be centered).\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        ``normalize`` was deprecated in version 1.0 and\n",
       "        will be removed in 1.2.\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If True, X will be copied; else, it may be overwritten.\n",
       "\n",
       "max_iter : int, default=None\n",
       "    Maximum number of iterations for conjugate gradient solver.\n",
       "    For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
       "    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
       "    For 'lbfgs' solver, the default value is 15000.\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    Precision of the solution.\n",
       "\n",
       "solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
       "    Solver to use in the computational routines:\n",
       "\n",
       "    - 'auto' chooses the solver automatically based on the type of data.\n",
       "\n",
       "    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
       "      coefficients. More stable for singular matrices than 'cholesky'.\n",
       "\n",
       "    - 'cholesky' uses the standard scipy.linalg.solve function to\n",
       "      obtain a closed-form solution.\n",
       "\n",
       "    - 'sparse_cg' uses the conjugate gradient solver as found in\n",
       "      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
       "      more appropriate than 'cholesky' for large-scale data\n",
       "      (possibility to set `tol` and `max_iter`).\n",
       "\n",
       "    - 'lsqr' uses the dedicated regularized least-squares routine\n",
       "      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
       "      procedure.\n",
       "\n",
       "    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
       "      its improved, unbiased version named SAGA. Both methods also use an\n",
       "      iterative procedure, and are often faster than other solvers when\n",
       "      both n_samples and n_features are large. Note that 'sag' and\n",
       "      'saga' fast convergence is only guaranteed on features with\n",
       "      approximately the same scale. You can preprocess the data with a\n",
       "      scaler from sklearn.preprocessing.\n",
       "\n",
       "    - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
       "      `scipy.optimize.minimize`. It can be used only when `positive`\n",
       "      is True.\n",
       "\n",
       "    All last six solvers support both dense and sparse data. However, only\n",
       "    'sag', 'sparse_cg', and 'lbfgs' support sparse input when `fit_intercept`\n",
       "    is True.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "\n",
       "positive : bool, default=False\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "    Only 'lbfgs' solver is supported in this case.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       `random_state` to support Stochastic Average Gradient.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    Weight vector(s).\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    Independent term in decision function. Set to 0.0 if\n",
       "    ``fit_intercept = False``.\n",
       "\n",
       "n_iter_ : None or ndarray of shape (n_targets,)\n",
       "    Actual number of iterations for each target. Available only for\n",
       "    sag and lsqr solvers. Other solvers will return None.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "RidgeClassifier : Ridge classifier.\n",
       "RidgeCV : Ridge regression with built-in cross validation.\n",
       ":class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
       "    combines ridge regression with the kernel trick.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.linear_model import Ridge\n",
       ">>> import numpy as np\n",
       ">>> n_samples, n_features = 10, 5\n",
       ">>> rng = np.random.RandomState(0)\n",
       ">>> y = rng.randn(n_samples)\n",
       ">>> X = rng.randn(n_samples, n_features)\n",
       ">>> clf = Ridge(alpha=1.0)\n",
       ">>> clf.fit(X, y)\n",
       "Ridge()\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af238ffc-b5bf-44bb-8a1c-76fb850de714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train-MSE: 0.205622, Test-MSE: 0.192794\n"
     ]
    }
   ],
   "source": [
    "Ridge_reg = Ridge(alpha=0.5).fit(train_oof.iloc[:, :3], y_train)\n",
    "Ridge_train_prediction = Ridge_reg.predict(train_oof.iloc[:, :3])\n",
    "Ridge_test_prediction = Ridge_reg.predict(test_predict)\n",
    "print('The results of LR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Ridge_train_prediction, y_train), \n",
    "                                       mean_squared_error(Ridge_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7be2afdb-e32f-4506-b1cd-dee526d56aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train-MSE: 0.205622, Test-MSE: 0.192786\n"
     ]
    }
   ],
   "source": [
    "Ridge_reg = Ridge(alpha=0.1).fit(train_oof.iloc[:, :3], y_train)\n",
    "Ridge_train_prediction = Ridge_reg.predict(train_oof.iloc[:, :3])\n",
    "Ridge_test_prediction = Ridge_reg.predict(test_predict)\n",
    "print('The results of LR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Ridge_train_prediction, y_train), \n",
    "                                       mean_squared_error(Ridge_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb28e7-d535-4126-abe6-ef39d7ed8881",
   "metadata": {},
   "source": [
    "类似的，LASSO中通过减少$\\alpha$取值，同样能提升元学习器的预测效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e35b93e6-2735-4439-87d7-1862c9856527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of Lasso-final:\n",
      "Train-MSE: 1.109248, Test-MSE: 1.150578\n"
     ]
    }
   ],
   "source": [
    "# LASSO\n",
    "from sklearn.linear_model import Lasso\n",
    "Lasso_reg = Lasso().fit(train_oof.iloc[:, :3], y_train)\n",
    "Lasso_train_prediction = Lasso_reg.predict(train_oof.iloc[:, :3])\n",
    "Lasso_test_prediction = Lasso_reg.predict(test_predict)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Lasso_train_prediction, y_train), \n",
    "                                       mean_squared_error(Lasso_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8cfc9c5-482f-433e-9a2a-986f51733854",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mselection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cyclic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
       "\n",
       "The optimization objective for Lasso is::\n",
       "\n",
       "    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
       "\n",
       "Technically the Lasso model is optimizing the same objective function as\n",
       "the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
       "\n",
       "Read more in the :ref:`User Guide <lasso>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Constant that multiplies the L1 term. Defaults to 1.0.\n",
       "    ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
       "    by the :class:`LinearRegression` object. For numerical\n",
       "    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
       "    Given this, you should use the :class:`LinearRegression` object.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to calculate the intercept for this model. If set\n",
       "    to False, no intercept will be used in calculations\n",
       "    (i.e. data is expected to be centered).\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        ``normalize`` was deprecated in version 1.0 and will be removed in\n",
       "        1.2.\n",
       "\n",
       "precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
       "    Whether to use a precomputed Gram matrix to speed up\n",
       "    calculations. The Gram matrix can also be passed as argument.\n",
       "    For sparse input this option is always ``False`` to preserve sparsity.\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If ``True``, X will be copied; else, it may be overwritten.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    The tolerance for the optimization: if the updates are\n",
       "    smaller than ``tol``, the optimization code checks the\n",
       "    dual gap for optimality and continues until it is smaller\n",
       "    than ``tol``.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "positive : bool, default=False\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    The seed of the pseudo random number generator that selects a random\n",
       "    feature to update. Used when ``selection`` == 'random'.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "selection : {'cyclic', 'random'}, default='cyclic'\n",
       "    If set to 'random', a random coefficient is updated every iteration\n",
       "    rather than looping over features sequentially by default. This\n",
       "    (setting to 'random') often leads to significantly faster convergence\n",
       "    especially when tol is higher than 1e-4.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    Parameter vector (w in the cost function formula).\n",
       "\n",
       "dual_gap_ : float or ndarray of shape (n_targets,)\n",
       "    Given param alpha, the dual gaps at the end of the optimization,\n",
       "    same shape as each observation of y.\n",
       "\n",
       "sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
       "    Readonly property derived from ``coef_``.\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    Independent term in decision function.\n",
       "\n",
       "n_iter_ : int or list of int\n",
       "    Number of iterations run by the coordinate descent solver to reach\n",
       "    the specified tolerance.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "lars_path : Regularization path using LARS.\n",
       "lasso_path : Regularization path using Lasso.\n",
       "LassoLars : Lasso Path along the regularization parameter usingLARS algorithm.\n",
       "LassoCV : Lasso alpha parameter by cross-validation.\n",
       "LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n",
       "sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The algorithm used to fit the model is coordinate descent.\n",
       "\n",
       "To avoid unnecessary memory duplication the X argument of the fit method\n",
       "should be directly passed as a Fortran-contiguous numpy array.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import linear_model\n",
       ">>> clf = linear_model.Lasso(alpha=0.1)\n",
       ">>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
       "Lasso(alpha=0.1)\n",
       ">>> print(clf.coef_)\n",
       "[0.85 0.  ]\n",
       ">>> print(clf.intercept_)\n",
       "0.15...\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     MultiTaskElasticNet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Lasso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65ee9828-e9a6-4e8d-97ed-343431cc52e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of Lasso-final:\n",
      "Train-MSE: 0.216589, Test-MSE: 0.203833\n"
     ]
    }
   ],
   "source": [
    "# LASSO\n",
    "Lasso_reg = Lasso(alpha=0.1).fit(train_oof.iloc[:, :3], y_train)\n",
    "Lasso_train_prediction = Lasso_reg.predict(train_oof.iloc[:, :3])\n",
    "Lasso_test_prediction = Lasso_reg.predict(test_predict)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Lasso_train_prediction, y_train), \n",
    "                                       mean_squared_error(Lasso_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d4b145f-7f4a-43dd-8b91-3371eff8d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of Lasso-final:\n",
      "Train-MSE: 0.205775, Test-MSE: 0.193191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "Lasso_reg = Lasso(alpha=0.01).fit(train_oof.iloc[:, :3], y_train)\n",
    "Lasso_train_prediction = Lasso_reg.predict(train_oof.iloc[:, :3])\n",
    "Lasso_test_prediction = Lasso_reg.predict(test_predict)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Lasso_train_prediction, y_train), \n",
    "                                       mean_squared_error(Lasso_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473e205-6f8c-43cc-88ad-7aa92f074a1f",
   "metadata": {},
   "source": [
    "> 能够发现，从$\\alpha$的绝对数值对应的模型训练结果能看出，相同的$\\alpha$取值下LASSO效果更差，说明LASSO对结构风险的惩罚力度要大于岭回归，这也是为何有时我们可以借助LASSO来进行特征筛选的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f202b7a-dba9-45c2-9252-4d32cf1bfd26",
   "metadata": {},
   "source": [
    "而对于弹性网，则需要同时减少$\\alpha$取值、并增加$\\rho$取值，则可提升模型效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f8f721e-bf78-498c-93a5-39f331be1fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of Lasso-final:\n",
      "Train-MSE: 0.549212, Test-MSE: 0.561142\n"
     ]
    }
   ],
   "source": [
    "# 弹性网\n",
    "from sklearn.linear_model import ElasticNet\n",
    "ElasticNet_reg = ElasticNet().fit(train_oof.iloc[:, :3], y_train)\n",
    "ElasticNet_train_prediction = ElasticNet_reg.predict(train_oof.iloc[:, :3])\n",
    "ElasticNet_test_prediction = ElasticNet_reg.predict(test_predict)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(ElasticNet_train_prediction, y_train), \n",
    "                                       mean_squared_error(ElasticNet_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e9f2515-d5f3-4660-8bd1-7488b0f0ea59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mElasticNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mselection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cyclic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear regression with combined L1 and L2 priors as regularizer.\n",
       "\n",
       "Minimizes the objective function::\n",
       "\n",
       "        1 / (2 * n_samples) * ||y - Xw||^2_2\n",
       "        + alpha * l1_ratio * ||w||_1\n",
       "        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
       "\n",
       "If you are interested in controlling the L1 and L2 penalty\n",
       "separately, keep in mind that this is equivalent to::\n",
       "\n",
       "        a * ||w||_1 + 0.5 * b * ||w||_2^2\n",
       "\n",
       "where::\n",
       "\n",
       "        alpha = a + b and l1_ratio = a / (a + b)\n",
       "\n",
       "The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
       "alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
       "= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
       "unless you supply your own sequence of alpha.\n",
       "\n",
       "Read more in the :ref:`User Guide <elastic_net>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Constant that multiplies the penalty terms. Defaults to 1.0.\n",
       "    See the notes for the exact mathematical meaning of this\n",
       "    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
       "    solved by the :class:`LinearRegression` object. For numerical\n",
       "    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
       "    Given this, you should use the :class:`LinearRegression` object.\n",
       "\n",
       "l1_ratio : float, default=0.5\n",
       "    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
       "    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
       "    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether the intercept should be estimated or not. If ``False``, the\n",
       "    data is assumed to be already centered.\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        ``normalize`` was deprecated in version 1.0 and will be removed in\n",
       "        1.2.\n",
       "\n",
       "precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
       "    Whether to use a precomputed Gram matrix to speed up\n",
       "    calculations. The Gram matrix can also be passed as argument.\n",
       "    For sparse input this option is always ``False`` to preserve sparsity.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations.\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If ``True``, X will be copied; else, it may be overwritten.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    The tolerance for the optimization: if the updates are\n",
       "    smaller than ``tol``, the optimization code checks the\n",
       "    dual gap for optimality and continues until it is smaller\n",
       "    than ``tol``.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "positive : bool, default=False\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    The seed of the pseudo random number generator that selects a random\n",
       "    feature to update. Used when ``selection`` == 'random'.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "selection : {'cyclic', 'random'}, default='cyclic'\n",
       "    If set to 'random', a random coefficient is updated every iteration\n",
       "    rather than looping over features sequentially by default. This\n",
       "    (setting to 'random') often leads to significantly faster convergence\n",
       "    especially when tol is higher than 1e-4.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    Parameter vector (w in the cost function formula).\n",
       "\n",
       "sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
       "    Sparse representation of the `coef_`.\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    Independent term in decision function.\n",
       "\n",
       "n_iter_ : list of int\n",
       "    Number of iterations run by the coordinate descent solver to reach\n",
       "    the specified tolerance.\n",
       "\n",
       "dual_gap_ : float or ndarray of shape (n_targets,)\n",
       "    Given param alpha, the dual gaps at the end of the optimization,\n",
       "    same shape as each observation of y.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ElasticNetCV : Elastic net model with best model selection by\n",
       "    cross-validation.\n",
       "SGDRegressor : Implements elastic net regression with incremental training.\n",
       "SGDClassifier : Implements logistic regression with elastic net penalty\n",
       "    (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "To avoid unnecessary memory duplication the X argument of the fit method\n",
       "should be directly passed as a Fortran-contiguous numpy array.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.linear_model import ElasticNet\n",
       ">>> from sklearn.datasets import make_regression\n",
       "\n",
       ">>> X, y = make_regression(n_features=2, random_state=0)\n",
       ">>> regr = ElasticNet(random_state=0)\n",
       ">>> regr.fit(X, y)\n",
       "ElasticNet(random_state=0)\n",
       ">>> print(regr.coef_)\n",
       "[18.83816048 64.55968825]\n",
       ">>> print(regr.intercept_)\n",
       "1.451...\n",
       ">>> print(regr.predict([[0, 0]]))\n",
       "[1.451...]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Lasso\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ElasticNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71c0650c-608a-4ee5-8f92-b46c0addbad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of Lasso-final:\n",
      "Train-MSE: 0.214485, Test-MSE: 0.203406\n"
     ]
    }
   ],
   "source": [
    "# 弹性网\n",
    "from sklearn.linear_model import ElasticNet\n",
    "ElasticNet_reg = ElasticNet(alpha=0.1, l1_ratio=0.9).fit(train_oof.iloc[:, :3], y_train)\n",
    "ElasticNet_train_prediction = ElasticNet_reg.predict(train_oof.iloc[:, :3])\n",
    "ElasticNet_test_prediction = ElasticNet_reg.predict(test_predict)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(ElasticNet_train_prediction, y_train), \n",
    "                                       mean_squared_error(ElasticNet_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d11e29ba-4b41-4c45-8223-e28995370519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of Lasso-final:\n",
      "Train-MSE: 0.205773, Test-MSE: 0.193204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "ElasticNet_reg = ElasticNet(alpha=0.01, l1_ratio=0.99).fit(train_oof.iloc[:, :3], y_train)\n",
    "ElasticNet_train_prediction = ElasticNet_reg.predict(train_oof.iloc[:, :3])\n",
    "ElasticNet_test_prediction = ElasticNet_reg.predict(test_predict)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(ElasticNet_train_prediction, y_train), \n",
    "                                       mean_squared_error(ElasticNet_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a5043-17b5-4856-9bcd-996ac58b518a",
   "metadata": {},
   "source": [
    "不过我们发现，无论如何调整，这一组模型的预测效果都不会超过线性回归（这也是模型原理导致）。因此在大多数情况下，我们都不会考虑采用这些模型作为元学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33feb938-f15e-449b-bd98-c54ff97b1a0c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而与此不同的是，贝叶斯回归则是另一个备选的可以用于回归问题Stacking的元学习器。和线性回归通过最下二乘法求解线性方程系数不同，贝叶斯回归是通过贝叶斯推断来进行线性方程系数。考虑到该求解过程较为复杂，且其原理中提供的超参数几乎对建模结果没有任何影响，外加贝叶斯回归本身预测效力很弱（和线性回归旗鼓相当），因此课上并不会对其原理进行深入介绍，对其原理感兴趣的同学可以参看《模式识别与机器学习（PRML）》第三章第三节的贝叶斯线性回归部分内容讲解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3db7b834-3267-4f24-8c28-5f9cf8a250dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mBayesianRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlambda_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlambda_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlambda_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompute_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Bayesian ridge regression.\n",
       "\n",
       "Fit a Bayesian ridge model. See the Notes section for details on this\n",
       "implementation and the optimization of the regularization parameters\n",
       "lambda (precision of the weights) and alpha (precision of the noise).\n",
       "\n",
       "Read more in the :ref:`User Guide <bayesian_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_iter : int, default=300\n",
       "    Maximum number of iterations. Should be greater than or equal to 1.\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    Stop the algorithm if w has converged.\n",
       "\n",
       "alpha_1 : float, default=1e-6\n",
       "    Hyper-parameter : shape parameter for the Gamma distribution prior\n",
       "    over the alpha parameter.\n",
       "\n",
       "alpha_2 : float, default=1e-6\n",
       "    Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
       "    Gamma distribution prior over the alpha parameter.\n",
       "\n",
       "lambda_1 : float, default=1e-6\n",
       "    Hyper-parameter : shape parameter for the Gamma distribution prior\n",
       "    over the lambda parameter.\n",
       "\n",
       "lambda_2 : float, default=1e-6\n",
       "    Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
       "    Gamma distribution prior over the lambda parameter.\n",
       "\n",
       "alpha_init : float, default=None\n",
       "    Initial value for alpha (precision of the noise).\n",
       "    If not set, alpha_init is 1/Var(y).\n",
       "\n",
       "        .. versionadded:: 0.22\n",
       "\n",
       "lambda_init : float, default=None\n",
       "    Initial value for lambda (precision of the weights).\n",
       "    If not set, lambda_init is 1.\n",
       "\n",
       "        .. versionadded:: 0.22\n",
       "\n",
       "compute_score : bool, default=False\n",
       "    If True, compute the log marginal likelihood at each iteration of the\n",
       "    optimization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to calculate the intercept for this model.\n",
       "    The intercept is not treated as a probabilistic parameter\n",
       "    and thus has no associated variance. If set\n",
       "    to False, no intercept will be used in calculations\n",
       "    (i.e. data is expected to be centered).\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        ``normalize`` was deprecated in version 1.0 and will be removed in\n",
       "        1.2.\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If True, X will be copied; else, it may be overwritten.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Verbose mode when fitting the model.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : array-like of shape (n_features,)\n",
       "    Coefficients of the regression model (mean of distribution)\n",
       "\n",
       "intercept_ : float\n",
       "    Independent term in decision function. Set to 0.0 if\n",
       "    ``fit_intercept = False``.\n",
       "\n",
       "alpha_ : float\n",
       "   Estimated precision of the noise.\n",
       "\n",
       "lambda_ : float\n",
       "   Estimated precision of the weights.\n",
       "\n",
       "sigma_ : array-like of shape (n_features, n_features)\n",
       "    Estimated variance-covariance matrix of the weights\n",
       "\n",
       "scores_ : array-like of shape (n_iter_+1,)\n",
       "    If computed_score is True, value of the log marginal likelihood (to be\n",
       "    maximized) at each iteration of the optimization. The array starts\n",
       "    with the value of the log marginal likelihood obtained for the initial\n",
       "    values of alpha and lambda and ends with the value obtained for the\n",
       "    estimated alpha and lambda.\n",
       "\n",
       "n_iter_ : int\n",
       "    The actual number of iterations to reach the stopping criterion.\n",
       "\n",
       "X_offset_ : float\n",
       "    If `normalize=True`, offset subtracted for centering data to a\n",
       "    zero mean.\n",
       "\n",
       "X_scale_ : float\n",
       "    If `normalize=True`, parameter used to scale data to a unit\n",
       "    standard deviation.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ARDRegression : Bayesian ARD regression.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "There exist several strategies to perform Bayesian ridge regression. This\n",
       "implementation is based on the algorithm described in Appendix A of\n",
       "(Tipping, 2001) where updates of the regularization parameters are done as\n",
       "suggested in (MacKay, 1992). Note that according to A New\n",
       "View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n",
       "update rules do not guarantee that the marginal likelihood is increasing\n",
       "between two consecutive iterations of the optimization.\n",
       "\n",
       "References\n",
       "----------\n",
       "D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n",
       "Vol. 4, No. 3, 1992.\n",
       "\n",
       "M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n",
       "Journal of Machine Learning Research, Vol. 1, 2001.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import linear_model\n",
       ">>> clf = linear_model.BayesianRidge()\n",
       ">>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
       "BayesianRidge()\n",
       ">>> clf.predict([[1, 1]])\n",
       "array([1.])\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BayesianRidge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf67f2e-371a-4e69-a16e-5efe8cbddf1c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里需要注意，sklearn中的贝叶斯回归是提供了四个超参数选项的，分别是alpha_1、alpha_2、lambda_1、lambda_2，默认超参数取值都是1e-6。而由于Python本身计算精度的问题，建模过程中这四个超参数对模型的影响非常不可控，甚至无法通过超参数优化寻找到最有超参数组，例如默认参数情况下，贝叶斯回归元学习器建模效果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "202ca67c-d252-476c-bfda-3a0a194bc05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of BayesianRidge-final:\n",
      "Train-MSE: 0.205622, Test-MSE: 0.192801\n"
     ]
    }
   ],
   "source": [
    "# 贝叶斯回归\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "BayesianRidge_reg = BayesianRidge().fit(train_oof.iloc[:, :3], y_train)\n",
    "BayesianRidge_train_prediction = BayesianRidge_reg.predict(train_oof.iloc[:, :3])\n",
    "BayesianRidge_test_prediction = BayesianRidge_reg.predict(test_predict)\n",
    "print('The results of BayesianRidge-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(BayesianRidge_train_prediction, y_train), \n",
    "                                       mean_squared_error(BayesianRidge_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c752c-3474-4c4a-8572-ec2d95064976",
   "metadata": {},
   "source": [
    "而如果对其进行超参数搜索，得到结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "de6afc69-4dc2-4a35-bf6c-bc2ca863a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "BR_space = {'alpha_1': hp.uniform('alpha_1', 1e-6, 1e-2), \n",
    "            'alpha_2': hp.uniform('alpha_2', 1e-6, 1e-2), \n",
    "            'lambda_1': hp.uniform('lambda_1', 1e-6, 1e-2), \n",
    "            'lambda_2': hp.uniform('lambda_2', 1e-6, 1e-2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fd8d99cb-b3c2-4653-a393-7c0b45370a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BR_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    alpha_1 = params['alpha_1']\n",
    "    alpha_2 = params['alpha_2']\n",
    "    lambda_1 = params['lambda_1']\n",
    "    lambda_2 = params['lambda_2']\n",
    "        \n",
    "    # 模型创建\n",
    "    BayesianRidge_reg = BayesianRidge(n_iter=200000, \n",
    "                                      alpha_1=alpha_1, \n",
    "                                      alpha_2=alpha_2, \n",
    "                                      lambda_1=lambda_1, \n",
    "                                      lambda_2=lambda_2)\n",
    "    if train == True:\n",
    "        res = -cross_val_score(BayesianRidge_reg, \n",
    "                               train_oof.iloc[:, :3], \n",
    "                               y_train, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               n_jobs=15).mean()\n",
    "    else:\n",
    "        res = BayesianRidge_reg.fit(train_oof.iloc[:, :3], y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c7607b3f-0ccd-4c73-893b-767a86aa0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BR_param_search(max_evals=500):\n",
    "    params_best = fmin(BR_param_objective,\n",
    "                       space = BR_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "34cb8173-71d8-4b0f-a301-55a3a7c01da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 2000/2000 [00:55<00:00, 35.79trial/s, best loss: 0.20591380724221203]\n"
     ]
    }
   ],
   "source": [
    "BR_best_param = BR_param_search(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5071853-d82e-474d-bd3f-fc3258c8a630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha_1': 0.00781092764422319,\n",
       " 'alpha_2': 0.00919958807547112,\n",
       " 'lambda_1': 0.009997955393171507,\n",
       " 'lambda_2': 1.5458991132666618e-06}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BR_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3df4d072-7bd6-47c4-9a4b-3e3970e6b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BR_reg = BR_param_objective(BR_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7dcbf4aa-7457-445b-b365-175462001b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2056222234194353, 0.19280159602984515)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(BR_reg.predict(train_oof.iloc[:, :3]), y_train), mean_squared_error(BR_reg.predict(test_predict), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d91296-ebfb-4e4f-8d03-498b2f279049",
   "metadata": {},
   "source": [
    "结果几乎没有任何变化，该情况其实是贝叶斯回归建模过程中的一般情况。因此，大多数时候，当我们采用线性回归和贝叶斯回归作为元学习器进行Stacking时，并不需要要进行任何形式的超参数调整（线性回归是因为没有超参数、而贝叶斯回归则是因为超参数优化效果不大）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffd945-33cb-4734-a069-cd129cc1ba2a",
   "metadata": {},
   "source": [
    "> 稍微拓展一些，其实贝叶斯回归模型超参数优化无效的原因，是在于对于同一个数据集，不同的超参数取值有可能会导向相同的训练集结果，从而让超参数优化函数失去判断的标准，哪怕这些不同取值的超参数会对测试集产生不同的影响，但由于训练集上表现相同，因此优化函数是无法判断哪组超参数最优的，因此优化效果很不稳定。这个是贝叶斯回归存在的问题，但贝叶斯回归也有自己的优势，相比线性回归，贝叶斯回归不会因为共线性问题导致模型效果下降，因此整体表现比线性回归更加稳健。不过在模型融合环节，还是建议两个模型都建模、多次输出结果再从中筛选。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c9be9-a57c-49ba-bb20-182db6ef23fb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而如果是更复杂的树模型或者是集成算法，作为元学习器则会不可避免的出现过拟合的问题，并且和分类问题类似，这种过拟合问题是无法通过超参数优化来解决的。以决策树模型为例，超参数优化过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b91ebdf9-e9af-48b1-acdd-5002154c1147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of tree_reg-final:\n",
      "Train-MSE: 0.000000, Test-MSE: 0.385108\n"
     ]
    }
   ],
   "source": [
    "# 决策树回归\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "tree_train_prediction = tree_reg.predict(train_oof.iloc[:, :3])\n",
    "tree_test_prediction = tree_reg.predict(test_predict)\n",
    "print('The results of tree_reg-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(tree_train_prediction, y_train), \n",
    "                                       mean_squared_error(tree_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "951e84bd-0ead-4683-ba03-0e579d964043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'squared_error',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'random_state': None,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb53331-8d74-4d92-a764-6498fd3a00cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeRegressor(), n_jobs=12,\n",
       "             param_grid={'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                       14, 15],\n",
       "                         'max_leaf_nodes': [6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n",
       "                                            16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "                                            25, 26, 27, 28, 29],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [1, 2, 3, 4]})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化决策树评估器\n",
    "tree_final = DecisionTreeRegressor()\n",
    "\n",
    "tree_param = {'max_depth': np.arange(2, 16, 1).tolist(), \n",
    "              'min_samples_split': np.arange(1, 5, 1).tolist(), \n",
    "              'min_samples_leaf': np.arange(1, 4, 1).tolist(), \n",
    "              'max_leaf_nodes':np.arange(6, 30, 1).tolist()}\n",
    "\n",
    "# 实例化网格搜索评估器\n",
    "tfg = GridSearchCV(estimator = tree_final,\n",
    "                   param_grid = tree_param,\n",
    "                   n_jobs = 12)\n",
    "\n",
    "tfg.fit(train_oof.iloc[:, :3], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a0b459b-1387-4787-b85b-dfcbbdd3c043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5,\n",
       " 'max_leaf_nodes': 29,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9434daa8-d3ee-49e5-b80a-56a0fdc27d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of tree_reg-final:\n",
      "Train-MSE: 0.203716, Test-MSE: 0.200287\n"
     ]
    }
   ],
   "source": [
    "tree_train_prediction = tfg.predict(train_oof.iloc[:, :3])\n",
    "tree_test_prediction = tfg.predict(test_predict)\n",
    "print('The results of tree_reg-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(tree_train_prediction, y_train), \n",
    "                                       mean_squared_error(tree_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198822d3-25a2-4920-b1d9-c18ceb532f96",
   "metadata": {},
   "source": [
    "能够发现，通过超参数优化，尽管可以限制过拟合问题，但效果不如线性回归或者贝叶斯回归。除了树模型外，其他集成学习也类似，这里就不一一进行举例了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e182b1-82b3-484f-8cb3-2bb7dc307400",
   "metadata": {},
   "source": [
    "| 元学习器 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>线性回归 | <center>0.205622 | <center>0.192784 |\n",
    "| <center>贝叶斯回归 | <center>0.205622 | <center>0.192801 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185f265-c2f0-4954-b333-fb90e8999aaa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过上述实验，总的结论是，在回归问题的Stacking融合的元学习器选择上，优先选择线性回归和贝叶斯回归，只有在极少数的超大规模的模型融合的情况下，才会考虑更加复杂的模型作为元学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b6014-0de6-4bb3-9c55-c84d4128f234",
   "metadata": {},
   "source": [
    "- 线性方程元学习器与加权平均融合比较"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b83c1-d71d-4df4-9257-266f8ce935ca",
   "metadata": {},
   "source": [
    "&emsp;&emsp;实际上，基于交叉训练的加权平均融合和以线性方程作为元学习器的Stacking融合本质上非常类似，都是利用train_oof的特征乘以某种系数作为最终预测结果。只不过线性回归和贝叶斯回归允许在加权求和后增加一个常数项。例如在当前示例中，线性回归元学习器最终得到方程系数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bafb6219-6356-4cf6-9c6d-97dbcc7f6169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06107945,  0.35375414,  0.73250402])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44eb201-fc0f-48f8-88ac-1f9c861eed10",
   "metadata": {},
   "source": [
    "常数项如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bdde220-6a32-4a83-a30e-93bf602895cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05678572469977139"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25364a21-9d77-4cc1-bbd1-ee9d1db8e4c3",
   "metadata": {},
   "source": [
    "当然，贝叶斯回归也是类似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cdf73c2-fd8b-457c-af38-31978e69662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05900869,  0.35252576,  0.7317419 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BayesianRidge_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1de8840f-a6eb-45f7-8966-45846eb1818a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.056949861828635484"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BayesianRidge_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47a272-9f56-4415-b942-dd9371168989",
   "metadata": {},
   "source": [
    "而实际上的融合过程，就是以这些系数为权重，进行加权求和，并加上常数项，这里以线性回归为例，算出最终预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9338f204-2afb-4b52-8cfb-e326465b7c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.963588\n",
       "1        1.306244\n",
       "2        2.601891\n",
       "3        4.774066\n",
       "4        2.536553\n",
       "           ...   \n",
       "16507    2.229541\n",
       "16508    1.412220\n",
       "16509    1.064653\n",
       "16510    2.231537\n",
       "16511    2.022835\n",
       "Length: 16512, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_oof.iloc[:, :3] * lr_reg.coef_).sum(1) + lr_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "981ab356-d394-4b43-9ed4-fc57b3cf6f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.96358772, 1.30624403, 2.60189094, ..., 1.06465319, 2.23153731,\n",
       "       2.02283499])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reg.predict(train_oof.iloc[:, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02833a2d-2630-4a92-ba2a-8d33bfbbd559",
   "metadata": {},
   "source": [
    "而此前介绍的加权平均融合，完全可以看成不带常数项的线性回归元学习器下的Stacking过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "223b0d79-89ab-4637-952c-1cd655c75bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.23409605e-06, 5.00041271e-02, 9.98934894e-01])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58d09c-5107-4da9-828d-964e97e945ea",
   "metadata": {},
   "source": [
    "此时“线性回归”的系数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b5e168e-9f1a-471d-8ebd-32e41586f1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17651701e-06, 4.76710917e-02, 9.52327732e-01])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param / best_param.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2748ffce-148d-45be-a01d-7adfad04f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_norm = best_param / best_param.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7c80d08-0d4b-4299-90a4-ab439b52797e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3.000826\n",
       "1        1.301808\n",
       "2        2.532426\n",
       "3        4.776642\n",
       "4        2.466820\n",
       "           ...   \n",
       "16507    2.197079\n",
       "16508    1.426716\n",
       "16509    1.089502\n",
       "16510    2.263913\n",
       "16511    1.974831\n",
       "Length: 16512, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_oof.iloc[:, :3] * best_param_norm).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67d4c4-e0b1-472f-9519-de7b12760ee4",
   "metadata": {},
   "source": [
    "正是因为模型方程本身结构差异不大，导致二者结果往往也不会有非常本质的差异："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d4d0ab3-9ee9-4409-a579-2bbd5907e132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19278440531899835"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性回归结果\n",
    "mean_squared_error(lr_test_prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f46575f-cc8e-43db-a43e-e44e08b0a6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19170078526754764"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加权平均融合结果\n",
    "mean_squared_error((test_predict * best_param_norm).sum(1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1928283-97ac-405d-bae2-e34dde5c3b72",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <font color=\"red\"><center>**0.1980** |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |\n",
    "| <center>指数梯度权重 | <center>0.0209 | <font color=\"red\"><center>**0.1963** |\n",
    "| <center>基于交叉验证的TPE搜索（50次迭代） | <center>0.0169 | <center>0.1957 |\n",
    "| <center>基于交叉验证的TPE搜索（100次迭代） | <center>0.0140 | <font color=\"red\"><center>**0.1953** |\n",
    "| <center>空间裁剪的TPE搜索（50次迭代） | <center>0.0180 | <center>0.1956 |\n",
    "| <center>交叉训练的TPE搜索（200次迭代） | <center>0.2066 | <center>0.19333 |\n",
    "| <center>交叉训练的TPE搜索（2000次迭代） | <center>0.2066 | <center>0.19301 |\n",
    "| <center>交叉训练的TPE搜索（10000次迭代） | <center>0.2066 | <center>0.19300 |\n",
    "| <center>交叉训练+空间裁剪TPE搜索（2000次迭代） |<center>0.2066 | <font color=\"red\"><center>**0.1927** |\n",
    "| <center>Stacking+LR | <center>0.2056 | <center>0.1927 |\n",
    "| <center>Stacking+BR | <center>0.2056 | <center>0.1928 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec05b2-66fd-41ed-a2f5-4b5aa95815e7",
   "metadata": {},
   "source": [
    "- 元学习器优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8a697-ff29-4e4d-a9e4-12137837d272",
   "metadata": {},
   "source": [
    "&emsp;&emsp;正式因为很多时候线性回归和贝叶斯回归作为元学习器，并不能和加权平均融合形成算法结构层面的差异，并且由于线性回归和贝叶斯回归没有超参数优化的必要，因此回归问题的Stacking融合的关键，就在于能否借助Bagging对其进行元学习器的优化。具体执行过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f33ec-4c21-4f8a-b491-1652a18ca2f9",
   "metadata": {},
   "source": [
    "首先查看默认超参数下Bagging预测效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5efcf2a2-6e5c-4093-a16b-a9c174c9aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of GBR-final:\n",
      "Train-MSE: 0.044133, Test-MSE: 0.229778\n"
     ]
    }
   ],
   "source": [
    "bagging_reg = BaggingRegressor().fit(train_oof.iloc[:, :3], y_train)\n",
    "bagging_train_prediction = bagging_reg.predict(train_oof.iloc[:, :3])\n",
    "bagging_test_prediction = bagging_reg.predict(test_predict)\n",
    "print('The results of GBR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(bagging_train_prediction, y_train), \n",
    "                                       mean_squared_error(bagging_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7bcbcd88-e26b-4406-b59d-d3bd94edf7af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mBaggingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbootstrap_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A Bagging regressor.\n",
       "\n",
       "A Bagging regressor is an ensemble meta-estimator that fits base\n",
       "regressors each on random subsets of the original dataset and then\n",
       "aggregate their individual predictions (either by voting or by averaging)\n",
       "to form a final prediction. Such a meta-estimator can typically be used as\n",
       "a way to reduce the variance of a black-box estimator (e.g., a decision\n",
       "tree), by introducing randomization into its construction procedure and\n",
       "then making an ensemble out of it.\n",
       "\n",
       "This algorithm encompasses several works from the literature. When random\n",
       "subsets of the dataset are drawn as random subsets of the samples, then\n",
       "this algorithm is known as Pasting [1]_. If samples are drawn with\n",
       "replacement, then the method is known as Bagging [2]_. When random subsets\n",
       "of the dataset are drawn as random subsets of the features, then the method\n",
       "is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
       "on subsets of both samples and features, then the method is known as\n",
       "Random Patches [4]_.\n",
       "\n",
       "Read more in the :ref:`User Guide <bagging>`.\n",
       "\n",
       ".. versionadded:: 0.15\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "base_estimator : object, default=None\n",
       "    The base estimator to fit on random subsets of the dataset.\n",
       "    If None, then the base estimator is a\n",
       "    :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
       "\n",
       "n_estimators : int, default=10\n",
       "    The number of base estimators in the ensemble.\n",
       "\n",
       "max_samples : int or float, default=1.0\n",
       "    The number of samples to draw from X to train each base estimator (with\n",
       "    replacement by default, see `bootstrap` for more details).\n",
       "\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max_samples * X.shape[0]` samples.\n",
       "\n",
       "max_features : int or float, default=1.0\n",
       "    The number of features to draw from X to train each base estimator (\n",
       "    without replacement by default, see `bootstrap_features` for more\n",
       "    details).\n",
       "\n",
       "    - If int, then draw `max_features` features.\n",
       "    - If float, then draw `max_features * X.shape[1]` features.\n",
       "\n",
       "bootstrap : bool, default=True\n",
       "    Whether samples are drawn with replacement. If False, sampling\n",
       "    without replacement is performed.\n",
       "\n",
       "bootstrap_features : bool, default=False\n",
       "    Whether features are drawn with replacement.\n",
       "\n",
       "oob_score : bool, default=False\n",
       "    Whether to use out-of-bag samples to estimate\n",
       "    the generalization error. Only available if bootstrap=True.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit\n",
       "    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel for both :meth:`fit` and\n",
       "    :meth:`predict`. ``None`` means 1 unless in a\n",
       "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
       "    processors. See :term:`Glossary <n_jobs>` for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the random resampling of the original dataset\n",
       "    (sample wise and feature wise).\n",
       "    If the base estimator accepts a `random_state` attribute, a different\n",
       "    seed is generated for each instance in the ensemble.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "base_estimator_ : estimator\n",
       "    The base estimator from which the ensemble is grown.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when :meth:`fit` is performed.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "estimators_ : list of estimators\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "estimators_samples_ : list of arrays\n",
       "    The subset of drawn samples (i.e., the in-bag samples) for each base\n",
       "    estimator. Each subset is defined by an array of the indices selected.\n",
       "\n",
       "estimators_features_ : list of arrays\n",
       "    The subset of drawn features for each base estimator.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_prediction_ : ndarray of shape (n_samples,)\n",
       "    Prediction computed with out-of-bag estimate on the training\n",
       "    set. If n_estimators is small it might be possible that a data point\n",
       "    was never left out during the bootstrap. In this case,\n",
       "    `oob_prediction_` might contain NaN. This attribute exists only\n",
       "    when ``oob_score`` is True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "BaggingClassifier : A Bagging classifier.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       ".. [1] L. Breiman, \"Pasting small votes for classification in large\n",
       "       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
       "\n",
       ".. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
       "       1996.\n",
       "\n",
       ".. [3] T. Ho, \"The random subspace method for constructing decision\n",
       "       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
       "       1998.\n",
       "\n",
       ".. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
       "       Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.svm import SVR\n",
       ">>> from sklearn.ensemble import BaggingRegressor\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> X, y = make_regression(n_samples=100, n_features=4,\n",
       "...                        n_informative=2, n_targets=1,\n",
       "...                        random_state=0, shuffle=False)\n",
       ">>> regr = BaggingRegressor(base_estimator=SVR(),\n",
       "...                         n_estimators=10, random_state=0).fit(X, y)\n",
       ">>> regr.predict([[0, 0, 0, 0]])\n",
       "array([-2.8720...])\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BaggingRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6237a25-df9a-4a3f-ac22-cc60df1d23d3",
   "metadata": {},
   "source": [
    "然后将默认评估器改为线性回归，输出结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b77ea177-1bf0-4175-a301-81e57da8caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5333952903747559\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 设置超参数空间\n",
    "parameter_space = {\n",
    "    \"n_estimators\": range(10, 21), \n",
    "    \"max_samples\": np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "# 实例化模型与评估器\n",
    "bagging_final = BaggingRegressor(base_estimator=LinearRegression(), random_state=21)\n",
    "\n",
    "BG = GridSearchCV(bagging_final, parameter_space, n_jobs=15)\n",
    "\n",
    "# 模型训练\n",
    "BG.fit(train_oof.iloc[:, :3], y_train)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "76ed7e42-ff35-459d-8b29-310f53c09b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_samples': 0.9, 'n_estimators': 10}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e84b2188-4748-4e4f-9f72-4737eeb8a31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8442282084464816"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ec7c9524-3b55-4c07-8569-b50d7afd11cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20562645147601102, 0.19285269873019006)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(BG.predict(train_oof.iloc[:, :3]), y_train), mean_squared_error(BG.predict(test_predict), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56270d6-6972-4c28-832f-9e2644d65a6a",
   "metadata": {},
   "source": [
    "对比结果不难看出，相比原始Bagging，效果有了明显提升。不过相比先线性回归或者贝叶斯回归，还有一定的差距。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59412fc-6080-4968-a1b1-47cdfe41bd62",
   "metadata": {},
   "source": [
    "| 元学习器 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>线性回归 | <center>0.205622 | <center>0.192784 |\n",
    "| <center>贝叶斯回归 | <center>0.205622 | <center>0.192801 |\n",
    "| <center>Bagging | <center>0.044133 | <center>0.229778 |    \n",
    "| <center>Bagging+lr | <center>0.205626 | <center>0.192852 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6032c80-9d04-46fd-a9af-dca598b0e738",
   "metadata": {},
   "source": [
    "接下来继续考虑以贝叶斯回归作为基础分类器，带入Bagging过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "82ce4fd5-0513-4ed6-ba2d-b9b00dbfc740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8406734466552734\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 设置超参数空间\n",
    "parameter_space = {\n",
    "    \"n_estimators\": range(10, 21), \n",
    "    \"max_samples\": np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "# 实例化模型与评估器\n",
    "bagging_final = BaggingRegressor(base_estimator=BayesianRidge(), random_state=1)\n",
    "\n",
    "BG = GridSearchCV(bagging_final, parameter_space, n_jobs=15)\n",
    "\n",
    "# 模型训练\n",
    "BG.fit(train_oof.iloc[:, :3], y_train)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f2b8fc71-ccf5-406a-9d7a-ddcd45259b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_samples': 1.0, 'n_estimators': 19}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e30605cc-c641-4be7-8abc-9f6c4d8cee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844191916402167"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7ad438ff-bb0a-4510-9403-d6b179e2a3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.205627607165818, 0.19259294571070013)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(BG.predict(train_oof.iloc[:, :3]), y_train), mean_squared_error(BG.predict(test_predict), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfee4d-3ae6-4749-b25c-8847e0e943ba",
   "metadata": {},
   "source": [
    "而进一步对比加权平均法，当前数据集下Stacking融合得到了一个更好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727db3f-ad94-433e-8c6e-3ef111db4759",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <font color=\"red\"><center>0.1980 |\n",
    "| <center>平均融合法 | <center>0.0109 | <center>0.2049 |\n",
    "| <center>倍数梯度权重 | <center>0.0106 | <center>0.1989 |\n",
    "| <center>指数梯度权重 | <center>0.0209 | <font color=\"red\"><center>0.1963 |\n",
    "| <center>基于交叉验证的TPE搜索（50次迭代） | <center>0.0169 | <center>0.1957 |\n",
    "| <center>基于交叉验证的TPE搜索（100次迭代） | <center>0.0140 | <font color=\"red\"><center>0.1953 |\n",
    "| <center>空间裁剪的TPE搜索（50次迭代） | <center>0.0180 | <center>0.1956 |\n",
    "| <center>基于交叉训练的TPE搜索（1万次迭代） | <center>0.2066 | <font color=\"red\"><center>0.1927 |\n",
    "| <center>Stacking线性回归 | <center>0.205622 | <center>0.192784 |\n",
    "| <center>Stacking贝叶斯回归 | <center>0.205622 | <center>0.192801 |\n",
    "| <center>Stacking Bagging | <center>0.044133 | <center>0.229778 |    \n",
    "| <center>Stacking Bagging+LR | <center>0.205626 | <center>0.192852 |\n",
    "| <center>Stacking Bagging+BR | <center>0.205627 | <font color=\"red\"><center>**0.192592** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17176a1-1604-4b57-9f91-c9782ce5a361",
   "metadata": {},
   "source": [
    "#### 2.2 Blending融合法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8679340-e1a7-47cc-9a69-97e159687d7a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，继续尝试Blending融合。相比Stacking，Blending的核心区别在于留出集的划分，以达到一级学习器和元学习器数据隔离的目的，从而进一步抑制融合过程过拟合问题。这里首先回顾Blending融合原理如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5b09d-9b84-4071-bac0-6decd79e854a",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/Blending模型融合流程 (2).jpeg\" alt=\"Blending模型融合流程 (2)\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de62a2-c240-4186-a508-3d2fefecfb02",
   "metadata": {},
   "source": [
    "同时，尽管Blending本身是作为Stacking的优化策略而诞生，但由于留出集的划分会牺牲一级学习器的训练数据，因此并不是每个Blending的过程都能够的到比Stacking更好的结果。并且留出集的划分方式，也成为决定Blending融合成败的关键因素。关于回归问题的Blending融合优化，我们会在下一小节进行详细探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717ee38-9723-4254-b416-5d3c589b6d74",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来尝试手动实现Blending全流程。首先是数据集的划分，这里还是按照一般情况，按照8：2的比例进行留出集的划分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0ba00a5e-ee12-4526-bd36-6f1a1e35c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train, y_train,  test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c6cbfb34-6ebe-459e-a26e-a788a3641a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13620</th>\n",
       "      <td>4.8750</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.168022</td>\n",
       "      <td>1.078591</td>\n",
       "      <td>921.0</td>\n",
       "      <td>2.495935</td>\n",
       "      <td>34.10</td>\n",
       "      <td>-118.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303</th>\n",
       "      <td>2.4196</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.518248</td>\n",
       "      <td>2.700730</td>\n",
       "      <td>253.0</td>\n",
       "      <td>1.846715</td>\n",
       "      <td>37.68</td>\n",
       "      <td>-122.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>2.9926</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.634490</td>\n",
       "      <td>1.046638</td>\n",
       "      <td>2946.0</td>\n",
       "      <td>3.195228</td>\n",
       "      <td>33.74</td>\n",
       "      <td>-117.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>0.7403</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4.491429</td>\n",
       "      <td>1.148571</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>2.988571</td>\n",
       "      <td>37.96</td>\n",
       "      <td>-122.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>2.8882</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.610390</td>\n",
       "      <td>1.006494</td>\n",
       "      <td>550.0</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>38.72</td>\n",
       "      <td>-121.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "13620  4.8750      10.0  5.168022   1.078591       921.0  2.495935     34.10   \n",
       "14303  2.4196      26.0  8.518248   2.700730       253.0  1.846715     37.68   \n",
       "4419   2.9926      25.0  4.634490   1.046638      2946.0  3.195228     33.74   \n",
       "1948   0.7403      37.0  4.491429   1.148571      1046.0  2.988571     37.96   \n",
       "2560   2.8882      32.0  4.610390   1.006494       550.0  3.571429     38.72   \n",
       "\n",
       "       Longitude  \n",
       "13620    -118.18  \n",
       "14303    -122.08  \n",
       "4419     -117.91  \n",
       "1948     -122.37  \n",
       "2560     -121.71  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "84a2df17-4246-42c4-9b94-2350c9cb67a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 8)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "66c07946-7133-4183-a3a2-a4485ed215ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13209, 8)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "acebef23-35a9-41b7-9105-1621f9637809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3303, 8)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eda5c278-63ab-4c65-b295-f2d04f3c1ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.004, 2.75 , 1.832, ..., 1.591, 1.266, 1.114])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6da03f-d17b-40c4-8f92-61e8bb1b610e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来来进行模型训练。对于Blending来说，由于训练数据重新划分，因此一级学习器需要重新进行训练。这里还是采用上一小节超参数优化方法对随机森林、极端随机树和GBDT进行超参数搜索，具体实现过程如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d681fd9-ed15-4427-b4ac-7082b6575775",
   "metadata": {},
   "source": [
    "- Blending融合一级学习器训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47577e08-a4b3-4784-bf86-0c39257e0de0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是随机森林模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "86d0bb31-10a3-4df0-826b-0eef59cb929a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto',\n",
       " 'sqrt',\n",
       " 'log2',\n",
       " None,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features_range = [\"auto\", \"sqrt\", \"log2\", None] + np.arange(0.1, 1., 0.1).tolist()\n",
    "max_features_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c47fb093-5b94-4990-80d6-ccc1d53e8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {'max_features': hp.choice('max_features', max_features_range),\n",
    "            'n_estimators': hp.quniform('n_estimators', 20, 700, 1), \n",
    "            'max_samples': hp.uniform('max_samples', 0.2, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1cdb3123-905e-4421-ba5f-5fcd5ef7c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    max_samples = params['max_samples']\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "        \n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(n_estimators = n_estimators, \n",
    "                                   max_samples = max_samples, \n",
    "                                   max_features = max_features,\n",
    "                                   random_state=12)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_RF, X_train1, y_train1, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f15bb9e5-8e23-4cd9-b1d8-e47b70d20f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    params_best = fmin(RF_param_objective,\n",
    "                       space = RF_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87068762-0dd3-443e-9fa2-b33c4df45b3c",
   "metadata": {},
   "source": [
    "然后进行超参数搜索，这里我们尝试100次搜索，得到结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fbf609cf-387f-4fe4-a350-659ce5d5490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [13:08<00:00,  7.89s/trial, best loss: 0.2516114786950027]\n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a1c85b73-6f96-4a97-9a81-ec3f31a17831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 7, 'max_samples': 0.9975624861890051, 'n_estimators': 626.0}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1451d1b4-57a2-490a-978f-b8d41de560cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Blending = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dd9235e2-eb07-463e-84d6-4757c8f6fcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03232190567836679, 0.23142760467841164)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_Blending.predict(X_train1), y_train1), mean_squared_error(RF_Blending.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afbf80-378d-4ba4-88ae-052e698e218a",
   "metadata": {},
   "source": [
    "能够看出，随着训练数据的减少，对模型效果的影响还是非常明显的："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063925c1-4c90-4cac-badd-753b65036891",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <center>0.1980 |\n",
    "| <center>随机森林_Blending | <center>0.0323 | <center>0.2314 |    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e5d8c-4bea-49a6-b33e-3364764de8ef",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来继续训练极端随机树模型，训练过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e347b8b9-88cc-4ccb-a877-856ecc399d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_space = {'max_depth': hp.quniform('max_depth', 2, 50, 1), \n",
    "            'n_estimators': hp.quniform('n_estimators', 20, 700, 1), \n",
    "            'max_features': hp.choice('max_features', max_features_range)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1aecd31-42e8-4c79-b7fb-b34c7b7d2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    max_depth = int(params['max_depth'])\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    \n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "    \n",
    "    # 模型创建\n",
    "    reg_ET = ExtraTreesRegressor(max_depth = max_depth, \n",
    "                                 n_estimators = n_estimators, \n",
    "                                 max_features = max_features, \n",
    "                                 random_state=12)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_ET, X_train1, y_train1, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_ET.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c5de54e7-ca96-4e28-b302-d5bb448a030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_search(max_evals=500):\n",
    "    params_best = fmin(ET_param_objective,\n",
    "                       space = ET_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b8a437df-b3a8-4b98-8858-cf19611ca6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [04:23<00:00,  2.63s/trial, best loss: 0.24416803714988985]\n"
     ]
    }
   ],
   "source": [
    "ET_best_param = ET_param_search(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5cf18ada-f183-43f6-9b5b-e3855feb8c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 32.0, 'max_features': 7, 'n_estimators': 339.0}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "31f7e583-22d6-46b5-95d9-0fc88e333dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_Blending = ET_param_objective(ET_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7f2a28c1-7403-460e-b63e-c0dece602681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(max_depth=32, max_features=0.4, n_estimators=339,\n",
       "                    random_state=12)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad62c6-6ad4-4a14-9a19-0d5cb96454e4",
   "metadata": {},
   "source": [
    "测试模型在训练集和测试集上表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fba3927a-4074-45b3-aad7-c9de04e05cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0003684233856979253, 0.22303701455306532)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_Blending.predict(X_train1), y_train1), mean_squared_error(ET_Blending.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a6976-e538-45b6-b0ec-668a769e02da",
   "metadata": {},
   "source": [
    "同样，极端随机树的模型效果也有所下降："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cefe4-2a07-40dc-beb3-8b4f22e9444a",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <center>0.1980 |\n",
    "| <center>随机森林_Blending | <center>0.0323 | <center>0.2314 |        \n",
    "| <center>极端随机树_Blending | <center>0.0003 | <center>0.2230 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ec710-4377-4819-824a-207b89ffe173",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后进行GBDT模型训练，训练过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e5ccb660-5092-4512-bc48-02ef0813902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_space = {'n_estimators': hp.quniform('n_estimators', 20, 701, 1),\n",
    "             'learning_rate': hp.uniform('learning_rate', 0.02, 0.2),\n",
    "             'subsample': hp.uniform('subsample', 0.1, 1.0),\n",
    "             'max_depth': hp.quniform('max_depth', 2, 20, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b338e313-ffad-4c2a-9323-4cc9c78795b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_objective(params, train=True):\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    learning_rate = params['learning_rate']\n",
    "    subsample = params['subsample']\n",
    "    max_depth = int(params['max_depth'])\n",
    "    \n",
    "    reg_GBR = GradientBoostingRegressor(n_estimators = n_estimators, \n",
    "                                        learning_rate = learning_rate, \n",
    "                                        subsample = subsample, \n",
    "                                        max_depth = max_depth, \n",
    "                                        random_state=12)\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_GBR, X_train1, y_train1, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_GBR.fit(X_train, y_train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4c98f1be-d338-40f5-9b3c-b3560c47089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_search(max_evals=500):\n",
    "    params_best = fmin(GBR_param_objective,\n",
    "                       space = GBR_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6b28477b-0e01-44ed-832f-387adf8c1981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [07:14<00:00,  8.70s/trial, best loss: 0.21898687697414307]\n"
     ]
    }
   ],
   "source": [
    "GBR_best_param = GBR_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6dc0dd13-d193-4e38-9902-d6a353c7248c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.07520948105762229,\n",
       " 'max_depth': 6.0,\n",
       " 'n_estimators': 608.0,\n",
       " 'subsample': 0.57387966677063}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8c8206a6-f615-4a11-ba1e-ef2fd92aad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_Blending = GBR_param_objective(GBR_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3c6bd15e-5fca-487b-a471-40d67628287c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.07520948105762229, max_depth=6,\n",
       "                          n_estimators=608, random_state=12,\n",
       "                          subsample=0.57387966677063)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4f62cbf0-8386-4229-9c16-a34195c6b623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.046335758486162816, 0.2015167616055982)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_Blending.predict(X_train1), y_train1), mean_squared_error(GBR_Blending.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa78b8a-9083-4907-a7a9-bb75f1248ad3",
   "metadata": {},
   "source": [
    "三个模型的模型效果均有所下降，相比全数据集上训练的到的模型，MSE数值均在百分位上有所上升："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb8be-777e-4498-9c76-ca0baa540867",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ |\n",
    "| <center>随机森林_OPT | <center>0.0322 | <center>0.2311 |\n",
    "| <center>极端随机树_OPT | <center>1.37e-07 | <center>0.2225 |\n",
    "| <center>GBDT_OPT | <center>0.0253 | <center>0.1980 |\n",
    "| <center>随机森林_Blending | <center>0.0323 | <center>0.2314 |        \n",
    "| <center>极端随机树_Blending | <center>0.0003 | <center>0.2230 |\n",
    "| <center>GBDT_Blending | <center>0.0463 | <center>0.2015 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ac548-1a8d-4104-8bd9-ea1432b6bca7",
   "metadata": {},
   "source": [
    "- Blending融合元学习器训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ccdca-8e8b-4b9c-a751-f49e6c7cfeaf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来元学习器训练过程，首先我们需要准备元学习器的训练数据。和Stacking不同，Blending的元学习器训练数据是一级学习器在留出集上的预测结果。这里我们创建train_oof_blending数据集如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "976bc1bb-b422-4f46-ab64-df9e25eac312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35470607, 1.63394409, 2.60049043, ..., 1.95473165, 2.86436927,\n",
       "       3.0097876 ])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_Blending.predict(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0bdab842-87c0-461e-8492-01dac1988a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof_blending = pd.DataFrame({'RF_oof_blending': RF_Blending.predict(X_train2), \n",
    "                                   'ET_oof_blending': ET_Blending.predict(X_train2),\n",
    "                                   'GBR_oof_blending': GBR_Blending.predict(X_train2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "784b6fdd-c631-4358-ad0a-b0b18fadbdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_oof_blending</th>\n",
       "      <th>ET_oof_blending</th>\n",
       "      <th>GBR_oof_blending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.354706</td>\n",
       "      <td>1.322524</td>\n",
       "      <td>1.356082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.633944</td>\n",
       "      <td>1.641734</td>\n",
       "      <td>1.727232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.600490</td>\n",
       "      <td>2.605924</td>\n",
       "      <td>2.720413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.376776</td>\n",
       "      <td>1.234060</td>\n",
       "      <td>1.499121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.688415</td>\n",
       "      <td>0.609165</td>\n",
       "      <td>0.784617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>3.573957</td>\n",
       "      <td>3.653165</td>\n",
       "      <td>3.708306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>2.915882</td>\n",
       "      <td>2.909153</td>\n",
       "      <td>2.976229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>1.954732</td>\n",
       "      <td>1.880811</td>\n",
       "      <td>1.829703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>2.864369</td>\n",
       "      <td>2.705647</td>\n",
       "      <td>2.730535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3302</th>\n",
       "      <td>3.009788</td>\n",
       "      <td>3.152261</td>\n",
       "      <td>2.962970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RF_oof_blending  ET_oof_blending  GBR_oof_blending\n",
       "0            1.354706         1.322524          1.356082\n",
       "1            1.633944         1.641734          1.727232\n",
       "2            2.600490         2.605924          2.720413\n",
       "3            1.376776         1.234060          1.499121\n",
       "4            0.688415         0.609165          0.784617\n",
       "...               ...              ...               ...\n",
       "3298         3.573957         3.653165          3.708306\n",
       "3299         2.915882         2.909153          2.976229\n",
       "3300         1.954732         1.880811          1.829703\n",
       "3301         2.864369         2.705647          2.730535\n",
       "3302         3.009788         3.152261          2.962970\n",
       "\n",
       "[3303 rows x 3 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof_blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ecaad2-8f65-4b2d-9115-03ca4b321009",
   "metadata": {},
   "source": [
    "然后通过一级学习器在测试集上的预测结果，创建test_predict_blending数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8b91e69e-0a4d-4597-bd70-5a503c841df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_blending = pd.DataFrame({'RF_oof_blending': RF_Blending.predict(X_test), \n",
    "                                      'ET_oof_blending': ET_Blending.predict(X_test),\n",
    "                                      'GBR_oof_blending': GBR_Blending.predict(X_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5d8ad831-ed7d-439c-819e-77eec173bc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_oof_blending</th>\n",
       "      <th>ET_oof_blending</th>\n",
       "      <th>GBR_oof_blending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.012107</td>\n",
       "      <td>2.119217</td>\n",
       "      <td>2.020528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.023596</td>\n",
       "      <td>1.970096</td>\n",
       "      <td>1.983483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.092524</td>\n",
       "      <td>2.172498</td>\n",
       "      <td>2.025244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.621594</td>\n",
       "      <td>1.528015</td>\n",
       "      <td>1.408382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.152850</td>\n",
       "      <td>1.178118</td>\n",
       "      <td>0.983733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>2.722764</td>\n",
       "      <td>2.673605</td>\n",
       "      <td>2.814638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4124</th>\n",
       "      <td>3.165139</td>\n",
       "      <td>2.821868</td>\n",
       "      <td>3.272724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.905988</td>\n",
       "      <td>0.833379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>4.021703</td>\n",
       "      <td>3.926552</td>\n",
       "      <td>4.247018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>2.028866</td>\n",
       "      <td>2.075985</td>\n",
       "      <td>2.268370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RF_oof_blending  ET_oof_blending  GBR_oof_blending\n",
       "0            2.012107         2.119217          2.020528\n",
       "1            2.023596         1.970096          1.983483\n",
       "2            2.092524         2.172498          2.025244\n",
       "3            1.621594         1.528015          1.408382\n",
       "4            1.152850         1.178118          0.983733\n",
       "...               ...              ...               ...\n",
       "4123         2.722764         2.673605          2.814638\n",
       "4124         3.165139         2.821868          3.272724\n",
       "4125         0.819155         0.905988          0.833379\n",
       "4126         4.021703         3.926552          4.247018\n",
       "4127         2.028866         2.075985          2.268370\n",
       "\n",
       "[4128 rows x 3 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929186b-7880-49ad-910c-093c6333bdc6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行Blending融合的元学习器测试。和Stacking类似，我们还是首先进行大范围模型测试，选择总共十二个回归模型进行测试，测试过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "05445303-5b23-488d-b2ed-e4d50fdecf03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train-MSE: 0.000295, Test-MSE: 0.225926\n",
      "The results of Ridge-final:\n",
      "Train-MSE: 0.000299, Test-MSE: 0.225453\n",
      "The results of Lasso-final:\n",
      "Train-MSE: 0.764305, Test-MSE: 0.914045\n",
      "The results of ElasticNet-final:\n",
      "Train-MSE: 0.299994, Test-MSE: 0.475550\n",
      "The results of BayesianRidge-final:\n",
      "Train-MSE: 0.000295, Test-MSE: 0.225926\n",
      "The results of SVR-final:\n",
      "Train-MSE: 0.002158, Test-MSE: 0.222606\n",
      "The results of tree_reg-final:\n",
      "Train-MSE: 0.000000, Test-MSE: 0.224173\n",
      "The results of GBR-final:\n",
      "Train-MSE: 0.000064, Test-MSE: 0.223413\n",
      "The results of GBR-final:\n",
      "Train-MSE: 0.000049, Test-MSE: 0.223417\n",
      "The results of GBR-final:\n",
      "Train-MSE: 0.004504, Test-MSE: 0.224965\n",
      "The results of GBR-final:\n",
      "Train-MSE: 0.000233, Test-MSE: 0.223113\n",
      "The results of XGB-final:\n",
      "Train-MSE: 0.000043, Test-MSE: 0.224620\n"
     ]
    }
   ],
   "source": [
    "# 线性回归\n",
    "lr_reg = LinearRegression().fit(train_oof_blending, y_train2)\n",
    "lr_train_prediction = lr_reg.predict(train_oof_blending)\n",
    "lr_test_prediction = lr_reg.predict(test_predict_blending)\n",
    "print('The results of LR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(lr_train_prediction, y_train2), \n",
    "                                       mean_squared_error(lr_test_prediction, y_test)))\n",
    "\n",
    "# 岭回归\n",
    "Ridge_reg = Ridge().fit(train_oof_blending, y_train2)\n",
    "Ridge_train_prediction = Ridge_reg.predict(train_oof_blending)\n",
    "Ridge_test_prediction = Ridge_reg.predict(test_predict_blending)\n",
    "print('The results of Ridge-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Ridge_train_prediction, y_train2), \n",
    "                                       mean_squared_error(Ridge_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# LASSO\n",
    "Lasso_reg = Lasso().fit(train_oof_blending, y_train2)\n",
    "Lasso_train_prediction = Lasso_reg.predict(train_oof_blending)\n",
    "Lasso_test_prediction = Lasso_reg.predict(test_predict_blending)\n",
    "print('The results of Lasso-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(Lasso_train_prediction, y_train2), \n",
    "                                       mean_squared_error(Lasso_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# 弹性网\n",
    "ElasticNet_reg = ElasticNet().fit(train_oof_blending, y_train2)\n",
    "ElasticNet_train_prediction = ElasticNet_reg.predict(train_oof_blending)\n",
    "ElasticNet_test_prediction = ElasticNet_reg.predict(test_predict_blending)\n",
    "print('The results of ElasticNet-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(ElasticNet_train_prediction, y_train2), \n",
    "                                       mean_squared_error(ElasticNet_test_prediction, y_test)))\n",
    "\n",
    "# 贝叶斯回归\n",
    "BayesianRidge_reg = BayesianRidge().fit(train_oof_blending, y_train2)\n",
    "BayesianRidge_train_prediction = BayesianRidge_reg.predict(train_oof_blending)\n",
    "BayesianRidge_test_prediction = BayesianRidge_reg.predict(test_predict_blending)\n",
    "print('The results of BayesianRidge-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(BayesianRidge_train_prediction, y_train2), \n",
    "                                       mean_squared_error(BayesianRidge_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# SVR\n",
    "SVR_reg = SVR().fit(train_oof_blending, y_train2)\n",
    "SVR_train_prediction = SVR_reg.predict(train_oof_blending)\n",
    "SVR_test_prediction = SVR_reg.predict(test_predict_blending)\n",
    "print('The results of SVR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(SVR_train_prediction, y_train2), \n",
    "                                       mean_squared_error(SVR_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# 决策树回归\n",
    "tree_reg = DecisionTreeRegressor().fit(train_oof_blending, y_train2)\n",
    "tree_train_prediction = tree_reg.predict(train_oof_blending)\n",
    "tree_test_prediction = tree_reg.predict(test_predict_blending)\n",
    "print('The results of tree_reg-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(tree_train_prediction, y_train2), \n",
    "                                       mean_squared_error(tree_test_prediction, y_test)))\n",
    "\n",
    "\n",
    "# Bagging\n",
    "bagging_reg = BaggingRegressor().fit(train_oof_blending, y_train2)\n",
    "bagging_train_prediction = bagging_reg.predict(train_oof_blending)\n",
    "bagging_test_prediction = bagging_reg.predict(test_predict_blending)\n",
    "print('The results of GBR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(bagging_train_prediction, y_train2), \n",
    "                                       mean_squared_error(bagging_test_prediction, y_test)))\n",
    "\n",
    "# 随机森林\n",
    "RFR = RandomForestRegressor().fit(train_oof_blending, y_train2)\n",
    "RFR_train_prediction = RFR.predict(train_oof_blending)\n",
    "RFR_test_prediction = RFR.predict(test_predict_blending)\n",
    "print('The results of GBR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(RFR_train_prediction, y_train2), \n",
    "                                       mean_squared_error(RFR_test_prediction, y_test)))\n",
    "\n",
    "# AdaBoost\n",
    "ABR = AdaBoostRegressor().fit(train_oof_blending, y_train2)\n",
    "ABR_train_prediction = ABR.predict(train_oof_blending)\n",
    "ABR_test_prediction = ABR.predict(test_predict_blending)\n",
    "print('The results of GBR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(ABR_train_prediction, y_train2), \n",
    "                                       mean_squared_error(ABR_test_prediction, y_test)))\n",
    "\n",
    "# GBDT\n",
    "GBR = GradientBoostingRegressor().fit(train_oof_blending, y_train2)\n",
    "GBR_train_prediction = GBR.predict(train_oof_blending)\n",
    "GBR_test_prediction = GBR.predict(test_predict_blending)\n",
    "print('The results of GBR-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(GBR_train_prediction, y_train2), \n",
    "                                       mean_squared_error(GBR_test_prediction, y_test)))\n",
    "\n",
    "# XGB\n",
    "XGB = XGBRegressor().fit(train_oof_blending, y_train2)\n",
    "XGB_train_prediction = XGB.predict(train_oof_blending)\n",
    "XGB_test_prediction = XGB.predict(test_predict_blending)\n",
    "print('The results of XGB-final:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(XGB_train_prediction, y_train2), \n",
    "                                       mean_squared_error(XGB_test_prediction, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022a51b-d914-49ee-bd75-8ea9838b2ed7",
   "metadata": {},
   "source": [
    "能够发现，虽然线性回归和贝叶斯回归效果仍然较好，但整体效果都有所下降，相比Stacking差距甚远，甚至不如加权平均融合效果。究其原因其实是元学习器过拟合导致。这里能看出，连线性回归都呈现出过拟合问题，说明train_oof_blending对于线性回归来说学习空间很小，外加一级学习器本身效果下降，最终导致Blending整体融合效果下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3ec15-c1d4-4a80-aa4b-8e87f33048be",
   "metadata": {},
   "source": [
    "> 一般来说，在Blending融合过程中，如果一级学习器学习效果下降不大、而元学习器过拟合，则可以给留出集更大的划分比例。但此处情况非常特殊，一级学习器学习效果也下降的非常快，因此基本可以判断Blending融合对当前问题无效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a2318-a0a8-4d02-811c-c25e8cbfe7cf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而在Blending融合整体效果较差的情况下，元学习器优化也很难起到起死回生的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7923490e-031c-4f16-a78d-6f1f7df4700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9558672904968262\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 设置超参数空间\n",
    "parameter_space = {\n",
    "    \"n_estimators\": range(10, 21), \n",
    "    \"max_samples\": np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "# 实例化模型与评估器\n",
    "bagging_final = BaggingRegressor(base_estimator=LinearRegression(), random_state=22)\n",
    "\n",
    "BG = GridSearchCV(bagging_final, parameter_space, n_jobs=15)\n",
    "\n",
    "# 模型训练\n",
    "BG.fit(train_oof_blending, y_train2)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8183a893-2292-4687-b3be-05e6ce2dfdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_samples': 0.6, 'n_estimators': 11}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fc3e161d-113f-4114-bddc-5e2582e2ef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997714009375949"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c24de36c-45b2-460f-b5b6-969430f3607b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00029533441090624546, 0.22602045954083486)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(BG.predict(train_oof_blending), y_train2), mean_squared_error(BG.predict(test_predict_blending), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad100029-62f0-4489-9109-2c7381331e46",
   "metadata": {},
   "source": [
    "然后以贝叶斯回归作为基础分类器，带入Bagging过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "faf9091a-2f04-447f-ab34-d40839090c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1673202514648438\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 设置超参数空间\n",
    "parameter_space = {\n",
    "    \"n_estimators\": range(10, 21), \n",
    "    \"max_samples\": np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "# 实例化模型与评估器\n",
    "bagging_final = BaggingRegressor(base_estimator=BayesianRidge(), random_state=1)\n",
    "\n",
    "BG = GridSearchCV(bagging_final, parameter_space, n_jobs=15)\n",
    "\n",
    "# 模型训练\n",
    "BG.fit(train_oof_blending, y_train2)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "883b5e48-bc3f-4462-af7c-71a8b715fcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_samples': 0.1, 'n_estimators': 10}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6109f595-589f-480c-86f8-889662bbfc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999772111450814"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f53a761b-098f-4131-8b1b-4fadfe72daa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0002955653247170366, 0.22601985525046403)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(BG.predict(train_oof_blending), y_train2), mean_squared_error(BG.predict(test_predict_blending), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d5893-4787-428f-b851-e613edc36a84",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完成了一系列回归问题模型融合实践。从方法大类上来说，回归问题和分类问题并无本质区别，但具体每个方法的实践效果，不同的情况下会有很大差异，对于此前分类问题有效的优化方法、对于当前回归问题则不一定有效果。这其实也是模型融合实践过程中的常态，即没有哪一种方法是能够百分百一定起到优化效果，更多的还是需要我们反复尝试、则有输出，并且，更关键的是要理解这些方法背后的原理，才能灵活调整、不断优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdddc29-4d3a-48a9-a4a2-b10a20316191",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，围绕回归问题的融合方法其实并没有讨论完，下一小节我们将继续探讨进阶优化策略，以及在自动融合函数中添加回归问题的解决方案。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
