{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62527a3a-43f3-47b1-b81b-e5c36a942d4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> 【Kaggle】Telco Customer Churn 电信用户流失预测案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c897f-1edb-48db-89d3-684bf01d8bcb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71700b-a8e2-46d4-9139-8be0c65b9aaa",
   "metadata": {},
   "source": [
    "## <font face=\"仿宋\">第四部分导读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e326e-43e3-47b8-a843-be4bc0693b25",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">在案例的第二、三部分中，我们详细介绍了关于特征工程的各项技术，特征工程技术按照大类来分可以分为数据预处理、特征衍生、特征筛选三部分，其中特征预处理的目的是为了将数据集整理、清洗到可以建模的程度，具体技术包括缺失值处理、异常值处理、数据重编码等，是建模之前必须对数据进行的处理和操作；而特征衍生和特征筛选则更像是一类优化手段，能够帮助模型突破当前数据集建模的效果上界。并且我们在第二部分完整详细的介绍机器学习可解释性模型的训练、优化和解释方法，也就是逻辑回归和决策树模型。并且此前我们也一直以这两种算法为主，来进行各个部分的模型测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30362ee2-9fd1-4702-89ba-e81b56c37fbf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而第四部分，我们将开始介绍集成学习的训练和优化的实战技巧，尽管从可解释性角度来说，集成学习的可解释性并不如逻辑回归和决策树，但在大多数建模场景下，集成学习都将获得一个更好的预测结果，这也是目前效果优先的建模场景下最常使用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b51be-8469-4b21-955f-c5e7fd7b4ff4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">总的来说，本部分内容只有一个目标，那就是借助各类优化方法，抵达每个主流集成学习的效果上界。换而言之，本部分我们将围绕单模优化策略展开详细的探讨，涉及到的具体集成学习包括随机森林、XGBoost、LightGBM、和CatBoost等目前最主流的集成学习算法，而具体的优化策略则包括超参数优化器的使用、特征衍生和筛选方法的使用、单模型自融合方法的使用，这些优化方法也是截至目前，提升单模效果最前沿、最有效、同时也是最复杂的方法。其中有很多较为艰深的理论，也有很多是经验之谈，但无论如何，我们希望能够围绕当前数据集，让每个集成学习算法优化到极限。值得注意的是，在这个过程中，我们会将此前介绍的特征衍生和特征筛选视作是一种模型优化方法，衍生和筛选的效果，一律以模型的最终结果来进行评定。而围绕集成学习进行海量特征衍生和筛选，也才是特征衍生和筛选技术能发挥巨大价值的主战场。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a378479-b0ad-465d-bb16-c6869c9d2232",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而在抵达了单模的极限后，我们就会进入到下一阶段，也就是模型融合阶段。需要知道的是，只有单模的效果到达了极限，进一步的多模型融合、甚至多层融合，才是有意义的，才是有效果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ac0c1-17b7-4169-943e-ac014ec1aa52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5074ff-7996-4c9c-9f0d-a5e39e05e3dc",
   "metadata": {},
   "source": [
    "# <center>Part 4.集成算法的训练与优化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95840932-37bd-4a3f-be1e-6fcadd698df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72dc75-d55c-4f18-a8f5-ecf048a1ef72",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后执行Part 1中的数据清洗相关工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a895a4-a4be-479f-bec3-504dd3835723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "tcc = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# 标注连续/离散字段\n",
    "# 离散字段\n",
    "category_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "                'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "                'PaymentMethod']\n",
    "\n",
    "# 连续字段\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    " \n",
    "# 标签\n",
    "target = 'Churn'\n",
    "\n",
    "# ID列\n",
    "ID_col = 'customerID'\n",
    "\n",
    "# 验证是否划分能完全\n",
    "assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]\n",
    "\n",
    "# 连续字段转化\n",
    "tcc['TotalCharges']= tcc['TotalCharges'].apply(lambda x: x if x!= ' ' else np.nan).astype(float)\n",
    "tcc['MonthlyCharges'] = tcc['MonthlyCharges'].astype(float)\n",
    "\n",
    "# 缺失值填补\n",
    "tcc['TotalCharges'] = tcc['TotalCharges'].fillna(0)\n",
    "\n",
    "# 标签值手动转化 \n",
    "tcc['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "tcc['Churn'].replace(to_replace='No',  value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4379d19-fbbb-4a92-834c-d7a5f1214814",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tcc.drop(columns=[ID_col, target]).copy()\n",
    "labels = tcc['Churn'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb2136-885e-4ee5-9fdb-9a0183fd1930",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时，创建自然编码后的数据集以及经过时序特征衍生的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b267255-a5fc-432c-ae7e-2841e845d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train, test = train_test_split(tcc, random_state=22)\n",
    "\n",
    "X_train = train.drop(columns=[ID_col, target]).copy()\n",
    "X_test = test.drop(columns=[ID_col, target]).copy()\n",
    "\n",
    "y_train = train['Churn'].copy()\n",
    "y_test = test['Churn'].copy()\n",
    "\n",
    "X_train_seq = pd.DataFrame()\n",
    "X_test_seq = pd.DataFrame()\n",
    "\n",
    "# 年份衍生\n",
    "X_train_seq['tenure_year'] = ((72 - X_train['tenure']) // 12) + 2014\n",
    "X_test_seq['tenure_year'] = ((72 - X_test['tenure']) // 12) + 2014\n",
    "\n",
    "# 月份衍生\n",
    "X_train_seq['tenure_month'] = (72 - X_train['tenure']) % 12 + 1\n",
    "X_test_seq['tenure_month'] = (72 - X_test['tenure']) % 12 + 1\n",
    "\n",
    "# 季度衍生\n",
    "X_train_seq['tenure_quarter'] = ((X_train_seq['tenure_month']-1) // 3) + 1\n",
    "X_test_seq['tenure_quarter'] = ((X_test_seq['tenure_month']-1) // 3) + 1\n",
    "\n",
    "# 独热编码\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_train_seq)\n",
    "\n",
    "seq_new = list(X_train_seq.columns)\n",
    "\n",
    "# 创建带有列名称的独热编码之后的df\n",
    "X_train_seq = pd.DataFrame(enc.transform(X_train_seq).toarray(), \n",
    "                           columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "X_test_seq = pd.DataFrame(enc.transform(X_test_seq).toarray(), \n",
    "                          columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "# 调整index\n",
    "X_train_seq.index = X_train.index\n",
    "X_test_seq.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151882b7-da39-4d1d-a4fc-a19255967a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(X_train[category_cols])\n",
    "\n",
    "X_train_OE = pd.DataFrame(ord_enc.transform(X_train[category_cols]), columns=category_cols)\n",
    "X_train_OE.index = X_train.index\n",
    "X_train_OE = pd.concat([X_train_OE, X_train[numeric_cols]], axis=1)\n",
    "\n",
    "X_test_OE = pd.DataFrame(ord_enc.transform(X_test[category_cols]), columns=category_cols)\n",
    "X_test_OE.index = X_test.index\n",
    "X_test_OE = pd.concat([X_test_OE, X_test[numeric_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc19d5-5559-494f-a9a5-c76d78071074",
   "metadata": {},
   "source": [
    "然后是模型融合部分所需准备的数据以及训练好的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89143bcc-4f79-46ba-9bdf-cbee5c3d10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化KFold评估器\n",
    "kf = KFold(n_splits=5, random_state=12, shuffle=True)\n",
    "\n",
    "# 重置训练集和测试集的index\n",
    "X_train_OE = X_train_OE.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "train_part_index_l = []\n",
    "eval_index_l = []\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train_OE, y_train):\n",
    "    train_part_index_l.append(train_part_index)\n",
    "    eval_index_l.append(eval_index)\n",
    "    \n",
    "# 训练集特征\n",
    "X_train1 = X_train_OE.loc[train_part_index_l[0]]\n",
    "X_train2 = X_train_OE.loc[train_part_index_l[1]]\n",
    "X_train3 = X_train_OE.loc[train_part_index_l[2]]\n",
    "X_train4 = X_train_OE.loc[train_part_index_l[3]]\n",
    "X_train5 = X_train_OE.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集特征\n",
    "X_eval1 = X_train_OE.loc[eval_index_l[0]]\n",
    "X_eval2 = X_train_OE.loc[eval_index_l[1]]\n",
    "X_eval3 = X_train_OE.loc[eval_index_l[2]]\n",
    "X_eval4 = X_train_OE.loc[eval_index_l[3]]\n",
    "X_eval5 = X_train_OE.loc[eval_index_l[4]]\n",
    "\n",
    "# 训练集标签\n",
    "y_train1 = y_train.loc[train_part_index_l[0]]\n",
    "y_train2 = y_train.loc[train_part_index_l[1]]\n",
    "y_train3 = y_train.loc[train_part_index_l[2]]\n",
    "y_train4 = y_train.loc[train_part_index_l[3]]\n",
    "y_train5 = y_train.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集标签\n",
    "y_eval1 = y_train.loc[eval_index_l[0]]\n",
    "y_eval2 = y_train.loc[eval_index_l[1]]\n",
    "y_eval3 = y_train.loc[eval_index_l[2]]\n",
    "y_eval4 = y_train.loc[eval_index_l[3]]\n",
    "y_eval5 = y_train.loc[eval_index_l[4]]\n",
    "\n",
    "train_set = [(X_train1, y_train1), \n",
    "             (X_train2, y_train2), \n",
    "             (X_train3, y_train3), \n",
    "             (X_train4, y_train4), \n",
    "             (X_train5, y_train5)]\n",
    "\n",
    "eval_set = [(X_eval1, y_eval1), \n",
    "            (X_eval2, y_eval2), \n",
    "            (X_eval3, y_eval3), \n",
    "            (X_eval4, y_eval4), \n",
    "            (X_eval5, y_eval5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d044323-4a7f-42d5-837b-996eb0939308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型组\n",
    "grid_RF_1 = load('./models/grid_RF_1.joblib') \n",
    "grid_RF_2 = load('./models/grid_RF_2.joblib') \n",
    "grid_RF_3 = load('./models/grid_RF_3.joblib') \n",
    "grid_RF_4 = load('./models/grid_RF_4.joblib') \n",
    "grid_RF_5 = load('./models/grid_RF_5.joblib') \n",
    "\n",
    "RF_1 = grid_RF_1.best_estimator_\n",
    "RF_2 = grid_RF_2.best_estimator_\n",
    "RF_3 = grid_RF_3.best_estimator_\n",
    "RF_4 = grid_RF_4.best_estimator_\n",
    "RF_5 = grid_RF_5.best_estimator_\n",
    "\n",
    "RF_l = [RF_1, RF_2, RF_3, RF_4, RF_5]\n",
    "\n",
    "# 决策树模型组\n",
    "grid_tree_1 = load('./models/grid_tree_1.joblib')\n",
    "grid_tree_2 = load('./models/grid_tree_2.joblib')\n",
    "grid_tree_3 = load('./models/grid_tree_3.joblib')\n",
    "grid_tree_4 = load('./models/grid_tree_4.joblib')\n",
    "grid_tree_5 = load('./models/grid_tree_5.joblib')\n",
    "\n",
    "tree_1 = grid_tree_1.best_estimator_\n",
    "tree_2 = grid_tree_2.best_estimator_\n",
    "tree_3 = grid_tree_3.best_estimator_\n",
    "tree_4 = grid_tree_4.best_estimator_\n",
    "tree_5 = grid_tree_5.best_estimator_\n",
    "\n",
    "tree_l = [tree_1, tree_2, tree_3, tree_4, tree_5]\n",
    "\n",
    "# 逻辑回归模型组\n",
    "grid_lr_1 = load('./models/grid_lr_1.joblib')\n",
    "grid_lr_2 = load('./models/grid_lr_2.joblib')\n",
    "grid_lr_3 = load('./models/grid_lr_3.joblib')\n",
    "grid_lr_4 = load('./models/grid_lr_4.joblib')\n",
    "grid_lr_5 = load('./models/grid_lr_5.joblib')\n",
    "\n",
    "lr_1 = grid_lr_1.best_estimator_\n",
    "lr_2 = grid_lr_2.best_estimator_\n",
    "lr_3 = grid_lr_3.best_estimator_\n",
    "lr_4 = grid_lr_4.best_estimator_\n",
    "lr_5 = grid_lr_5.best_estimator_\n",
    "\n",
    "lr_l = [lr_1, lr_2, lr_3, lr_4, lr_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1876990f-cd12-401c-ab5c-2b630fc977ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1_predict_proba_RF = pd.Series(RF_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_RF = pd.Series(RF_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_RF = pd.Series(RF_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_RF = pd.Series(RF_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_RF = pd.Series(RF_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_RF = pd.concat([eval1_predict_proba_RF, \n",
    "                                   eval2_predict_proba_RF, \n",
    "                                   eval3_predict_proba_RF, \n",
    "                                   eval4_predict_proba_RF, \n",
    "                                   eval5_predict_proba_RF]).sort_index()\n",
    "\n",
    "eval1_predict_proba_tree = pd.Series(tree_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_tree = pd.Series(tree_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_tree = pd.Series(tree_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_tree = pd.Series(tree_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_tree = pd.Series(tree_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_tree = pd.concat([eval1_predict_proba_tree, \n",
    "                                     eval2_predict_proba_tree, \n",
    "                                     eval3_predict_proba_tree, \n",
    "                                     eval4_predict_proba_tree, \n",
    "                                     eval5_predict_proba_tree]).sort_index()\n",
    "\n",
    "eval1_predict_proba_lr = pd.Series(lr_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_lr = pd.Series(lr_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_lr = pd.Series(lr_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_lr = pd.Series(lr_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_lr = pd.Series(lr_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_lr = pd.concat([eval1_predict_proba_lr, \n",
    "                                   eval2_predict_proba_lr, \n",
    "                                   eval3_predict_proba_lr, \n",
    "                                   eval4_predict_proba_lr, \n",
    "                                   eval5_predict_proba_lr]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86813ab5-614b-4c2e-92d1-7f9e3bb817af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_proba_RF = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF.append(RF_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_RF = np.array(test_predict_proba_RF)\n",
    "test_predict_proba_RF = test_predict_proba_RF.mean(0)\n",
    "\n",
    "test_predict_proba_tree = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree.append(tree_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_tree = np.array(test_predict_proba_tree)\n",
    "test_predict_proba_tree = test_predict_proba_tree.mean(0)\n",
    "\n",
    "test_predict_proba_lr = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr.append(lr_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_lr = np.array(test_predict_proba_lr)\n",
    "test_predict_proba_lr = test_predict_proba_lr.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be650f1-ee5b-4155-add0-667aad25e41a",
   "metadata": {},
   "source": [
    "## <center>Ch.3 模型融合基础方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d53e7-3a7d-4faf-b5c8-4c40f58c9778",
   "metadata": {},
   "source": [
    "- 算法工程师的工程化实践能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e37dc8-1e7b-48dd-b311-3b567e9ec311",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在介绍了Stacking过程中一级学习器和元学习器的训练和优化策略后，本节我们将进一步介绍Stacking模型融合优化函数的创建和使用，即重点介绍如何工程化高效落地实践Stacking融合及其优化过程。在本小节中，我们将详细总结此前介绍的Stacking模型融合完整流程，并通过一系列函数和类的编写来将其进行封装整合，和简单的sklearn中的Stacking评估器不同，这部分内容函数的编写将融合此前所有的优化策略，以大幅提升Stacking融合结果。本部分编写的函数既是为实战过程补充“枪支弹药”，同时也是后续进阶优化——级联优化之必须。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7398a4-4c4c-407a-a76c-2abbbb9b9a2e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;自特征工程部分内容开始，课程中工程化实现的代码就开始大幅增加，这些手动编写的函数和类，不仅仅是为了补充现有第三方库功能上的不足、给大家提供更多趁手的工具，更是为了借此提升大家的工程化实践能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f74a0-d983-44f2-b74f-4fa9f8dd0566",
   "metadata": {},
   "source": [
    "&emsp;&emsp;正所谓“纸上得来终觉浅，绝知此事要躬行”。其实，对于算法工程人员来说，精通一个个算法背后的数学原理和调用第三方库来实现是远远不够的。很多时候，工程化实现能力也会很大程度影响最终的建模效果。以模型融合为例，能否设计一个高效的融合流程、能否通过编写代码实现这个流程、能否借助这个流程来批量的测试和筛选最佳融合方案（模型组合、特征组合、策略组合等），都将很大程度影响最终能否获得一个更好的结果。当然由此其实也能看出，其实算法工程师的工程化能力不仅仅是代码能力，算法流程的设计能力也是非常重要的一环。本届开始，借助模型融合优化函数的编写和封装，我们也将开始逐渐介绍如何设计一套完整、合理、高效的算法流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212994c-5487-4747-8ea4-00408d2e47ad",
   "metadata": {},
   "source": [
    "- 提前准备manual_ensemble.py文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376c0c4-172e-46d1-8fda-a73e9a4377d4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和特征工程类似，接下来我们也将单独创建一个py文件，作为模型融合函数库，存储一系列模型融合优化函数。因此需要提前创建好manual_ensemble.py文件，其基本格式和features_creation.py一致。创建完成后，需要将此前定义的VotingClassifier_threshold类和train_cross函数写入。编写完成后，即可按照如下方式进行导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42446a03-220c-424a-ab3b-14d769deb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import manual_ensemble as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83df986-9397-4591-85bb-04c91501039e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'manual_ensemble' from 'D:\\\\Work\\\\jupyter\\\\telco\\\\正式课程\\\\manual_ensemble.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        d:\\work\\jupyter\\telco\\正式课程\\manual_ensemble.py\n",
       "\u001b[1;31mDocstring:\u001b[0m   自动化批量特征衍生模块\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "me?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08091e36-561b-487b-8e5f-1e0d20b94a2f",
   "metadata": {},
   "source": [
    "然后即可查看目前已经写入的函数和类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba753bb-7c3f-44e8-a0de-86e2caa78c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from manual_ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "005242c0-bfcc-4194-a084-cfb379479884",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mVotingClassifier_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hard'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mthr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Base class for all estimators in scikit-learn.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "All estimators should specify all the parameters that can be set\n",
       "at the class level in their ``__init__`` as explicit keyword\n",
       "arguments (no ``*args`` or ``**kwargs``).\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\work\\jupyter\\telco\\正式课程\\manual_ensemble.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VotingClassifier_threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d98f61d4-ac16-4297-b927-e6a3be90598b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_cross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mblending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Stacking融合过程一级学习器交叉训练函数\n",
       "\n",
       ":param X_train: 训练集特征\n",
       ":param y_train: 训练集标签\n",
       ":param X_test: 测试集特征\n",
       ":param estimators: 一级学习器，由(名称,评估器)组成的列表\n",
       ":param n_splits: 交叉训练折数\n",
       ":param test_size: blending过程留出集占比\n",
       ":param random_state: 随机数种子\n",
       ":param blending: 是否进行blending融合\n",
       "\n",
       ":return：交叉训练后创建oof训练数据和测试集平均预测结果\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\work\\jupyter\\telco\\正式课程\\manual_ensemble.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cross?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4f490-a50c-4cf0-85e3-129091a3e48f",
   "metadata": {},
   "source": [
    "准备完毕后，正式进入到本节内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36417fd5-31f6-499c-955a-79890a3beb80",
   "metadata": {},
   "source": [
    "> 关于manual_ensemble.py内部代码结构，将在函数编写时逐渐完善。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3189191-4654-41f5-b431-1c5cfafd0863",
   "metadata": {},
   "source": [
    "## 十、Stacking融合优化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfef1f-b545-41ca-9043-67ecc89fca27",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Stacking本身的流程其实并不复杂，无非就是训练一级学习器并创建oof训练数据，然后带入元学习器进行第二轮的训练。而在实际工程化实践过程中，较为复杂的是如何高效的进行一级学习器的训练，以及如何进行元学习器的优化。接下来，我们围绕这两个环节的优化策略来进行整理和函数编写。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ce42f-7683-4a20-bfdf-0b7635e74b50",
   "metadata": {},
   "source": [
    "### 1.交叉训练过程超参数自动优化过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db7ab6-df56-40a4-a306-a691a5864356",
   "metadata": {},
   "source": [
    "- 两种交叉训练策略回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25c005-e7b9-4083-b94b-3c5f85bc016d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于一级学习器的训练，在上一小节我们介绍了目前通用的两种较为有效的训练策略：其一是在全部数据集上训练一组一级学习器并进行超参数优化，然后组内共用一组超参数来进行交叉训练，即训练过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351fc0ed-1b6f-4d63-bfb3-11597884c189",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221008113027435.png\" alt=\"image-20221008113027435\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4aa987-f5a5-40c8-b9c9-e96be26a630e",
   "metadata": {},
   "source": [
    "这其实是最通用的流程，并且并不难执行。一般来说，我们在机器学习建模的初期，都是需要围绕单模进行模型训练和优化的，此时我们只需要把训练好的单独模型组成estimators，然后带入train_cross输出oof数据集即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48863ee7-2db1-4668-bf70-cd783669fc73",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们重点讨论是第二种方法如何高效实现。上一小节我们介绍了基于该交叉训练过程实现的基本过程，相比之前的交叉训练过程，本方法重点在于需要实现组内每个模型每次训练的超参数优化，其具体实现过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182bab1b-40f6-4dae-b6d8-a532759ba740",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221008113103211.png\" alt=\"image-20221008113103211\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454bcd2-b52d-4263-a4f4-53ccbd631290",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其实从上一小节的融合结果我们已经能明显看出一级学习器的不同训练过程对结果的影响程度。尽管很多时候出于建模流程效率考虑，会选择在全数据集上训练一组超参数，然后在固定这组超参数的情况下进行交叉训练，但交叉训练过程单独训练每个模型的超参数的优势是确实存在的。交叉训练过程中组内超参数独立训练，能有效提升oof数据集中信息隔离效果，进而提高Stacking泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e1478-0b07-4380-81a2-bcd6de041f02",
   "metadata": {},
   "source": [
    "- 基于贝叶斯优化的交叉训练策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a0328-06fa-400e-9c5d-1f0f94d42683",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而具体如何实现，我们可以执行类似Part 4.3.4中单独创建五个不同的训练集子集、然后每个模型都手动优化得到5个模型，然后再创建train_oof数据集。不过该策略耗时较长，需要耗费大量的时间一个个模型进行超参数优化。当然，我们也可以采用一种更加自动化的方法来执行，即在train_cross函数执行过程中，fit过程直接带入网格搜索评估器，此时输出结果就将是一个个超参数优化后的评估器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2958371-3bf5-46a8-8726-929ec3d62dac",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过fit网格搜索评估器的方法会非常耗时，对于很多超参数范围较大的集成算法，单轮的搜索就需要至少十几甚至几十分钟，更何况为了让5个不同的模型在各自不同的训练数据集上自适应的搜索出最佳参数，参数空间也需要设置一个较大的范围，因此在train_cross过程中执行fit网格搜索评估器的操作几乎不可行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face3596-2028-44f0-a612-805f7f62ecef",
   "metadata": {},
   "source": [
    "&emsp;&emsp;那能否替换成别的优化器呢？其实相比之下，贝叶斯优化器会更适用于当前情况。贝叶斯优化器有较快的执行速度、并且低使用门槛、高效果上限，少量迭代即可初见成效，大量迭代也可以确保效果，至于具体迭代多少次，完全可以根据当前算力情况来决定。这里我们仍然可以考虑使用hyperOPT优化器来执行。但唯一的问题就是，但是，原生的hyperOPT优化过程是多个函数的计算流（定义超参数空间——定义目标函数——定义优化函数——执行优化过程），我们需要将这一过程封装成一个sklearn的评估器，才可借助一个fit语句就完成这一整个流程，进而才可带入train_cross中进行自动超参数优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88289cdc-2d11-4606-a3e7-52da12d065b1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们就尝试借助hyperOPT来执行超参数自动优化的交叉训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1c558-0c4a-417a-a6ce-0abddacbf15d",
   "metadata": {},
   "source": [
    "### 2.借助hyperOPT完成交叉训练过程超参数自动优化过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d9fa5-0324-453f-ad0f-ec6f838d26ad",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们首先从决策树模型入手，将原先hyperOPT过程封装成一个评估器，并测试fit调用结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec251e-96a0-416f-932d-327b854193c3",
   "metadata": {},
   "source": [
    "- 决策树模型超参数优化评估器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a98b41-80df-462a-b7ec-469a356d3497",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们先回顾下整个决策树模型的hyperOPT的优化过程，首先是参数搜索空间的创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b701f3e-fe65-41b0-b0d6-c43b03aae7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params_space = {'tree_max_depth': hp.choice('tree_max_depth', np.arange(2, 20).tolist()), \n",
    "                     'tree_min_samples_split': hp.choice('tree_min_samples_split', np.arange(2, 15).tolist()), \n",
    "                     'tree_min_samples_leaf': hp.choice('tree_min_samples_leaf', np.arange(1, 15).tolist()), \n",
    "                     'tree_max_leaf_nodes': hp.choice('tree_max_leaf_nodes', np.arange(2, 51).tolist())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471909c-cba2-40f9-a5cb-b055eeefa0ec",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后定义目标函数。在这里的目标函数定义时，为了方便后续直接带入搜索得到的最佳参数在测试集上进行验证，目标函数获取参数取值的方式分为两种，其一是训练过程，直接从params_space传入参数，而在测试过程，则直接传入搜索后的params_best，并将hp.choice对象得到的索引转化为具体数值。不同的训练过程通过train参数控制，这里我们先看train=True的情况，也就是训练过程，然后再讨论测试时索引值和具体数值的转化关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd307a66-3d64-4585-819a-f4cb70c482cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_tree(params, train=True):\n",
    "    # 读取参数\n",
    "    if train == True:\n",
    "        max_depth = params['tree_max_depth']\n",
    "        min_samples_split = params['tree_min_samples_split']\n",
    "        min_samples_leaf = params['tree_min_samples_leaf']\n",
    "        max_leaf_nodes = params['tree_max_leaf_nodes']\n",
    "    else: \n",
    "        max_depth = params['tree_max_depth'] + 2\n",
    "        min_samples_split = params['tree_min_samples_split'] + 2\n",
    "        min_samples_leaf = params['tree_min_samples_leaf'] + 1\n",
    "        max_leaf_nodes = params['tree_max_leaf_nodes'] + 2\n",
    "        \n",
    "    # 实例化模型\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, \n",
    "                                  min_samples_split=min_samples_split, \n",
    "                                  min_samples_leaf=min_samples_leaf, \n",
    "                                  max_leaf_nodes=max_leaf_nodes)\n",
    "    \n",
    "    if train == True:\n",
    "        res = -cross_val_score(tree, X_train_OE, y_train).mean()\n",
    "    else:\n",
    "        res = tree.fit(X_train_OE, y_train)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f58255a-f564-4c33-8bfa-a6d6adf18c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_hyperopt_tree(max_evals):\n",
    "    params_best = fmin(fn = hyperopt_tree,\n",
    "                       space = tree_params_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate=np.random.RandomState(9))    \n",
    "    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd3e49-d705-4462-8611-436cb17db3a0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，测试优化能否顺利运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe81d2b-672a-43fc-99b3-d5c59e90818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1000/1000 [00:48<00:00, 20.68trial/s, best loss: -0.7962873770820791]\n"
     ]
    }
   ],
   "source": [
    "tree_params_best = param_hyperopt_tree(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c0034c1-fcf7-4e34-9601-afc44b856c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree_max_depth': 3,\n",
       " 'tree_max_leaf_nodes': 25,\n",
       " 'tree_min_samples_leaf': 9,\n",
       " 'tree_min_samples_split': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47370dc-f946-4630-bbb7-4b81cef637b7",
   "metadata": {},
   "source": [
    "这里需要注意，在定义目标函数的时候，仍然是区分了目标函数的训练状态和测试状态，训练状态（train=True）是默认状态，作为超参数搜索时的目标函数时使用，而train=Fasle时则为测试状态，此时函数用于带入搜索出来的超参数，来直接输出最终的最优模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75216270-257f-4f3d-ab46-ef80214add95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5, max_leaf_nodes=27, min_samples_leaf=10,\n",
       "                       min_samples_split=3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperopt_tree(tree_params_best, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2859bcaf-d670-4715-ac87-427a139bb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = hyperopt_tree(tree_params_best, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3c404-f84b-41c3-8fc1-2f68c7153697",
   "metadata": {},
   "source": [
    "然后即可进一步测试模型在测试集上的评分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "510a5813-587d-4160-b65b-b40dd36ef768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7768313458262351"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a151b-b878-496c-b991-a7ea6da8c095",
   "metadata": {},
   "source": [
    "而训练状态和测试状态的重要区别，就在于参数的导入。对于hyperOPT来说，hp.choice的搜索结果其实是原始参数取值列表的索引值，例如max_depth：3，其实代表的是原始参数空间中'tree_max_depth': hp.choice('RF_max_depth', np.arange(2, 20).tolist())的第3个值，也就是2+3=5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d27d5b8-3068-4229-a3b1-8f0409c397d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fee43c1-1e0d-4c7f-a331-4ecd4b9ce630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2, 20)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38c0e7-f2f4-49cf-a760-2a696be3f7a9",
   "metadata": {},
   "source": [
    "因此目标函数在定义train=False的代码时，对于整数列表的数值提取，只需要用得到的索引值+列表初始值即可。再比如max_leaf_nodes的最佳值索引是27,则真实值为2+27=29。当然，对于字符串列表，则需要直接把字符串完整列表带入进行索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb967de-8e59-4111-9aff-34fbbe556506",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而通过目标函数不同模式的改写，能极大程度提高目标函数的复用率，使代码更加简洁，并且测试模式下输出的最佳模型，也是后面要用到的关键对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bebc0e1-4522-4081-b963-2cf81b62cb40",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在熟悉了改写后的hyperOPT后，接下来将这一整个流程封装为一个评估器，方便后续直接使用fit方法调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e1d8cb6-8aee-48a7-a72b-68605aa1eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tree_cascade(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, tree_params_space, max_evals=1000):\n",
    "        self.tree_params_space = tree_params_space\n",
    "        self.max_evals = max_evals\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        def hyperopt_tree(params, train=True):\n",
    "            # 读取参数\n",
    "            if train == True:\n",
    "                max_depth = params['tree_max_depth']\n",
    "                min_samples_split = params['tree_min_samples_split']\n",
    "                min_samples_leaf = params['tree_min_samples_leaf']\n",
    "                max_leaf_nodes = params['tree_max_leaf_nodes']\n",
    "            else: \n",
    "                max_depth = params['tree_max_depth'] + 2\n",
    "                min_samples_split = params['tree_min_samples_split'] + 2\n",
    "                min_samples_leaf = params['tree_min_samples_leaf'] + 1\n",
    "                max_leaf_nodes = params['tree_max_leaf_nodes'] + 2\n",
    "\n",
    "            # 实例化模型\n",
    "            tree = DecisionTreeClassifier(max_depth=max_depth, \n",
    "                                          min_samples_split=min_samples_split, \n",
    "                                          min_samples_leaf=min_samples_leaf, \n",
    "                                          max_leaf_nodes=max_leaf_nodes, \n",
    "                                          random_state=12)\n",
    "\n",
    "            if train == True:\n",
    "                res = -cross_val_score(tree, X, y).mean()\n",
    "            else:\n",
    "                res = tree.fit(X, y)\n",
    "\n",
    "            return res\n",
    "\n",
    "        def param_hyperopt_tree(max_evals):\n",
    "            params_best = fmin(fn = hyperopt_tree,\n",
    "                               space = self.tree_params_space,\n",
    "                               algo = tpe.suggest,\n",
    "                               max_evals = max_evals, \n",
    "                               rstate=np.random.RandomState(9))    \n",
    "\n",
    "            return params_best\n",
    "        \n",
    "        tree_params_best = param_hyperopt_tree(self.max_evals)\n",
    "        self.clf = hyperopt_tree(tree_params_best, train=False)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        res_proba = self.clf.predict_proba(X)\n",
    "        return res_proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self.clf.predict(X)\n",
    "        return res\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        res = self.clf.score(X, y)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb9261-e9fe-438f-a1a8-e602127023ce",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里需要注意，在定义评估器的过程中，围绕每个评估器都设置了一个默认的迭代次数，我们可以根据实际算力情况来对其进行灵活调整，迭代次数越多、最终结果也将更加精确。此外，需要注意tree_cascade类内部fit函数的X和y的传递过程，由于fit函数内部是有X和y这一组局部变量的，所以在fit函数内部的hyperopt_tree函数就可以对其直接引入，而不用以参数形式引入。并且，hyperopt_tree函数内部目标函数的输出结果是（五折）交叉验证结果，用以提高搜索结果的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da3381-51f3-4be0-bf3a-0fc08466e0cb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后测试评估器能否顺利执行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0100ff62-cc55-4ca3-b4f6-5a95fdfc4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_hyper = tree_cascade(tree_params_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f34e3ebf-4eb3-452c-b1a7-aa040ed9e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1000/1000 [00:49<00:00, 20.11trial/s, best loss: -0.7962873770820791]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tree_cascade(tree_params_space={'tree_max_depth': <hyperopt.pyll.base.Apply object at 0x00000288CEF8C9D0>,\n",
       "                                'tree_max_leaf_nodes': <hyperopt.pyll.base.Apply object at 0x00000288D04FA460>,\n",
       "                                'tree_min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x00000288CAD32100>,\n",
       "                                'tree_min_samples_split': <hyperopt.pyll.base.Apply object at 0x00000288CAD32040>})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_hyper.fit(X_train_OE, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f1980fd-21d4-435f-857e-420234205e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_hyper.predict(X_test_OE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8758d02-5d0c-44d8-a663-6a398bb50839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7768313458262351"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_hyper.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0b76bd1-25d8-4c8f-8756-536ce6d1f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 2000/2000 [01:51<00:00, 18.02trial/s, best loss: -0.7962873770820791]\n"
     ]
    }
   ],
   "source": [
    "tree_hyper = tree_cascade(tree_params_space, max_evals=2000).fit(X_train_OE, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c61535ae-5d59-47fd-974d-8f84b58986e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7768313458262351"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_hyper.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7e194-9a42-4909-baa9-c9642a836f78",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里需要注意，hyperOPT搜索效果其实也是有“上限”的，也就是说当hyperOPT的搜索会存在当迭代次数超过某个数值时，增加迭代次数并不会提升模型效果。从原理层面来说，是因为新的超参数组合结果并不会影响此前的估计，而具体达到效果上限需要多少次迭代，则和数据量、模型复杂度、甚至是随机数种子都有很大关系。这里，对于当前数据集来说，决策树模型迭代1000次是可以达到效果上限的，而1000次的迭代也仅需要1分钟不到的时间，因此，决策树的优化评估器一般无须调整迭代次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb86eef-ca34-4b16-86ef-52179d1937ed",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但对于更加复杂的集成学习来说，这个到达上限所需的迭代次数可能会很高，单次搜索达到上限的时间可能会很长，因此，在后续交叉训练过程中，我们是否要每次模型训练都令其达到这个上限，也需要根据实际情况来决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25015633-a6a1-47fd-a377-0a337bb69fb7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完成了决策树模型的超参数优化评估器的封装，接下来我们需要将其写入manual_ensemble.py模块中。当然，除了决策树模型外，我们还需要类似的改写另外两个模型的TPE优化过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da053e6-f357-4e01-a0fc-d9578b9b2bd8",
   "metadata": {},
   "source": [
    "- 随机森林超参数优化评估器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a20258b6-c31f-4c8f-8d05-7ddbf65bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_params_space = {'RF_min_samples_leaf': hp.choice('RF_min_samples_leaf', np.arange(1, 20).tolist()), \n",
    "                   'RF_min_samples_split': hp.choice('RF_min_samples_split', np.arange(2, 20).tolist()), \n",
    "                   'RF_max_depth': hp.choice('RF_max_depth', np.arange(2, 20).tolist()), \n",
    "                   'RF_max_leaf_nodes': hp.choice('RF_max_leaf_nodes', np.arange(20, 200).tolist()), \n",
    "                   'RF_n_estimators': hp.choice('RF_n_estimators', np.arange(20, 200).tolist()), \n",
    "                   'RF_max_samples': hp.uniform('RF_max_samples', 0.2, 0.8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c0aaa2d-9426-42e4-840b-829b4021de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF_cascade(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, RF_params_space, max_evals=500):\n",
    "        self.RF_params_space = RF_params_space\n",
    "        self.max_evals = max_evals\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        def hyperopt_RF(params, train=True):\n",
    "            # 读取参数\n",
    "            if train == True:\n",
    "                min_samples_leaf = params['RF_min_samples_leaf']\n",
    "                min_samples_split = params['RF_min_samples_split']\n",
    "                max_depth = params['RF_max_depth']\n",
    "                max_leaf_nodes = params['RF_max_leaf_nodes']\n",
    "                n_estimators = params['RF_n_estimators']\n",
    "                max_samples = params['RF_max_samples']\n",
    "            else: \n",
    "                min_samples_leaf = params['RF_min_samples_leaf'] + 1\n",
    "                min_samples_split = params['RF_min_samples_split'] + 2\n",
    "                max_depth = params['RF_max_depth'] + 2\n",
    "                max_leaf_nodes = params['RF_max_leaf_nodes'] + 20\n",
    "                n_estimators = params['RF_n_estimators'] + 20\n",
    "                max_samples = params['RF_max_samples']\n",
    "            # 实例化模型\n",
    "            RF = RandomForestClassifier(min_samples_leaf = min_samples_leaf, \n",
    "                                        min_samples_split = min_samples_split,\n",
    "                                        max_depth = max_depth, \n",
    "                                        max_leaf_nodes = max_leaf_nodes, \n",
    "                                        n_estimators = n_estimators, \n",
    "                                        max_samples = max_samples)\n",
    "            if train == True:\n",
    "                res = -cross_val_score(RF, X, y).mean()\n",
    "            else:\n",
    "                res = RF.fit(X, y)\n",
    "\n",
    "            return res\n",
    "\n",
    "        def param_hyperopt_RF(max_evals):\n",
    "            params_best = fmin(fn = hyperopt_RF,\n",
    "                               space = self.RF_params_space,\n",
    "                               algo = tpe.suggest,\n",
    "                               max_evals = max_evals)    \n",
    "\n",
    "            return params_best\n",
    "        \n",
    "        RF_params_best = param_hyperopt_RF(self.max_evals)\n",
    "        self.clf = hyperopt_RF(RF_params_best, train=False)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        res_proba = self.clf.predict_proba(X)\n",
    "        return res_proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self.clf.predict(X)\n",
    "        return res\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        res = self.clf.score(X, y)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82971f86-6498-4600-b25c-23a1d66727a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_hyper = RF_cascade(RF_params_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30633edb-5c81-4704-a47f-616e6ca9644e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 500/500 [07:39<00:00,  1.09trial/s, best loss: -0.8087839726498667]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RF_cascade(RF_params_space={'RF_max_depth': <hyperopt.pyll.base.Apply object at 0x000002AAD8A233A0>,\n",
       "                            'RF_max_leaf_nodes': <hyperopt.pyll.base.Apply object at 0x000002AAD8A23A00>,\n",
       "                            'RF_max_samples': <hyperopt.pyll.base.Apply object at 0x000002AAD8A33FA0>,\n",
       "                            'RF_min_samples_leaf': <hyperopt.pyll.base.Apply object at 0x000002AAD8A219D0>,\n",
       "                            'RF_min_samples_split': <hyperopt.pyll.base.Apply object at 0x000002AAD8A21A90>,\n",
       "                            'RF_n_estimators': <hyperopt.pyll.base.Apply object at 0x000002AAD8A30C70>})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_hyper.fit(X_train_OE, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f0790eb5-0ac1-4c8e-83bf-4c956f24bfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_hyper.predict(X_test_OE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "87103e0a-4383-4bb0-a71d-ac8f4ae5dee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7864849517319704"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_hyper.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346b8a4-1af6-49d5-9ed5-4024ce441798",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，其实对于当前数据集来说，随机森林迭代500次并没有达到效果的“上限”，这里我们可以尝试迭代1000次查看结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28b2212f-6098-4d8f-a262-33ff08a5e398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1000/1000 [14:57<00:00,  1.11trial/s, best loss: -0.8093528711906195]\n"
     ]
    }
   ],
   "source": [
    "RF_hyper = RF_cascade(RF_params_space, max_evals=1000).fit(X_train_OE, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a356fa54-f84b-430d-af70-ee2bcd4cac5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881885292447472"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_hyper.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18753119-175f-4226-986a-3df92dd3fb37",
   "metadata": {},
   "source": [
    "能够发现，增加迭代次数后模型效果有了更进一步提升，但1000次的迭代是否已经达到了TPE搜索的效果“上限”？我们是否应该测试更多次迭代的结果？并且，既然发现更多次的迭代能够有效果上的进一步提升，我们是否应该修改默认迭代次数？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c202b6b-e024-4384-8934-3d20be5a3f63",
   "metadata": {},
   "source": [
    "&emsp;&emsp;需要说明的是，这里设置默认迭代500次是为了便于后续交叉训练的快速执行，方便快速测试跑通代码（一次随机森林模型优化需要8分钟，五轮交叉训练就需要40分钟，然后还要加上其他模型的训练时间），以及后续进行更大范围不同模型组合和特征组合的效果验证。当然，在本小节的最后，在跑通了整个流程后，会有一次更多迭代次数的运行过程，以测试基于TPE交叉训练的效果极限。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85d44a-a1c3-4a65-bb20-4b69d1a39338",
   "metadata": {},
   "source": [
    "> 此外，估计类的优化算法尽管执行效率很高，但精度其实不如枚举的网格搜索。不过网格搜索所需时间远远高于TPE搜索，如Part 4.2中列举的过程，单模的高精度搜索就需要3-4个小时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cb7eb-43ac-4563-82a0-2294c2df3217",
   "metadata": {},
   "source": [
    "> 对于估计类优化算法，另外一个需要注意的地方就是，优化效果的提升会伴随着迭代次数增加而递减，例如随机森林的前500次迭代就已经达到了0.7864，而后面再增加500次迭代，也仅提升了0.2%准确率。换而言之，算力的消耗相比模型效果的提升，“性价比”是逐渐降低的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00278593-6a32-44f9-bc18-56874f9992a7",
   "metadata": {},
   "source": [
    "- 逻辑回归超参数优化评估器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78f3bc3d-037a-495c-a696-4f3b0d43af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params_space = {'lr_C': hp.uniform('lr_C', 0, 1), \n",
    "                   'lr_penalty': hp.choice('lr_penalty', ['l1', 'l2']), \n",
    "                   'lr_thr': hp.uniform('lr_thr', 0, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "021c6298-5cf1-4002-9bf7-5723267f427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_cascade(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, lr_params_space, max_evals=20):\n",
    "        self.lr_params_space = lr_params_space\n",
    "        self.max_evals = max_evals\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        def hyperopt_lr(params, train=True):\n",
    "            # 读取参数\n",
    "            if train == True:\n",
    "                C = params['lr_C']\n",
    "                penalty = params['lr_penalty']\n",
    "                thr = params['lr_thr']\n",
    "            else: \n",
    "                C = params['lr_C']\n",
    "                penalty = ['l1', 'l2'][params['lr_penalty']]\n",
    "                thr = params['lr_thr']\n",
    "            # 实例化模型\n",
    "            lr = logit_threshold(C = C,  \n",
    "                                 thr = thr, \n",
    "                                 penalty = penalty, \n",
    "                                 solver = 'saga', \n",
    "                                 max_iter = int(1e6))\n",
    "            \n",
    "            if train == True:\n",
    "                res = -cross_val_score(lr, X, y).mean()\n",
    "            else:\n",
    "                res = lr.fit(X, y)\n",
    "\n",
    "            return res\n",
    "\n",
    "        def param_hyperopt_lr(max_evals):\n",
    "            params_best = fmin(fn = hyperopt_lr,\n",
    "                               space = self.lr_params_space,\n",
    "                               algo = tpe.suggest,\n",
    "                               max_evals = max_evals, \n",
    "                               rstate=np.random.RandomState(9))    \n",
    "\n",
    "            return params_best\n",
    "        \n",
    "        lr_params_best = param_hyperopt_lr(self.max_evals)\n",
    "        self.clf = hyperopt_lr(lr_params_best, train=False)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        res_proba = self.clf.predict_proba(X)\n",
    "        return res_proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self.clf.predict(X)\n",
    "        return res\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        res = self.clf.score(X, y)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8ae32-9f46-4d7c-a0a2-0224a31780e3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于逻辑回归的超参数优化过程，由于逻辑回归本身特性导致不需要太多次的搜索就能得到一组较为稳定的结果，但saga优化器运行效率有限，每次计算都需要较长时间，因此默认迭代次数是20次。此外，对于当前数据集来说，逻辑回归约迭代50次左右能达到TPE搜索效果上限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5c11035-1a42-4d9f-a842-bbc84decdc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyper = lr_cascade(lr_params_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3cc666a3-4b6c-4220-a50a-54cfe3ec6bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 20/20 [02:24<00:00,  7.21s/trial, best loss: -0.788717174106247]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr_cascade(lr_params_space={'lr_C': <hyperopt.pyll.base.Apply object at 0x00000288D2BE9460>,\n",
       "                            'lr_penalty': <hyperopt.pyll.base.Apply object at 0x00000288D2BE95B0>,\n",
       "                            'lr_thr': <hyperopt.pyll.base.Apply object at 0x00000288D2BE9490>})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_hyper.fit(X_train_OE, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8fc08ff-3be2-4398-9fcc-7947886a71d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_hyper.predict(X_test_OE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1827679-8085-4d4c-9e55-a70841c65b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7773992049971608"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_hyper.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f6f56-d7c4-42d6-a53e-1170becc860f",
   "metadata": {},
   "source": [
    "- 基于超参数优化评估器的train_cross过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2768f0f-f5e9-4e38-9018-97c45b4ad18b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在定义了三个超参数优化评估器后，我们即可将其带入交叉训练函数中进行超参数搜索，并最终输出oof数据集。这里需要注意，此时带入训练的不再是一个个简单的模型，而是封装为评估器的超参数优化器，因此在实际训练的过程、每次划分完训练集和验证集之后，模型都会在给定的训练集上进行超参数优化和模型训练，然后再在测试集上输出预测结果。我们可以看下由此输出的oof数据集效果如何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ee2943d-2054-4c73-8aaa-4a2a4c9bf20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37fa5746-9aea-490b-a84f-4bec37c7afce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 20/20 [02:05<00:00,  6.30s/trial, best loss: -0.7886390532544378]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:03<00:00,  6.17s/trial, best loss: -0.7872189349112426]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:01<00:00,  6.09s/trial, best loss: -0.7882135213395444]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:02<00:00,  6.15s/trial, best loss: -0.7905773077622504]\n",
      "100%|███████████████████████████████████████████████| 20/20 [01:59<00:00,  5.95s/trial, best loss: -0.7858449788073356]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 23.99trial/s, best loss: -0.7988165680473374]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 23.97trial/s, best loss: -0.7917159763313609]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 24.08trial/s, best loss: -0.7986249248115043]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.69trial/s, best loss: -0.8033578133087135]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.52trial/s, best loss: -0.7912879264761424]\n",
      "100%|█████████████████████████████████████████████| 500/500 [05:46<00:00,  1.44trial/s, best loss: -0.8054437869822484]\n",
      "100%|██████████████████████████████████████████████| 500/500 [07:00<00:00,  1.19trial/s, best loss: -0.810414201183432]\n",
      "100%|█████████████████████████████████████████████| 500/500 [06:12<00:00,  1.34trial/s, best loss: -0.8121112929623567]\n",
      "100%|██████████████████████████████████████████████| 500/500 [06:19<00:00,  1.32trial/s, best loss: -0.810218641151538]\n",
      "100%|█████████████████████████████████████████████| 500/500 [06:59<00:00,  1.19trial/s, best loss: -0.8090352091988754]\n"
     ]
    }
   ],
   "source": [
    "train_oof, test_predict = train_cross(X_train_OE, y_train, X_test_OE, estimators=estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e65c8-de66-4002-b255-5d61c8d4479c",
   "metadata": {},
   "source": [
    "由此，我们自动完成了交叉训练过程中的自动超参数搜索。当然，要自动进行多组模型超参数搜索，还是需要花费一段运行时间的。为了方便后续计算运算时间对于结果提升的“能效比”，我们从此处开始需要统计不同流程得出一个融合结果所需要耗费的计算时间，以上述过程为例，在默认参数设置情况下，一级学习器的训练及oof数据集的创建约需要45分钟。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448cca5-563a-4084-a8cc-1b434c299b21",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行简单测试，以逻辑回归作为元学习器，输出最终融合结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5ecd946-8eb5-4fff-a5f8-4d4e51f0eb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8347216963271488, 0.7904599659284497)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置超参数空间\n",
    "logistic_param = [\n",
    "    {'thr': np.arange(0.1, 1, 0.1).tolist(), 'penalty': ['l1'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['saga']}, \n",
    "    {'thr': np.arange(0.1, 1, 0.1).tolist(), 'penalty': ['l2'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']}, \n",
    "]\n",
    "\n",
    "# 实例化相关评估器\n",
    "logistic_final = logit_threshold(max_iter=int(1e6))\n",
    "    \n",
    "# 执行网格搜索\n",
    "lfg = GridSearchCV(estimator = logistic_final,\n",
    "                   param_grid = logistic_param,\n",
    "                   scoring='accuracy',\n",
    "                   n_jobs = 15).fit(train_oof, y_train)\n",
    "\n",
    "lfg.score(train_oof, y_train), lfg.score(test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcdb4ba5-f363-42a2-b82a-a652a088e80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'penalty': 'l1', 'solver': 'saga', 'thr': 0.5}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09826db-ce5a-425b-a058-a7dd88ed27ea",
   "metadata": {},
   "source": [
    "能够发现，相比单模结果，Stacking融合结果结果有了进一步提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1b1c7-7d2f-4ba9-8029-8905c629c9d4",
   "metadata": {},
   "source": [
    "|得分|训练集|测试集|\n",
    "|:--:|:--:|:--:|\n",
    "|Tree单模|0.7962|0.7768|\n",
    "|RF单模|0.8087|0.7864|\n",
    "|LR单模|0.7887|0.7773|\n",
    "|final-lr|0.8347|0.7904|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b39c41-c605-4e11-b05b-0e1289141a39",
   "metadata": {},
   "source": [
    "### 2.元学习器自动优化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fadbd4-044c-46ea-bb3c-c970401a150a",
   "metadata": {},
   "source": [
    "- 函数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95570366-f3df-43fd-8f64-1f3368f881f9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;既然是为了更高效率、更高精度的执行Stacking融合，除了定义超参数自动优化的交叉训练函数外，还有非常重要的一环，那就是定义元学习器的自动优化函数。在上一小节中，我们总结了元学习器的模型训练流程，即逻辑回归&决策树单模优化、元学习器交叉训练、元学习器Bagging集成等方案，然后从这些模型结果中则优输出。我们可以总结这个流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b14f4-5e81-4da6-9ab0-e8fa2145ed18",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221008155028693.png\" alt=\"image-20221008155028693\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b1310-58c5-4328-918e-997ccc05918b",
   "metadata": {},
   "source": [
    "接下来，我们元学习器优化流程封装为一个函数final_model_opt，在输入元学习器组和超参数空间组的情况下，自动筛选最优元学习器建模流程，并输出在该流程下测试集上的预测结果。函数定义过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0a507182-e728-499d-aea0-13d15dbbee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model_opt(final_model_l, param_space_l, X, y, test_predict):\n",
    "    \"\"\"\n",
    "    Stacking元学习器自动优化与预测函数\n",
    "    \n",
    "    :param final_model_l: 备选元学习器组成的列表\n",
    "    :param param_space_l: 备选元学习器各自超参数搜索空间组成的列表\n",
    "    :param X: oof_train训练集特征\n",
    "    :param y: oof_train训练集标签\n",
    "    :param test_predict: 一级评估器输出的测试集预测结果\n",
    "    \n",
    "    :return：多组元学习器在oof_train上的最佳评分，以及最佳元学习器在test_predict上的预测结果\n",
    "    \"\"\"\n",
    "    \n",
    "    # 不同组元学习器结果存储列表\n",
    "    # res_l用于存储模型在训练集上的评分\n",
    "    res_l = np.zeros(len(final_model_l)).tolist()\n",
    "    # test_predict_l用于存储模型在测试集test_predict上的预测结果\n",
    "    test_predict_l = np.zeros(len(final_model_l)).tolist()\n",
    "    \n",
    "    for i, model in enumerate(final_model_l):\n",
    "        # 输出元学习器单模预测结果\n",
    "        # 执行网格搜索\n",
    "        model_grid = GridSearchCV(estimator = model,\n",
    "                                  param_grid = param_space_l[i],\n",
    "                                  scoring='accuracy',\n",
    "                                  n_jobs = 15)\n",
    "        model_grid.fit(X, y)\n",
    "        # 记录单模最佳模型，方便后续作为Bagging的基础评估器\n",
    "        res1_best_model = model_grid.best_estimator_\n",
    "        # 测试在训练oof数据集上的准确率\n",
    "        res1 = model_grid.score(X, y)\n",
    "        # 输出单模在test_predict上的预测结果\n",
    "        res1_test_predict = model_grid.predict_proba(test_predict)[:, 1]\n",
    "        \n",
    "        # 输出元学习器交叉训练预测结果\n",
    "        res2_temp = np.zeros(y.shape[0])\n",
    "        res2_test_predict = np.zeros(test_predict.shape[0])\n",
    "        # 交叉训练过程附带网格搜索以提升精度\n",
    "        folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=12)\n",
    "        for trn_idx, val_idx in folds.split(X, y):\n",
    "            model_grid = GridSearchCV(estimator = model,\n",
    "                                      param_grid = param_space_l[i],\n",
    "                                      scoring='accuracy',\n",
    "                                      n_jobs = 15)\n",
    "            model_grid.fit(X.loc[trn_idx], y.loc[trn_idx])\n",
    "            res2_temp += model_grid.predict_proba(X)[:, 1] / 10\n",
    "            # 记录测试集上的预测结果\n",
    "            res2_test_predict += model_grid.predict_proba(test_predict)[:, 1] / 10\n",
    "        # 交叉训练模型组评分\n",
    "        res2 = accuracy_score((res2_temp >= 0.5) * 1, y)\n",
    "\n",
    "        # 元学习器的Bagging过程\n",
    "        bagging_param_space = {\"n_estimators\": range(10, 21), \n",
    "                               \"max_samples\": np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "        \n",
    "        bagging_final = BaggingClassifier(res1_best_model)\n",
    "        BG = GridSearchCV(bagging_final, bagging_param_space, n_jobs=15).fit(X, y)\n",
    "        # Bagging元学习器评分\n",
    "        res3 = BG.score(X, y)\n",
    "        # Bagging元学习器在测试集上评分\n",
    "        res3_test_predict = BG.predict_proba(test_predict)[:, 1]\n",
    "        \n",
    "        # 三组模型评分组成列表\n",
    "        res_l_temp = [res1, res2, res3]\n",
    "        # 三组模型在测试集上预测结果组成列表\n",
    "        test_predict_l_temp = [res1_test_predict, res2_test_predict, res3_test_predict]\n",
    "        # 挑选评分最高模型\n",
    "        best_res = np.max(res_l_temp)\n",
    "        # 挑选评分最高模型输出的测试集概率预测结果\n",
    "        best_test_predict = test_predict_l_temp[np.argmax(res_l_temp)]\n",
    "        # 将最佳模型写入res_l对应位置\n",
    "        res_l[i] = best_res\n",
    "        # 将最佳模型在测试集上的评分写入test_predict_l\n",
    "        test_predict_l[i] = best_test_predict\n",
    "        \n",
    "    # 再从res_l中选取训练集上最佳评分\n",
    "    best_res_final = np.max(res_l) \n",
    "    # 根据训练集上的最佳评分，选取挑选最佳测试集预测结果\n",
    "    best_test_predict_final = test_predict_l[np.argmax(res_l)]\n",
    "    \n",
    "    return best_res_final, best_test_predict_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108e55d-d775-4a7c-891e-883e2a5e1863",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们在res1、res2、res3的创建和选取上，增加了更加一层备选，即增加了元学习器的决策树和逻辑回归两个模型备选的过程，即实际上我们是在res1_lr、res2_lr、res3_lr和res1_tree、res2_tree、res3_tree。当然，为了整体迭代方便，这里我们采用了两层筛选的机制，即先从res1_lr、res2_lr、res3_lr中挑选最佳结果，然后和res1_tree、res2_tree、res3_tree最佳结果进行比较"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4cb48-94fb-4d5c-9e2c-8ab5a8644979",
   "metadata": {},
   "source": [
    "- 函数性能及可拓展性讨论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449c3ea-cbc6-4894-9588-0dc755a1d180",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并且需要注意，在大多数情况下，真实的测试集标签其实是不可知的，我们更无法借助测试集的标签取值来筛选模型或者超参数。测试集作为需要被预测的对象，final_model_opt函数最终只能够输出测试集的概率预测结果。当然，如果当前测试集是存在标签的，那么也可以根据best_test_predict_final测试最终融合结果在测试集上的表现。但从通用性角度考虑，final_model_opt函数只提供元评估器优化后的预测结果。此外，对于元学习器（包括元学习器的Bagging评估器）的优化器采用的是更高精度的网格搜索，以便能够高效、精准的输出预测结果。当然，这里的优化器也可以换成贝叶斯优化器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729865a-d9bd-4c8d-a8ee-0140daa83e11",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而原学习器的选择方面，其实在不引入特征增强的情况下，并没有额外的选择，因此其实也可以考虑把原学习器写死在函数内部。而对于评分函数，目前是使用默认准确率，当然无论是函数内部的hyperOPT优化过程还是sklearn的网格搜索优化器，都可以非常便捷的调整评分函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ba646-8c00-4001-a70b-101c02b6a79b",
   "metadata": {},
   "source": [
    "- 函数使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ad911-e0bc-42f9-8cb2-8624652d6922",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来测试函数效果，首先定义函数的元学习器列表。不同于train_cross的一级学习器列表，元学习器列表不需要将单独的模型改写成元组再组成列表，直接实例化模型后组成列表即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cd47bfbb-f07d-4444-b0d4-6267e5302e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3bcba-8cd9-4179-905a-e386dbdf374d",
   "metadata": {},
   "source": [
    "然后是超参数空间列表的创建过程，这里只需要各超参数空间和元学习器的模型顺序保持一致即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8eb743a-a99b-4fe4-9a9d-11ff115ae6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_param = [{'thr': np.arange(0.1, 1.1, 0.1).tolist(), 'penalty': ['l1'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['saga']}, \n",
    "                  {'thr': np.arange(0.1, 1.1, 0.1).tolist(), 'penalty': ['l2'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']}]\n",
    "\n",
    "tree_final_param = {'max_depth': np.arange(2, 16, 1).tolist(), \n",
    "                    'min_samples_split': np.arange(1, 5, 1).tolist(), \n",
    "                    'min_samples_leaf': np.arange(1, 4, 1).tolist(), \n",
    "                    'max_leaf_nodes':np.arange(6, 30, 1).tolist()}\n",
    "\n",
    "param_space_l = [lr_final_param, tree_final_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4355ce-9caa-45c5-aff4-bdec0dfe44da",
   "metadata": {},
   "source": [
    "接下来测试元学习器优化函数的最终效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d0540580-b55c-4971-a35f-ea216690548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_res_final, best_test_predict_final = final_model_opt(final_model_l, param_space_l, train_oof, y_train, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "65e4fce4-6626-4a7b-98a5-8bcffe9c3029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7961385576377058"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score((best_test_predict_final >= 0.5) * 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec80f7-9473-4fd7-a434-f682fc2a1d66",
   "metadata": {},
   "source": [
    "在引入元学习器优化过程后，结果有了明显提升："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d656f5-6414-498c-9288-9f82649d6c1a",
   "metadata": {},
   "source": [
    "|得分|训练集|测试集|\n",
    "|:--:|:--:|:--:|\n",
    "|Tree单模|0.7962|0.7768|\n",
    "|RF单模|0.8087|0.7864|\n",
    "|LR单模|0.7887|0.7773|\n",
    "|final-lr|0.8347|0.7904|\n",
    "|meta-opt|-|0.7961|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f310fb-d8cb-4ff1-864a-03f93762eee9",
   "metadata": {},
   "source": [
    "至此，我们就完整构建了元学习器的训练和优化流程。本部分的final_model_opt函数，以及原学习器的超参数空间都需要写入manual_emsemble.py文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21602f2f-ccca-4373-a835-35e90176f8f0",
   "metadata": {},
   "source": [
    "### 3.自动Stacking模型融合流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01462481-d5f0-425d-b76d-c9efcc95a2c7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在定义了自动交叉训练函数和自动元学习器优化函数之后，即可将二者串联使用，来完成自动Stacking模型融合过程。其基本实现过程流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49294c1e-9b50-4f69-b715-2b9ff52f4538",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221009212324752.png\" alt=\"image-20221009212324752\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9004a2-06d2-4ca1-a961-9741c9ef9e37",
   "metadata": {},
   "source": [
    "该流程能够在极少量代码的情况下快速实现高精度模型融合，并且我们可以非常灵活的通过调整一级学习器的迭代次数来平衡融合效率和融合效果。当我们需要尽可能提升模型结果时，可以尽可能增加迭代次数，令其逼近TPE优化效果上限，以提升最终融合结果。而如果我们是希望快速对比测试不同模型的组合效果、或者对比不同特征分配的情况下融合效果，则可以相对设置更少的迭代次数，来进行快速的运行，此时尽管效果有限，但用于对比测试足以。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e044159-6504-4f70-916c-f60110eef792",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们提高一级学习器交叉训练的迭代次数，尝试借助这个自动Stacking的流程获得一个更好的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bd35f2f7-11d6-4086-90e4-05b893ff6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyper = lr_cascade(lr_params_space, max_evals=50)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space, max_evals=1000)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c545820b-741c-4028-a2bd-0384860fedf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [05:16<00:00,  6.32s/trial, best loss: -0.7924260355029585]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:28<00:00,  6.58s/trial, best loss: -0.7912426035502959]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:13<00:00,  6.28s/trial, best loss: -0.7922346720382727]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:07<00:00,  6.15s/trial, best loss: -0.7964955866101529]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:18<00:00,  6.37s/trial, best loss: -0.7893947151230293]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 23.90trial/s, best loss: -0.7988165680473374]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 24.02trial/s, best loss: -0.7917159763313609]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 24.12trial/s, best loss: -0.7986249248115043]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.69trial/s, best loss: -0.8033578133087135]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.68trial/s, best loss: -0.7912879264761424]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:13<00:00,  1.36trial/s, best loss: -0.8047337278106509]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:49<00:00,  1.30trial/s, best loss: -0.8123076923076924]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:38<00:00,  1.32trial/s, best loss: -0.8128216319051017]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [13:33<00:00,  1.23trial/s, best loss: -0.8111659462559626]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:51<00:00,  1.30trial/s, best loss: -0.8095083022087932]\n"
     ]
    }
   ],
   "source": [
    "train_oof, test_predict = train_cross(X_train_OE, y_train, X_test_OE, estimators=estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb181b1-84d7-4429-b889-4ab1e2218c6b",
   "metadata": {},
   "source": [
    "元学习器的交叉训练约用时1个半小时。其实相比手动训练模型过程，时间还是有大幅缩短的。接下来将元学习器训练数据带入元学习器优化函数，测试效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0b7fff4e-d526-4114-be6e-38aaa77ce44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db76225b-adb3-456b-903d-9b38b3a5438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_res_final, best_test_predict_final = final_model_opt(final_model_l, param_space_l, train_oof, y_train, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "43696e95-a3aa-4b3f-bf13-218410279a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7978421351504826"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score((best_test_predict_final >= 0.5) * 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be156cc-8b16-43c1-82b5-8933b65ceaaf",
   "metadata": {},
   "source": [
    "|得分|训练集|测试集|\n",
    "|:--:|:--:|:--:|\n",
    "|Tree单模|0.7962|0.7768|\n",
    "|RF单模|0.8087|0.7864|\n",
    "|LR单模|0.7887|0.7773|\n",
    "|final-lr|0.8347|0.7904|\n",
    "|meta-opt|-|0.7961|\n",
    "|final-opt|-|0.7978|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8bc16-507b-4581-87a4-23a1edc36a12",
   "metadata": {},
   "source": [
    "最终结果超过此前最好单模成绩（0.7955），低于此前手动融合的模型结果（0.8001）。当然，整体结果仍然是可用的结果，而结果的小幅随机扰动其实会跟优化器、随机数种子有较大关系。而一个自动化的流程，将在大范围自动搜索的过程中发挥更大的作用，也将在后续替换其他集成算法时展示出更优秀的优化结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc7787-132a-424f-bd4c-64631ce4a451",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后，简单探讨下关于暴力计算之于模型融合的实际价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd90e7-d556-4afe-90ce-866844385d5a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其实从本节开始，会有越来越多的长时间代码运算过程，用于复杂优化过程、特征筛选和创建、级联优化等，这也就是所谓的“暴力计算”。而其实暴力计算对于机器学习来说至关重要，很多时候一定量的计算时间，也是一个更好结果的基本保障，人们戏称这个过程为“炼丹”。尽管很多时候我们都希望能够有一个简洁美观的公式，通过一系列高效的计算就能迅速得出一个非常好的结论，但实际上，作为后验的算法，机器学习在很多时候还是需要暴力计算来算出一个还不错的结果。目前来说，无论是企业应用还是算法竞赛，我们都能常常看到复杂代码和暴力计算的身影。其实早在2006年奈飞组织的第一场数据科学竞赛，第一名的队伍就采用了107个算法融合的策略，总训练时间长达2000小时。当然，伴随着算法的不断推陈出新以及特征工程、模型融合的技术更新，模型优化和训练速度大幅提升，一个模型跑2000小时也早已成为历史，但暴力计算的之于深度学习和机器学习仍然非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dcc9a4-b43f-400b-a97a-72b9f865e9ef",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而在课上，围绕“通过暴力计算得出更好结果”这一议题，也将提供更多的实例。不过鉴于个人用户的算例有限，大多数暴力计算过程都会控制在3小时内，确保实战中可以运行。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
