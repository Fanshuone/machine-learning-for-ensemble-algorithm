{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d716bfd-7766-4281-ac85-8122446b99cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> 【Kaggle】Telco Customer Churn 电信用户流失预测案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b17ac-0198-4117-81c6-7f218e2e1421",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a98df-8879-4115-beba-72cce263bb26",
   "metadata": {},
   "source": [
    "## <font face=\"仿宋\">第四部分导读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28104c56-fd3d-42b8-8dc0-c1009c8fe12f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">在案例的第二、三部分中，我们详细介绍了关于特征工程的各项技术，特征工程技术按照大类来分可以分为数据预处理、特征衍生、特征筛选三部分，其中特征预处理的目的是为了将数据集整理、清洗到可以建模的程度，具体技术包括缺失值处理、异常值处理、数据重编码等，是建模之前必须对数据进行的处理和操作；而特征衍生和特征筛选则更像是一类优化手段，能够帮助模型突破当前数据集建模的效果上界。并且我们在第二部分完整详细的介绍机器学习可解释性模型的训练、优化和解释方法，也就是逻辑回归和决策树模型。并且此前我们也一直以这两种算法为主，来进行各个部分的模型测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918f206-5b36-45e8-a42b-d80d804b0424",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而第四部分，我们将开始介绍集成学习的训练和优化的实战技巧，尽管从可解释性角度来说，集成学习的可解释性并不如逻辑回归和决策树，但在大多数建模场景下，集成学习都将获得一个更好的预测结果，这也是目前效果优先的建模场景下最常使用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d9801-b60a-4117-939e-ca4fecf843e9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">总的来说，本部分内容只有一个目标，那就是借助各类优化方法，抵达每个主流集成学习的效果上界。换而言之，本部分我们将围绕单模优化策略展开详细的探讨，涉及到的具体集成学习包括随机森林、XGBoost、LightGBM、和CatBoost等目前最主流的集成学习算法，而具体的优化策略则包括超参数优化器的使用、特征衍生和筛选方法的使用、单模型自融合方法的使用，这些优化方法也是截至目前，提升单模效果最前沿、最有效、同时也是最复杂的方法。其中有很多较为艰深的理论，也有很多是经验之谈，但无论如何，我们希望能够围绕当前数据集，让每个集成学习算法优化到极限。值得注意的是，在这个过程中，我们会将此前介绍的特征衍生和特征筛选视作是一种模型优化方法，衍生和筛选的效果，一律以模型的最终结果来进行评定。而围绕集成学习进行海量特征衍生和筛选，也才是特征衍生和筛选技术能发挥巨大价值的主战场。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57680004-53aa-460f-8e08-f0a56962aecc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而在抵达了单模的极限后，我们就会进入到下一阶段，也就是模型融合阶段。需要知道的是，只有单模的效果到达了极限，进一步的多模型融合、甚至多层融合，才是有意义的，才是有效果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b93e13-682f-4c09-85d5-79cc28a94570",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd103a7-50af-43b0-a68e-1eb1abe101b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center>Part 4.集成算法的训练与优化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6926bf70-1615-44d3-bbc4-c3548b77447a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# 导入模型融合模块\n",
    "import manual_ensemble as me\n",
    "from manual_ensemble import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ff781-361f-462b-bcae-bd7ffc72a792",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后执行Part 1中的数据清洗相关工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89bafc9-6555-42a1-8297-44a414cd5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "tcc = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# 标注连续/离散字段\n",
    "# 离散字段\n",
    "category_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "                'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "                'PaymentMethod']\n",
    "\n",
    "# 连续字段\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    " \n",
    "# 标签\n",
    "target = 'Churn'\n",
    "\n",
    "# ID列\n",
    "ID_col = 'customerID'\n",
    "\n",
    "# 验证是否划分能完全\n",
    "assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]\n",
    "\n",
    "# 连续字段转化\n",
    "tcc['TotalCharges']= tcc['TotalCharges'].apply(lambda x: x if x!= ' ' else np.nan).astype(float)\n",
    "tcc['MonthlyCharges'] = tcc['MonthlyCharges'].astype(float)\n",
    "\n",
    "# 缺失值填补\n",
    "tcc['TotalCharges'] = tcc['TotalCharges'].fillna(0)\n",
    "\n",
    "# 标签值手动转化 \n",
    "tcc['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "tcc['Churn'].replace(to_replace='No',  value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e319c400-9654-4f9f-8204-2174b469ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tcc.drop(columns=[ID_col, target]).copy()\n",
    "labels = tcc['Churn'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0bbda-0a73-4348-9366-6303b89babdf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时，创建自然编码后的数据集以及经过时序特征衍生的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c3bcea-f6a5-4c1b-bcc6-cc642c07e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train, test = train_test_split(tcc, random_state=22)\n",
    "\n",
    "X_train = train.drop(columns=[ID_col, target]).copy()\n",
    "X_test = test.drop(columns=[ID_col, target]).copy()\n",
    "\n",
    "y_train = train['Churn'].copy()\n",
    "y_test = test['Churn'].copy()\n",
    "\n",
    "X_train_seq = pd.DataFrame()\n",
    "X_test_seq = pd.DataFrame()\n",
    "\n",
    "# 年份衍生\n",
    "X_train_seq['tenure_year'] = ((72 - X_train['tenure']) // 12) + 2014\n",
    "X_test_seq['tenure_year'] = ((72 - X_test['tenure']) // 12) + 2014\n",
    "\n",
    "# 月份衍生\n",
    "X_train_seq['tenure_month'] = (72 - X_train['tenure']) % 12 + 1\n",
    "X_test_seq['tenure_month'] = (72 - X_test['tenure']) % 12 + 1\n",
    "\n",
    "# 季度衍生\n",
    "X_train_seq['tenure_quarter'] = ((X_train_seq['tenure_month']-1) // 3) + 1\n",
    "X_test_seq['tenure_quarter'] = ((X_test_seq['tenure_month']-1) // 3) + 1\n",
    "\n",
    "# 独热编码\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_train_seq)\n",
    "\n",
    "seq_new = list(X_train_seq.columns)\n",
    "\n",
    "# 创建带有列名称的独热编码之后的df\n",
    "X_train_seq = pd.DataFrame(enc.transform(X_train_seq).toarray(), \n",
    "                           columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "X_test_seq = pd.DataFrame(enc.transform(X_test_seq).toarray(), \n",
    "                          columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "# 调整index\n",
    "X_train_seq.index = X_train.index\n",
    "X_test_seq.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a9463c-8cf9-44a8-bd41-cfb93f9d3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(X_train[category_cols])\n",
    "\n",
    "X_train_OE = pd.DataFrame(ord_enc.transform(X_train[category_cols]), columns=category_cols)\n",
    "X_train_OE.index = X_train.index\n",
    "X_train_OE = pd.concat([X_train_OE, X_train[numeric_cols]], axis=1)\n",
    "\n",
    "X_test_OE = pd.DataFrame(ord_enc.transform(X_test[category_cols]), columns=category_cols)\n",
    "X_test_OE.index = X_test.index\n",
    "X_test_OE = pd.concat([X_test_OE, X_test[numeric_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d3cd7-9a13-4e63-a039-3866f6408113",
   "metadata": {},
   "source": [
    "然后是模型融合部分所需的第三方库、准备的数据以及训练好的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bcad255-d2e6-4dcd-bdae-94343ec29d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化KFold评估器\n",
    "kf = KFold(n_splits=5, random_state=12, shuffle=True)\n",
    "\n",
    "# 重置训练集和测试集的index\n",
    "X_train_OE = X_train_OE.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "train_part_index_l = []\n",
    "eval_index_l = []\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train_OE, y_train):\n",
    "    train_part_index_l.append(train_part_index)\n",
    "    eval_index_l.append(eval_index)\n",
    "    \n",
    "# 训练集特征\n",
    "X_train1 = X_train_OE.loc[train_part_index_l[0]]\n",
    "X_train2 = X_train_OE.loc[train_part_index_l[1]]\n",
    "X_train3 = X_train_OE.loc[train_part_index_l[2]]\n",
    "X_train4 = X_train_OE.loc[train_part_index_l[3]]\n",
    "X_train5 = X_train_OE.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集特征\n",
    "X_eval1 = X_train_OE.loc[eval_index_l[0]]\n",
    "X_eval2 = X_train_OE.loc[eval_index_l[1]]\n",
    "X_eval3 = X_train_OE.loc[eval_index_l[2]]\n",
    "X_eval4 = X_train_OE.loc[eval_index_l[3]]\n",
    "X_eval5 = X_train_OE.loc[eval_index_l[4]]\n",
    "\n",
    "# 训练集标签\n",
    "y_train1 = y_train.loc[train_part_index_l[0]]\n",
    "y_train2 = y_train.loc[train_part_index_l[1]]\n",
    "y_train3 = y_train.loc[train_part_index_l[2]]\n",
    "y_train4 = y_train.loc[train_part_index_l[3]]\n",
    "y_train5 = y_train.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集标签\n",
    "y_eval1 = y_train.loc[eval_index_l[0]]\n",
    "y_eval2 = y_train.loc[eval_index_l[1]]\n",
    "y_eval3 = y_train.loc[eval_index_l[2]]\n",
    "y_eval4 = y_train.loc[eval_index_l[3]]\n",
    "y_eval5 = y_train.loc[eval_index_l[4]]\n",
    "\n",
    "train_set = [(X_train1, y_train1), \n",
    "             (X_train2, y_train2), \n",
    "             (X_train3, y_train3), \n",
    "             (X_train4, y_train4), \n",
    "             (X_train5, y_train5)]\n",
    "\n",
    "eval_set = [(X_eval1, y_eval1), \n",
    "            (X_eval2, y_eval2), \n",
    "            (X_eval3, y_eval3), \n",
    "            (X_eval4, y_eval4), \n",
    "            (X_eval5, y_eval5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45808472-3ef1-4f5e-98b2-9b206a890a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型组\n",
    "grid_RF_1 = load('./models/grid_RF_1.joblib') \n",
    "grid_RF_2 = load('./models/grid_RF_2.joblib') \n",
    "grid_RF_3 = load('./models/grid_RF_3.joblib') \n",
    "grid_RF_4 = load('./models/grid_RF_4.joblib') \n",
    "grid_RF_5 = load('./models/grid_RF_5.joblib') \n",
    "\n",
    "RF_1 = grid_RF_1.best_estimator_\n",
    "RF_2 = grid_RF_2.best_estimator_\n",
    "RF_3 = grid_RF_3.best_estimator_\n",
    "RF_4 = grid_RF_4.best_estimator_\n",
    "RF_5 = grid_RF_5.best_estimator_\n",
    "\n",
    "RF_l = [RF_1, RF_2, RF_3, RF_4, RF_5]\n",
    "\n",
    "# 决策树模型组\n",
    "grid_tree_1 = load('./models/grid_tree_1.joblib')\n",
    "grid_tree_2 = load('./models/grid_tree_2.joblib')\n",
    "grid_tree_3 = load('./models/grid_tree_3.joblib')\n",
    "grid_tree_4 = load('./models/grid_tree_4.joblib')\n",
    "grid_tree_5 = load('./models/grid_tree_5.joblib')\n",
    "\n",
    "tree_1 = grid_tree_1.best_estimator_\n",
    "tree_2 = grid_tree_2.best_estimator_\n",
    "tree_3 = grid_tree_3.best_estimator_\n",
    "tree_4 = grid_tree_4.best_estimator_\n",
    "tree_5 = grid_tree_5.best_estimator_\n",
    "\n",
    "tree_l = [tree_1, tree_2, tree_3, tree_4, tree_5]\n",
    "\n",
    "# 逻辑回归模型组\n",
    "grid_lr_1 = load('./models/grid_lr_1.joblib')\n",
    "grid_lr_2 = load('./models/grid_lr_2.joblib')\n",
    "grid_lr_3 = load('./models/grid_lr_3.joblib')\n",
    "grid_lr_4 = load('./models/grid_lr_4.joblib')\n",
    "grid_lr_5 = load('./models/grid_lr_5.joblib')\n",
    "\n",
    "lr_1 = grid_lr_1.best_estimator_\n",
    "lr_2 = grid_lr_2.best_estimator_\n",
    "lr_3 = grid_lr_3.best_estimator_\n",
    "lr_4 = grid_lr_4.best_estimator_\n",
    "lr_5 = grid_lr_5.best_estimator_\n",
    "\n",
    "lr_l = [lr_1, lr_2, lr_3, lr_4, lr_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b872c35d-f86e-4f3e-81a6-f6b171340c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1_predict_proba_RF = pd.Series(RF_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_RF = pd.Series(RF_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_RF = pd.Series(RF_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_RF = pd.Series(RF_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_RF = pd.Series(RF_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_RF = pd.concat([eval1_predict_proba_RF, \n",
    "                                   eval2_predict_proba_RF, \n",
    "                                   eval3_predict_proba_RF, \n",
    "                                   eval4_predict_proba_RF, \n",
    "                                   eval5_predict_proba_RF]).sort_index()\n",
    "\n",
    "eval1_predict_proba_tree = pd.Series(tree_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_tree = pd.Series(tree_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_tree = pd.Series(tree_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_tree = pd.Series(tree_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_tree = pd.Series(tree_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_tree = pd.concat([eval1_predict_proba_tree, \n",
    "                                     eval2_predict_proba_tree, \n",
    "                                     eval3_predict_proba_tree, \n",
    "                                     eval4_predict_proba_tree, \n",
    "                                     eval5_predict_proba_tree]).sort_index()\n",
    "\n",
    "eval1_predict_proba_lr = pd.Series(lr_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_lr = pd.Series(lr_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_lr = pd.Series(lr_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_lr = pd.Series(lr_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_lr = pd.Series(lr_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_lr = pd.concat([eval1_predict_proba_lr, \n",
    "                                   eval2_predict_proba_lr, \n",
    "                                   eval3_predict_proba_lr, \n",
    "                                   eval4_predict_proba_lr, \n",
    "                                   eval5_predict_proba_lr]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951cc5c-12b0-4527-8e7a-5864747ce06a",
   "metadata": {},
   "source": [
    "- 回归问题与分类问题融合方法差异"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cce1e7-6030-47f1-b1b9-be03fd3fd35e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总的来说，无论是分类问题还是回归问题，模型融合的方法大类和基本融合思想并无本质区别，并且都一定程度被过拟合问题所困扰。在实际应用过程中，除了不同问题要带入不同类型模型来进行建模，还有两点区别需要注意：\n",
    "- 首先是硬投票方法簇只能处理分类问题，包括硬投票、绝对多数票和排序融合法等方法，只适用于分类问题；\n",
    "- 其次，对于Stacking和Blending来说，回归问题的元学习器选取主要以贝叶斯回归为主，有时也会考虑岭回归和LASSO；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b7371-ae6d-4676-b45d-b2b72de91c8f",
   "metadata": {},
   "source": [
    "当然，关于更多回归问题元学习器选取与优化的方法，会在实践过程中进行详细探讨。而对于所涉及到的新的贝叶斯回归模型，我们也将在本节对其进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dd37a-9e87-491e-977b-f6b172280d8d",
   "metadata": {},
   "source": [
    "- 回归问题中模型融合效果与建模难度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79146c6b-b562-4514-b963-9348df8c8a66",
   "metadata": {},
   "source": [
    "&emsp;&emsp;尽管方法类似，但模型融合应用于回归问题时，往往更容易获得一个更好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dd054-34e1-4dad-b974-27361375b4c7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在此前的分类问题模型融合实践过程中我们发现，对于分类问题，融合一个更好的结果其实并不容易，往往简单的通用方法会适得其反，而只有广泛尝试优化策略、并因地制宜进行调整，才往往能够获得一个更好的融合结果。但对于回归问题，可能并不存在这样的困扰。大多数回归问题在套用一个基础融合流程时就能活动一定程度的效果提升，并且很多优化方法也能起到立竿见影的效果。因此对于实践环节来说，就通过融合来提升效果这一点而言，回归问题要比分类问题更容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d14c2-6d39-4c86-bef2-f9f13e64f69b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;究其原因，其实是因为回归问题的数值预测给了第二阶段学习更大的学习空间。无论是平均法还是学习结合器法，由于数据的重复学习，过拟合问题始终是模型融合的“头号大敌”。且尽管分类问题大多数情况是围绕概率值进行连续性数值的预测和计算，但最终输出的预测结果仍然是离散变量，融合过程中更加丰富的数值多样性表现不一定能直接体现在最终的预测结果上。而回归问题则自始自终都是围绕连续性数值进行预测，哪怕融合结果有千分之一的数值提升，都能提升最终结果评估指标。此外，就建模难度而言，回归问题的建模难度往往高于分类问题，而更高的建模难度也给复杂的融合流程提供了更大的学习空间，适得一些在分类问题上容易过拟合的流程、在回归问题上反而能起到提高泛化能力的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca2fa2-0604-4572-a68a-f2e8c0b9a65f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，效果更易提升并不代表回归问题整体模型融合执行流程变得更简单。实际上，对于回归问题来说，简单流程能提升效果、复杂流程更有可能提升更大。因此，面对回归问题，我们也需要反复多次尝试，训练构建多组模型融合结果来则优输出，并且如果是在测试集标签未知的情况下，也同样是需要多次线上提交来测试最终融合效果的。这一点并不会随着融合出效果的难易而发生变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938442d4-06d5-4816-ae26-49afa91e70e8",
   "metadata": {},
   "source": [
    "> 在实际应用场景中，总的来看，分类问题的场景略多于回归问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3744efd-bb2c-4cfe-b3f6-afa278fb4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_proba_RF = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF.append(RF_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_RF = np.array(test_predict_proba_RF)\n",
    "test_predict_proba_RF = test_predict_proba_RF.mean(0)\n",
    "\n",
    "test_predict_proba_tree = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree.append(tree_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_tree = np.array(test_predict_proba_tree)\n",
    "test_predict_proba_tree = test_predict_proba_tree.mean(0)\n",
    "\n",
    "test_predict_proba_lr = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr.append(lr_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_lr = np.array(test_predict_proba_lr)\n",
    "test_predict_proba_lr = test_predict_proba_lr.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043e137-300e-491a-9690-06594d8bb140",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center>Ch.3.11 回归集成算法优化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca0978-9176-4328-9fbb-d60655eb65f7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在此前的内容中，我们已经完整介绍了目前模型融合中全部常用方法及其优化技巧，但之前的内容都是基于分类问题进行的技术介绍。本节开始，我们将进一步探讨这些模型融合方法在回归类问题中的应用方法，并在manual_ensemble函数库添加更多支持回归问题融合的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca93192-42d7-4d5e-861f-8eae14c5b996",
   "metadata": {},
   "source": [
    "- 数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d70ace-1a66-41c7-9d7f-15521d71649c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们将借助一个回归数据集，来尝试进行回归类问题的模型融合。这里我们选取sklearn中的加州房价数据集:[California Housing dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)来进行建模实验。该数据集是一个来自StatLib的统计数据集，属于统计数据集中偏难得数据集，但对于当前机器学习来说建模难度并不大，非常适合实验性质建模训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551b747-fd96-45f3-91ba-1ffc701a4158",
   "metadata": {},
   "source": [
    "> 所谓统计数据集，往往指的是出于科研或调研需要，人工搜集统计的来得数据集。例如鸢尾花数据集就是一个统计数据集，出于生物科学研究，统计学家、生物学家Fisher围绕鸢尾花的不同性状特征来进行的精细化测量和记录的结果。由于统计数据集数据记录成本较高，因此数据集的特征往往是经过精心设计的，且往往都和标签具备业务上的关联性。并且，每条数据往往也是按照标准格式进行记录，相当于是清洗后的数据，这也是自动化数据采集设备出现之前人们进行数据记录的常态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2efb9-e880-4188-a0e8-385301449be2",
   "metadata": {},
   "source": [
    "> 本节内容重点介绍如何在回归问题中进行有效的模型融合，后续案例中将会有更加复杂的回归问题建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ada5f3-5041-4748-9a68-023802749425",
   "metadata": {},
   "source": [
    "&emsp;&emsp;该数据集可以直接通过如下方式从sklearn中导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e792f6ff-00cb-4132-ab21-d826ec3a52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f376a-f2b5-46ec-97a3-fc6214c767c4",
   "metadata": {},
   "source": [
    "sklearn导入的数据对象类型是data类型，我们需要从中提取我们所需的数据特征、标签、特征名称等，并将其转化为Dataframe格式类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1dbb394-9ff3-40ac-ba33-83609b6abe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "cal_housing = fetch_california_housing()\n",
    "# 提取特征及特征名称，并转化为DataFrame\n",
    "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "# 提取标签\n",
    "y = cal_housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22eea38d-1397-4884-bacc-8622c1f5c79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1604c883-0941-4dc3-83ea-c6a5d8b39308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96131166-ce39-478b-8da9-a13b39885e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.870671</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>5.429000</td>\n",
       "      <td>1.096675</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>3.070655</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>-119.569704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.899822</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2.474173</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>10.386050</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>2.003532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.563400</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.440716</td>\n",
       "      <td>1.006079</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>2.429741</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>-121.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.534800</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.229129</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>2.818116</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>-118.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.743250</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.052381</td>\n",
       "      <td>1.099526</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>3.282261</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>-118.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
       "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
       "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
       "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
       "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
       "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
       "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
       "\n",
       "           AveOccup      Latitude     Longitude  \n",
       "count  20640.000000  20640.000000  20640.000000  \n",
       "mean       3.070655     35.631861   -119.569704  \n",
       "std       10.386050      2.135952      2.003532  \n",
       "min        0.692308     32.540000   -124.350000  \n",
       "25%        2.429741     33.930000   -121.800000  \n",
       "50%        2.818116     34.260000   -118.490000  \n",
       "75%        3.282261     37.710000   -118.010000  \n",
       "max     1243.333333     41.950000   -114.310000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f430a-5059-4ae6-a040-50bfc9e5c710",
   "metadata": {},
   "source": [
    "该数据源自上世纪90年代美国人口普查统计得到的数据，集记录了加利福尼亚地区房屋价值的中位数，其中标签（房屋价值）的单位是10万美元。其中，每条数据代表每个街区统计汇总得到的数据，各字段解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbddab-e41f-4d0c-ab00-6c3270b5208d",
   "metadata": {},
   "source": [
    "| 字段 | 解释 |\n",
    "| ------ | ------ |\n",
    "| MedInc | 街区收入中位数（10万） |\n",
    "| HouseAge | 街区房屋年龄中位数 |\n",
    "| AveRooms | 街区房屋平均房间数量 |\n",
    "| AveBedrms | 街区房屋平均卧室数量 |\n",
    "| Population | 街区人口数量 |\n",
    "| AveOccup | 街区平均家庭成员数量 |\n",
    "| Latitude | 街区纬度 |\n",
    "| Longitude | 街区经度 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd6dd8-648d-4bab-87c8-5fc79e302c71",
   "metadata": {},
   "source": [
    "能够发现数据集中全部特征均为连续特征，且多为统计指标，且由于数据集并没有任何缺失值，因此不需要进行数据清洗。另外，考虑到后续模型均为集成算法，因此也不需要对该数据集进行任何数据重编码或者分箱操作，直接带入建模即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c90e3a-a7fa-4e66-af4b-fbd7de7a9304",
   "metadata": {},
   "source": [
    "> 一般来说，对于多个相邻的区域，经纬度亦可视作连续变量，具体数值上的差异代表地理位置上的相邻程度，很多指标（包括该数据集的标签：房价）都和地理位置息息相关。而其他情况，例如假设数据集中统计的是不同国家不同城市的平均房价，则由于地理位置相隔较远，经纬度本身数值并无意义，此时可以考虑将经纬度进行自然数编码，将其转化为名义型离散变量，方便后续建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea3418-98ba-4948-8fe2-165e946256ed",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来划分训练集和测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a4dea9-2131-4612-9d95-3186303f11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58dc7ec3-6f88-47b0-921c-9159173ff197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c93455-1118-46a1-9604-ae4f75f8ab8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>6.6134</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.560729</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>3.141700</td>\n",
       "      <td>37.93</td>\n",
       "      <td>-121.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14652</th>\n",
       "      <td>2.3578</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.455598</td>\n",
       "      <td>1.007722</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>4.131274</td>\n",
       "      <td>32.80</td>\n",
       "      <td>-117.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>5.5111</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.716747</td>\n",
       "      <td>1.037905</td>\n",
       "      <td>3903.0</td>\n",
       "      <td>2.689869</td>\n",
       "      <td>34.25</td>\n",
       "      <td>-118.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>8.1124</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.623188</td>\n",
       "      <td>1.019324</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>2.785024</td>\n",
       "      <td>34.11</td>\n",
       "      <td>-118.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18445</th>\n",
       "      <td>6.2957</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.627832</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>3.443366</td>\n",
       "      <td>37.25</td>\n",
       "      <td>-121.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "1652   6.6134       4.0  6.560729   0.939271      1552.0  3.141700     37.93   \n",
       "14652  2.3578      41.0  5.455598   1.007722      1070.0  4.131274     32.80   \n",
       "3548   5.5111      16.0  5.716747   1.037905      3903.0  2.689869     34.25   \n",
       "6730   8.1124      52.0  6.623188   1.019324      1153.0  2.785024     34.11   \n",
       "18445  6.2957      25.0  6.627832   1.008091      2128.0  3.443366     37.25   \n",
       "\n",
       "       Longitude  \n",
       "1652     -121.97  \n",
       "14652    -117.15  \n",
       "3548     -118.61  \n",
       "6730     -118.14  \n",
       "18445    -121.81  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a74f7-e53f-417a-bc69-b5f4272d6a4c",
   "metadata": {},
   "source": [
    "然后重置index："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d142c09-058b-4a72-9227-e5eea2787aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "273316f1-7aab-4486-b05e-2c3df89a6c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.6134</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.560729</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>3.141700</td>\n",
       "      <td>37.93</td>\n",
       "      <td>-121.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.3578</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.455598</td>\n",
       "      <td>1.007722</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>4.131274</td>\n",
       "      <td>32.80</td>\n",
       "      <td>-117.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5111</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.716747</td>\n",
       "      <td>1.037905</td>\n",
       "      <td>3903.0</td>\n",
       "      <td>2.689869</td>\n",
       "      <td>34.25</td>\n",
       "      <td>-118.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.1124</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.623188</td>\n",
       "      <td>1.019324</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>2.785024</td>\n",
       "      <td>34.11</td>\n",
       "      <td>-118.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2957</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.627832</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>3.443366</td>\n",
       "      <td>37.25</td>\n",
       "      <td>-121.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  6.6134       4.0  6.560729   0.939271      1552.0  3.141700     37.93   \n",
       "1  2.3578      41.0  5.455598   1.007722      1070.0  4.131274     32.80   \n",
       "2  5.5111      16.0  5.716747   1.037905      3903.0  2.689869     34.25   \n",
       "3  8.1124      52.0  6.623188   1.019324      1153.0  2.785024     34.11   \n",
       "4  6.2957      25.0  6.627832   1.008091      2128.0  3.443366     37.25   \n",
       "\n",
       "   Longitude  \n",
       "0    -121.97  \n",
       "1    -117.15  \n",
       "2    -118.61  \n",
       "3    -118.14  \n",
       "4    -121.81  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d3bce-1bbd-4c67-b616-dd247be9f519",
   "metadata": {},
   "source": [
    "> 此外，由于数据集整体数据较为规整，且为了快速进入到模型融合环节，此处省略数据探索部分操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaa6de-ff05-4d10-a203-6021fc0977e5",
   "metadata": {},
   "source": [
    "## 十二、回归问题的集成学习超参数优化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2745d1-abcf-46d2-884e-410b9572a0d5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于当前回归任务，这里考虑分别训练随机森林、极端随机树和GBDT三个模型，测试单模建模效果，并作为一级学习器带入后续的融合环节中。这三个模型属于集成学习中较强的算法，同时也是sklearn内部集成的效果最好的集成学习算法，也是算法工程人员必须掌握使用的三个算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6358e-b669-4507-b09c-70fb8675c8c9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，首先围绕这三个算法进行模型训练与优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a167ebfd-19c2-43a2-8760-9d212d9bf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a65809-d10f-4271-abcd-910c58bc9363",
   "metadata": {},
   "source": [
    "然后测试单模在默认超参数情况下下的预测能力，这里以MSE作为模型评估指标，模型测试结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "362f2254-588b-40cf-a250-d52ee35e62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of RandomForest:\n",
      "Train-MSE: 0.035168, Test-MSE: 0.250861\n",
      "The results of ExtraTrees:\n",
      "Train-MSE: 0.000000, Test-MSE: 0.243643\n",
      "The results of GBDT:\n",
      "Train-MSE: 0.260555, Test-MSE: 0.277794\n"
     ]
    }
   ],
   "source": [
    "# 随机森林\n",
    "reg1 = RandomForestRegressor(random_state=12).fit(X_train, y_train)\n",
    "print('The results of RandomForest:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(reg1.predict(X_train), y_train), \n",
    "                                       mean_squared_error(reg1.predict(X_test), y_test)))\n",
    "\n",
    "# 极端随机树\n",
    "reg2 = ExtraTreesRegressor(random_state=12).fit(X_train, y_train)\n",
    "print('The results of ExtraTrees:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(reg2.predict(X_train), y_train), \n",
    "                                       mean_squared_error(reg2.predict(X_test), y_test)))\n",
    "\n",
    "# GBDT\n",
    "reg3 = GradientBoostingRegressor(random_state=12).fit(X_train, y_train)\n",
    "print('The results of GBDT:')\n",
    "print('Train-MSE: %f, Test-MSE: %f' % (mean_squared_error(reg3.predict(X_train), y_train), \n",
    "                                       mean_squared_error(reg3.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326cc31-8ca6-494b-adfd-a40b4d160798",
   "metadata": {},
   "source": [
    "能够看出，三个模型在默认参数下都取得了较好的预测结果，其中随机森林和极端随机树出现了一定的过拟合情况，有待后续通过超参数优化来解决这一问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72639485-6745-4ea1-854d-45b589960a67",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并且，通过这组结果我们能够看出，极端随机树和随机森林模型表现较为接近，甚至对数据的拟合能力有过之而无不及。其实在大多数实际情况下，对于简单数据集，极端随机树往往会表现出非常强的预测能力，而对于复杂数据集，则整体性能会弱于随机森林，这也就是所谓的方差较小但偏差较大的模型的具体性能体现。也正是由于该特性，使得极端随机树具备了作为基础学习器进一步参与“集成”的可能性，在模型融合过程中该模型是非常值得尝试的一级学习器，而深度森林算法，则更是将其视作唯一的基础学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a195f-3bb8-4163-abe0-024764bfd391",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进一步考虑对这三个模型进行超参数优化。这里我们仍然考虑使用TPE搜索对其进行超参数优化。出于某些情况考虑，我们先计算这组模型在默认超参数情况下交叉验证均值得分情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1782243e-7eab-4859-9c11-5839d5b618b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of RandomForest:\n",
      "Train-cross-mean: 0.259973\n",
      "The results of ExtraTrees:\n",
      "Train-cross-mean: 0.258106\n",
      "The results of GBDT:\n",
      "Train-cross-mean: 0.287060\n"
     ]
    }
   ],
   "source": [
    "# 随机森林\n",
    "reg1 = RandomForestRegressor(random_state=12)\n",
    "print('The results of RandomForest:')\n",
    "print('Train-cross-mean: %f' % -(cross_val_score(reg1, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15)).mean())\n",
    "\n",
    "# 极端随机树\n",
    "reg2 = ExtraTreesRegressor(random_state=12)\n",
    "print('The results of ExtraTrees:')\n",
    "print('Train-cross-mean: %f' % -(cross_val_score(reg2, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15)).mean())\n",
    "\n",
    "# GBDT\n",
    "reg3 = GradientBoostingRegressor(random_state=12)\n",
    "print('The results of GBDT:')\n",
    "print('Train-cross-mean: %f' % -(cross_val_score(reg3, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8ab0d-8505-4d2a-930b-63df224bc61c",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| <center>随机森林 | <center>0.0351 |<center> 0.2599 | <center>0.2508 |\n",
    "| <center>极端随机树 | <center>0 |<center> 0.2581 | <center>0.2436 |\n",
    "| <center>GBDT | <center>0.2605 |<center> 0.2870 | <center>0.2777 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdb671-3228-4641-b590-91bc302f050d",
   "metadata": {},
   "source": [
    "能够发现，相比训练集得分，交叉验证得分和测试集得分更加接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf08536-4402-40db-9f3b-24340fb6e0ce",
   "metadata": {},
   "source": [
    "### 1.回归随机森林超参数优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7890b037-b8c6-46ac-a719-0033ed8a64d1",
   "metadata": {},
   "source": [
    "#### 1.1 基于TPE的回归随机森林模型超参数搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386b491-1845-4289-a6f2-bf2e98fabdd9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是随机森林模型。对于回归随机森林模型的超参数优化，具体操作层面和随机森林的分类模型并无区别。尽管随机森林回归模型的criterion和分类模型的criterion可选参数不同，但在实际超参数优化过程中并不建议对其进行调整。该参数的调整对结果并无明显改善，且调整默认criterion取值也将极大程度影响模型运算速度。因此我们几乎可以创建一个和分类随机森林模型超参数搜索空间完全相同的搜索空间进行超参数优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "376d5904-2ef7-4a13-91fa-0a1d4e0b2f06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'squared_error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : int, default=100\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``n_estimators`` changed from 10 to 100\n",
       "       in 0.22.\n",
       "\n",
       "criterion : {\"squared_error\", \"absolute_error\", \"poisson\"},             default=\"squared_error\"\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"squared_error\" for the mean squared error, which is equal to\n",
       "    variance reduction as feature selection criterion, \"absolute_error\"\n",
       "    for the mean absolute error, and \"poisson\" which uses reduction in\n",
       "    Poisson deviance to find splits.\n",
       "    Training using \"absolute_error\" is significantly slower\n",
       "    than when using \"squared_error\".\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Mean Absolute Error (MAE) criterion.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "       Poisson criterion.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
       "        version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
       "        version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `round(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "bootstrap : bool, default=True\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole dataset is used to build each tree.\n",
       "\n",
       "oob_score : bool, default=False\n",
       "    Whether to use out-of-bag samples to estimate the generalization score.\n",
       "    Only available if bootstrap=True.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
       "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
       "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors. See :term:`Glossary\n",
       "    <n_jobs>` for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls both the randomness of the bootstrapping of the samples used\n",
       "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
       "    features to consider when looking for the best split at each node\n",
       "    (if ``max_features < n_features``).\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "max_samples : int or float, default=None\n",
       "    If bootstrap is True, the number of samples to draw from X\n",
       "    to train each base estimator.\n",
       "\n",
       "    - If None (default), then draw `X.shape[0]` samples.\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
       "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "base_estimator_ : DecisionTreeRegressor\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "estimators_ : list of DecisionTreeRegressor\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
       "    Prediction computed with out-of-bag estimate on the training set.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
       "sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
       "    tree regressors.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data,\n",
       "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
       "of the criterion is identical for several splits enumerated during the\n",
       "search of the best split. To obtain a deterministic behaviour during\n",
       "fitting, ``random_state`` has to be fixed.\n",
       "\n",
       "The default value ``max_features=\"auto\"`` uses ``n_features``\n",
       "rather than ``n_features / 3``. The latter was originally suggested in\n",
       "[1], whereas the former was more recently justified empirically in [2].\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
       "\n",
       ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
       "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.ensemble import RandomForestRegressor\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
       "...                        random_state=0, shuffle=False)\n",
       ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
       ">>> regr.fit(X, y)\n",
       "RandomForestRegressor(...)\n",
       ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
       "[-8.32987858]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RandomForestRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ea405-b1ca-4666-a262-0fea493072b9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们仿造分类随机森林的超参数设置，创建超参数搜索空间如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4af86acd-3602-447d-968b-b1736c403efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 20, 1), \n",
    "            'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1), \n",
    "            'max_depth': hp.quniform('max_depth', 2, 50, 1), \n",
    "            'max_leaf_nodes': hp.quniform('max_leaf_nodes', 50, 300, 1), \n",
    "            'n_estimators': hp.quniform('n_estimators', 50, 500, 1), \n",
    "            'max_samples': hp.uniform('max_samples', 0.2, 0.8)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99498cd-a7cc-4717-ad47-fb31e8348bdf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于上述超参数搜索空间，有两点需要请注意，其一是超参数搜索空间创建函数选择上，这里在创建超参数搜索空间时对于连续型整数搜索空间采用了quniform的方法进行创建，通过该方法创建的搜索空间最终返回最优解将是一个空间内最优解取值，而非最优解的索引值，如此一来，相比choice创建的搜索空间（返回的是索引），使用quniform则可在定义目标函数时不再区分训练状态和测试状态不同的超参数读取流程。但这里由两点需要注意：其一，quniform默认是在连续变量之间搜索，虽然通过设置步长可以锁定在几个整数值之间搜索，但默认仍然是假定搜索空间是连续分布的，对于贝叶斯估计来说，尽管对连续分布空间的估计会更快，但连续分布中的估计方式并不适用于本质上仍然是离散取值的一组变量，因此（相比choice方法）估计的精度会下降；其二，quniform创建的搜索空间，每次读取的超参数取值都是连续型变量，需要将其转化为整型再输出到模型中，否则模型将无法读取参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb07ec5-4d0d-4ab3-87f1-48ad6416d7aa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其二则是关于具体超参数选择及超参数空间范围选择。根据此前介绍的决策树和随机森林基本原理可知，相比分类问题，回归问题中树的复杂度往往要高得多，因此回归随机森林中对于单独每个树的剪枝参数往往需要设置一个较大的搜索空间，例如在当前问题中，max_leaf_nodes就设置了在50到300之间，这其实就相当于给单独基础分类器的复杂度设置了较大的容忍空间。接下来定义目标函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5515b508-8444-4e8d-864c-07f9b07c786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    min_samples_leaf = int(params['min_samples_leaf'])\n",
    "    min_samples_split = int(params['min_samples_split'])\n",
    "    max_depth = int(params['max_depth'])\n",
    "    max_leaf_nodes = int(params['max_leaf_nodes'])\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    max_samples = params['max_samples']\n",
    "\n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(min_samples_leaf = min_samples_leaf, \n",
    "                                   min_samples_split = min_samples_split,\n",
    "                                   max_depth = max_depth, \n",
    "                                   max_leaf_nodes = max_leaf_nodes, \n",
    "                                   n_estimators = n_estimators, \n",
    "                                   max_samples = max_samples)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_RF, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2af4c6-0c65-43c7-8c39-4000a5643411",
   "metadata": {},
   "source": [
    "同样，训练过程返回MSE，测试过程返回训练好的模型。其中neg_mean_squared_error返回的是-MSE，再加上符号最终得到MSE，然后求MSE最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a925e-b13f-4972-a319-1c4a47c66319",
   "metadata": {},
   "source": [
    "&emsp;&emsp;优化函数定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e630eab-bf0b-429f-9889-dd7a9545b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    params_best = fmin(RF_param_objective,\n",
    "                       space = RF_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfac053-a5f7-4bec-b75a-492867fb398d",
   "metadata": {},
   "source": [
    "然后开始执行超参数搜索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0db7b389-146b-4b58-b318-d29133593edb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 500/500 [1:05:48<00:00,  7.90s/trial, best loss: 0.28503053546834717]\n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb1e4b-afbc-4e59-b316-9fed94315db8",
   "metadata": {},
   "source": [
    "查看最优超参数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd889897-4b2b-41d8-bcce-90bd61a3ec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 19.0,\n",
       " 'max_leaf_nodes': 300.0,\n",
       " 'max_samples': 0.774285170020038,\n",
       " 'min_samples_leaf': 3.0,\n",
       " 'min_samples_split': 19.0,\n",
       " 'n_estimators': 425.0}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfde3b2-c241-4005-867e-6ff02e923eec",
   "metadata": {},
   "source": [
    "然后输出最佳超参数组在训练集上训练得到的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa66c379-8742-489e-b1d3-bdf89d7ed079",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa81cea4-2d02-45f0-a7c5-08545cbaedbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=19, max_leaf_nodes=300,\n",
       "                      max_samples=0.774285170020038, min_samples_leaf=3,\n",
       "                      min_samples_split=19, n_estimators=425)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22dac2-c551-45d7-992f-b4ebea8d4064",
   "metadata": {},
   "source": [
    "测试模型在训练集和测试集上表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd29df13-53cc-444d-aa8e-3b376e4116c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1943075293502321, 0.2760535922830271)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg.predict(X_train), y_train), mean_squared_error(RF_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c565c-ee6c-4c4a-af3c-68ecdb0117f4",
   "metadata": {},
   "source": [
    "| 训练方式 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| 默认超参数 | <center>0.0351 |<center>0.2599 | <center>0.2508 |\n",
    "| 小范围搜索 | <center>0.1943 |<center>0.2850 | <center>0.2760 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362c2ee-14cb-4402-9dc8-2bf85b1f9ed5",
   "metadata": {},
   "source": [
    "通过上述结果，有以下几个要点需要深度探讨："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4609ee6-b5b2-4db2-b5ce-e49218442bab",
   "metadata": {},
   "source": [
    "- 首先，基于交叉验证评分的搜索过程是有效的        \n",
    "&emsp;&emsp;经过超参数优化，模型过拟合倾向已得到较为有效的抑制，尽管目前训练集上评分和测试集评分还是有一定差距（还是存在一定过拟合倾向），但交叉验证中验证集的平均得分和测试集得分接近，说明交叉验证得分已经能够较好的衡量模型的泛化能力，因此搜索迭代的方向也是朝着提升模型泛化能力方向进行的，上述过程是有效的；但于此同时，模型在测试集上的评分仍然较低，甚至比默认超参数情况下预测结果更差，而在搜索流程没问题的情况下，问题可能就出在超参数搜索空间的设计上；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9a5e3-b426-4145-a429-9d46e3cb38ac",
   "metadata": {},
   "source": [
    "- 其次，简单回归问题也需要较大的超参数搜索空间        \n",
    "&emsp;&emsp;根据超参数搜索结果我们发现，哪怕是在一个较大的超参数搜索空间内进行搜索，部分参数的最佳取值仍然取得了（或者逼近了）搜索空间的上限（'max_leaf_nodes'和'n_estimators'），这说明对于当前数据集，超参数的搜索空间还应该更进一步放大。但进一步放大超参数搜索空间必然导致搜索时间进一步增加，且这个进一步放大的空间的边界也不易确定，关于如何确定超参数搜索空间，其实也就是很多回归问题超参数优化过程面临的最核心问题；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225bb80-3208-4055-ba55-ea94454fa995",
   "metadata": {},
   "source": [
    " - 其三，当前的过拟合风险实际上是由于超参数搜索不完整所导致的        \n",
    "&emsp;&emsp;能够发现，此处为何经过超优化后的模型仍然存在一定过拟合问题，也是一个非常值得深度探讨的问题。我们知道此时模型参与搜索的部分超参数的取值是已经确定了的，但其他还有一些没有搜索的超参数，其默认取值是按照模型复杂度最高的标准进行的设置，因此这会导致模型在训练参数的过程仍然还是具备一定的过拟合风险。不过该问题并不会很大程度影响模型搜索效果——因为交叉验证能很好的得到一组和测试集较为接近的评分。因此，在以交叉验证结果作为模型泛化能力评估标准的时候，这种过拟合并不会影响超参数搜索优化过程；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfb4b1-1647-4eb6-9704-4dbac601ff5f",
   "metadata": {},
   "source": [
    "> 需要注意的是，这种因为带入搜索的超参数不完全所导致的过拟合风险，主要体现在回归类模型的超参数搜索中，分类问题往往没有这种问题。并且相对简单的回归数据集问题尤其明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2daf168-423b-47ed-8c45-3f273d61e0a3",
   "metadata": {},
   "source": [
    "> 此外，切莫以为这里的过拟合风险是搜索空间太大导致，若裁剪搜索空间，不仅不会消除过拟合风险，反而会降低模型学习能力。这里简单进行一组测试，当缩小超参数空间时，最终模型学习能力如下：<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221030161102038.png\" alt=\"image-20221030161102038\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b42c3-93f6-4c21-bdcf-9a929acea9be",
   "metadata": {},
   "source": [
    "#### 1.2 大范围超参数搜索优化（方案A）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c7c59-637e-4944-a192-d1a951c6d364",
   "metadata": {},
   "source": [
    "&emsp;&emsp;基于此，后续应该如何进行优化呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229c8d0-37a1-419b-8468-0e5f46c0694d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先也是最容易想到的策略就是增加搜索的超参数，以此降低模型过拟合风险，并且同时增加各超参数搜索范围，以此提升模型泛化能力。这当然是理论上的最优方案，但实际执行难度较大，最核心的困难就在于算力不够。目前的数据集在当前搜索空间范围内迭代500次需要1小时时间，而如果增加超参数数量、并增加搜索空间，超参数优化计算所需时间将呈指数级上升。此外，对于部分模型的部分超参数甚至根本无法带入进行搜索，例如回归随机森林中的criterion参数，根据参数说明可知，当criterion取值为absolute_error时将极大程度影响计算效率、计算时间将大幅提升，这里我们可以简单进行测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30a5d294-3c7e-4b0b-8b2b-8d9bd8f08e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.248455762863159\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "RandomForestRegressor().fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdb336d3-583a-47e4-987d-475f4822723b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308.26254439353943\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "RandomForestRegressor(criterion=\"absolute_error\").fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b58b4-2fe8-4b7d-9f43-36c24aa686c3",
   "metadata": {},
   "source": [
    "能够发现计算时间增幅达50倍！当然这不仅仅是回归随机森林单独模型的问题，很多模型在处理回归问题时都有类似的超参数设置情况，稍后建模的回归GBDT模型更是有过之而无不及。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4e6f8-efa4-4ab9-82b8-aa4938fb33c6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这其实也给我们启示，在算力限制情况下，我们其实根本无法带入全参数并在超大空间内进行搜索，必须要舍弃部分参数，进而兼顾计算效率和预测结果。根据长期实践经验，综合考虑当前算力情况，这里我们重新设计超参数搜索过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d2fa8ca-7533-492c-997e-c3738eb0e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto',\n",
       " 'sqrt',\n",
       " 'log2',\n",
       " None,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features_range = [\"auto\", \"sqrt\", \"log2\", None] + np.arange(0.1, 1., 0.1).tolist()\n",
    "max_features_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a989b721-4a09-4c41-b36b-97ddfa36148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {'max_features': hp.choice('max_features', max_features_range),\n",
    "            'n_estimators': hp.quniform('n_estimators', 200, 800, 1), \n",
    "            'max_samples': hp.uniform('max_samples', 0.2, 1),\n",
    "            'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 50, 1), \n",
    "            'min_samples_split': hp.quniform('min_samples_split', 2, 50, 1), \n",
    "            'max_depth': hp.quniform('max_depth', 20, 120, 1), \n",
    "            'max_leaf_nodes': hp.quniform('max_leaf_nodes', 200, 600, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17e258-40bc-4a7c-a34e-bf566e7b3e6e",
   "metadata": {},
   "source": [
    "这里我们加入加入了'max_features'特征分配超参数，并设置了该超参数的搜索范围max_features_range，该超参数的搜索范围较为特殊，需要在一组同时包含了字符串和浮点数的列表中进行搜索，因此采用了hp.choice的方式进行搜索空间创建。此外这里也增加了基础学习器个数的搜索空间。关于超参数搜索空设置，可以按照此前课程介绍的“小步前进、快速迭代”的方式，先设置较少迭代次数、反复试出一个大概的最佳取值范围，然后再设置一个大的搜索范围进行更高精度的搜索。这里展示的是经过多次尝试后最终决定的超参数搜索范围。       \n",
    "&emsp;&emsp;接下来继续定义目标函数和优化函数，这里由于增加了choice方式搜索，因此超参数的读取需要区分训练模式和测试模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8398d06d-5509-4cb7-bbe3-a3866d6a2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    min_samples_leaf = int(params['min_samples_leaf'])\n",
    "    min_samples_split = int(params['min_samples_split'])\n",
    "    max_depth = int(params['max_depth'])\n",
    "    max_leaf_nodes = int(params['max_leaf_nodes'])\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    max_samples = params['max_samples']\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "        \n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(min_samples_leaf = min_samples_leaf, \n",
    "                                   min_samples_split = min_samples_split,\n",
    "                                   max_depth = max_depth, \n",
    "                                   max_leaf_nodes = max_leaf_nodes, \n",
    "                                   n_estimators = n_estimators, \n",
    "                                   max_samples = max_samples,\n",
    "                                   max_features = max_features,\n",
    "                                   random_state=12)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_RF, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ba9b653-fb24-476a-89e7-7507e135292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    params_best = fmin(RF_param_objective,\n",
    "                       space = RF_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52a2e1a5-6db7-4397-ac5a-2016ed6a8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 1000/1000 [2:12:03<00:00,  7.92s/trial, best loss: 0.2578642167906655]\n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57149f4c-c259-4b29-ac58-4ee8e82e6212",
   "metadata": {},
   "source": [
    "然后测试超参数优化效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "861f137c-ecc8-4464-b4cd-70a094f6cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg_A = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c42a439-4c17-4e46-87da-03fd18bbab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1328016085201358, 0.2500023597601782)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg_A.predict(X_train), y_train), mean_squared_error(RF_reg_A.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482b3e7-f682-4972-aab9-441455205a09",
   "metadata": {},
   "source": [
    "我们发现最终结果有了明显提升，不仅测试集准确率有了较大提升，并且训练集和测试集的评分也保持一个相对接近的水平。当然，由于当前超参数搜索仍然不完整，因此训练集上仍然还有一定的过拟合倾向。但同样，由于我们可用通过交叉验证得分评估模型泛化能力，训练集上过拟合倾向并不会对搜索过程有效性造成任何影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ec1ee-e874-4ff6-9e74-9dc2f1cb3b98",
   "metadata": {},
   "source": [
    "| 训练方式 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| 默认超参数 | <center>0.0351 |<center>0.2599 | <center>0.2508 |\n",
    "| 小范围搜索 | <center>0.1943 |<center>0.2850 | <center>0.2760 |\n",
    "| 大范围搜索（A） | <center>0.1328 |<center>0.2578 | <center>0.2500 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f610477-4065-4533-bc6f-90051d8a0a2f",
   "metadata": {},
   "source": [
    "但是，这个搜索优化的过程所耗费的计算时间也达到了2个小时，需要知道的是，当前数据和特征情况还是相对简单的情况，真实企业级应用场景中（包括很多竞赛中）数据情况远比当前数据集更加复杂，考虑到可能还需要进行特征衍生、以及融合环节还需要多次训练模型，计算时间将是无法承担的。因此，必须要考虑一种更加高效的模型训练流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cef668-a3bc-4943-b74e-084519c95cb4",
   "metadata": {},
   "source": [
    "#### 1.3 更高效的搜索流程（方案B）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222b364-ba77-4afd-b358-39bd85255d3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们考虑另一种更高效的搜索流程：直接舍弃基础学习器的超参数搜索，只搜索集成相关超参数。这里需要注意的是，舍弃基础学习器的超参数搜索其实就等于带入基础学习器的默认超参数组，而基础学习器的默认超参数组是不会剪枝的、即默认执行最高模型复杂度的模型训练，每个基础学习器实际上是容易过拟合的，但是考虑到回归问题本身就要求更复杂的模型进行训练，且已被上述搜索过程验证，因此基础学习器选取默认超参数并不会对预测结果有较大的影响。并且除了基础学习器的超参数外、对模型泛化能力影响更大的继承超参数仍然是朝着泛化能力更强的方向进行搜索，外加舍弃了这部分超参数搜索，整体超参数搜索的速度将变得更快，这将有助于在更短的时间内快速搜索出一组更好的结果。        \n",
    "&emsp;&emsp;但是需要注意，如果接下来采用方案二进行超参数搜索，当我们舍弃了更多的基础学习器的超参数的时候，最终训练集的预测结果将会有更大的过拟合风险，因此在方案二的超参数搜索时，仍然需要坚持使用交叉验证结果作为模型泛化能力的评分标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4eb7fb-809f-4d66-995f-43910e2f5858",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们尝试方案二的搜索流程，即直接舍弃基础学习器的超参数搜索，转而进行更加全面深入的集成参数的搜索。具体搜索流程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68443257-57f2-4375-8ced-50ddfe4738a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {'max_features': hp.choice('max_features', max_features_range),\n",
    "            'n_estimators': hp.quniform('n_estimators', 20, 700, 1), \n",
    "            'max_samples': hp.uniform('max_samples', 0.2, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562f152-8421-4809-9c4f-dfe9471db600",
   "metadata": {},
   "source": [
    "这里我们删除了'max_depth'、'max_leaf_nodes'等基础学习器的超参数，加入了'max_features'特征分配超参数，并设置了该超参数的搜索范围max_features_range，该超参数的搜索范围较为特殊，需要在一组同时包含了字符串和浮点数的列表中进行搜索，因此采用了hp.choice的方式进行搜索空间创建。并且这里略微增加了基础学习器个数的搜索空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78447346-3d5a-4516-90ed-fa84ebc8c0c7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来定义目标函数和优化函数。这里由于增加了choice方式搜索，因此超参数的读取需要区分训练模式和测试模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9d96681-b4cf-45f1-84f8-aa6d69d48da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    max_samples = params['max_samples']\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "        \n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(n_estimators = n_estimators, \n",
    "                                   max_samples = max_samples, \n",
    "                                   max_features = max_features,\n",
    "                                   random_state=12)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_RF, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abab1f88-69fc-43d3-a6c9-d4246f3f4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    params_best = fmin(RF_param_objective,\n",
    "                       space = RF_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8ad18-67f3-4101-a86e-be1707c0a072",
   "metadata": {},
   "source": [
    "然后进行超参数搜索，这里我们先基础尝试50次搜索，得到结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b554e3d6-3e29-4ac2-9ac3-6da9b73f6986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [08:20<00:00, 10.00s/trial, best loss: 0.24145571958697482]\n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "803c9553-23ad-412a-bd51-ae79423d424f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 2, 'max_samples': 0.9990956320729892, 'n_estimators': 619.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7d5d7cf-ae4c-4731-9e3a-110e5e6688f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg_B1 = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36e149f4-ea22-4db5-a438-69301d432c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03217159873008787, 0.2305968021002135)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg_B1.predict(X_train), y_train), mean_squared_error(RF_reg_B1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a20da6-1deb-423c-99f5-55d13133a802",
   "metadata": {},
   "source": [
    "能够发现，在少量迭代时，该流程就已经能获得一个不错的预测结果，测试集结果明显提升，这也是目前得到的最好结果，并且10分钟的计算时长也是完全可以接受的。总的来说，高效搜索流程通过削减带入搜索的超参数个数，反而提升了最终模型效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de865ffe-c8d3-447b-9d12-d7dfbb1038f4",
   "metadata": {},
   "source": [
    "| 训练方式 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| 默认超参数 | <center>0.0351 |<center>0.2599 | <center>0.2508 |\n",
    "| 小范围搜索 | <center>0.1943 |<center>0.2850 | <center>0.2760 |\n",
    "| 大范围搜索（A） | <center>0.1328 |<center>0.2578 | <center>0.2500 |\n",
    "| 高效搜索（B） | <center>0.0321 |<center>0.2414 | <center>0.2305 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10fd0d4-0650-46d5-b460-a829b228b395",
   "metadata": {},
   "source": [
    "当然，此时由于诸多基础学习器的超参数未经过优化，因此在训练集上的过拟合风险加剧，但交叉验证的平均得分和测试集评分接近，说明交叉验证的平均得分仍然能够非常好的衡量模型泛化能力，说明模型搜索过程仍然有效。当然，通过和此前结果对比我们不难判断，训练集上的过拟合问题确实是由于超参数设置和搜索不完全导致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e908426-10d3-42f5-8636-846d46c448be",
   "metadata": {},
   "source": [
    "- 增加迭代次数尝试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a666e2b-05bb-4137-af59-9f7967efb8cc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并且，当我们判断超参数优化方向是正确的时候，甚至可以增加更多迭代次数测试最终效果。但对于当前相对简单的数据情况来说，增加迭代次数对最终效果提升影响不大，大家可以课后自行进行尝试，这里仅展示迭代500次数和1000次的计算时间和最终结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737c831-afa2-47ce-91c4-a64c4808c6fe",
   "metadata": {},
   "source": [
    "&emsp;&emsp;500次迭代计算时，耗时104分钟，测试集评分为0.2312"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbfa80-e4e8-4959-a8b3-9883079c2dd4",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221030215915746.png\" alt=\"image-20221030215915746\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731582a-463a-41fa-a632-cb67405d40e3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;1000次迭代计算时，耗时104分钟，测试集准确率为0.2307"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79a619-a741-4a95-8924-7751881fc15d",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221030215849538.png\" alt=\"image-20221030215849538\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bad3e2-d519-4ba7-8313-52b59bfac9aa",
   "metadata": {},
   "source": [
    "能够发现，迭代次数的增加并不能进一步提升模型效果，这也说明当前设置的50次计算是一种非常高效的获得较好结果的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0e893-c2f5-4436-a3b2-4f3588a25bf5",
   "metadata": {},
   "source": [
    "> 当然，TPE搜索的最佳迭代次数也和数据复杂度、模型复杂度、超参数空间设置有很大关系，并没有固定的最佳迭代次数。当然我们也可以通过设置提前停止参数来尽可能的寻定一个较为合适的迭代次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e01b4-3197-4859-9e3c-d1d842e726f0",
   "metadata": {},
   "source": [
    "- 高效搜索流程可用性探讨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848daac7-8640-47d6-9873-dc1c3aff943a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从唯结果论的角度来看，方案B无疑是最佳策略，仅用了10分钟就获得了一个比方案A的2个小时计算更好的结果，省时省力。但不管怎样，方案B训练得到的模型在训练集上仍然还是表现出了过拟合倾向，相信还是有很多小伙伴对方案B训练的模型可用性存疑。需要知道的是，尽管训练集上存在过拟合倾向，但我们对模型泛化能力的判别完全可以使用交叉验证结果，而非训练集上准确率，因此，无论是模型之间的横向比较、还是模型内部超参数搜索，训练集上准确率并不会造成任何影响。因此，该结果和整体流程仍然是可用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7d391-6dff-4335-ac0c-f33078667a0b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，我们这里也需要分析这四组不同的模型结果背后所代表的含义，帮助大家从实践角度理解过拟合和模型学习能力之间的辩证关系："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae975900-6901-4e41-870a-65e2d1949f18",
   "metadata": {},
   "source": [
    "| 模型编号 | 训练方式 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ | ------ |\n",
    "| <center>1| 默认超参数 | <center>0.0351 |<center>0.2599 | <center>0.2508 |\n",
    "| <center>2| 小范围搜索 | <center>0.1943 |<center>0.2850 | <center>0.2760 |\n",
    "| <center>2| 大范围搜索（A） | <center>0.1328 |<center>0.2578 | <center>0.2500 |\n",
    "| <center>4| 高效搜索（B） | <center>0.0321 |<center>0.2414 | <center>0.2305 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f5091-0a14-4421-8dc9-1cfb479d4dd7",
   "metadata": {},
   "source": [
    "首先需要介绍一个模型超参数优化过程中的基本矛盾，那就是手段和目的不匹配的问题。即模型优化的最终目的是为了让模型提升泛化能力，即尽可能学习全局规律而抛弃局部规律，但所采用的手段——超参数搜索其实是一种限制（或者增加）模型学习能力的策略，这二者并不完全匹配。无论是局部规律还是全局规律，都有难的规律和简单的规律。而超参数搜索无论是提高模型学习能力还是降低模型学习能力，都必然会同时学习到或者损失掉一些全局规律和局部规律。因此，并非限制模型学习能力，损失掉的、没学到的规律就一定是局部规律，就一定有助于模型解决过拟合问题；反之也一样，并不是提升模型学习能力，就一定能学到全局规律，有可能也会学到局部规律，从而降低模型在测试集上的评分。这点在实践过程中屡见不鲜。相信大家在实践过程中也遇到过不少类似的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f721941-5a99-47e2-9919-8f62c01f8db4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但超参数搜索为何还是有效的呢？因为超参数搜索能够在通过调配模型学习能力，最终让模型获得一个最佳的学习能力——能最大程度同时捕获适用于训练集和测试集的规律，因而提升模型的泛化能力，也就是提升模型对测试集的预测能力。但是，从理论上来说，要做到这点必须要对模型进行全超参数的高精度搜索才行，例如上述模型3基本就是这种情况。但是全参数大范围高精度搜索其实是非常费时费力的，稍有不慎就可能得不到这个最佳的模型，反而可能会得到一个学习能力不足、泛化能力不够的模型——例如上述模型2。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf3982-cee4-43c4-94a0-405434a7d5ad",
   "metadata": {},
   "source": [
    "&emsp;&emsp;那要如何改善这点呢？当然，与其说是改善，不如说是算力不够时“两权相害取其轻”的结果——相比通过限制搜索空间来提高搜索效率，有的时候适度的放开模型的学习能力，哪怕让模型学到了一些局部规律，只要不严重影响测试集预测结果、不影响模型整体泛化能力，其实也没有太大问题，例如模型4。对于4号模型来说，由于不对基础模型的超参数进行限制，因此其学习能力是非常强的，但由于我们对其集成超参数进行了搜索和优化，因此仍然一定程度上对其学习能力进行了调整，在有限的调整空间下，我们无法获得类似模型3中获得的严谨结果，但仍然保证了其能够有效学习全局规律，因此最终预测集上结果有所提升。但由于还有很多不受控的超参数在影响模型的学习能力，因此模型在训练集的训练过程中也学习了非常多局部规律，导致其但从训练集上来看过拟合风险偏高。但就模型泛化能力、也就是对新数据的预测能力来说，模型4和模型3并无本质区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a49734-2859-4586-a6cc-6feb3c5afd11",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，从理论上来说，模型4的结果和背后的模型训练流程，都是可用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18fd7a-0212-45cd-9e1d-078a267ab70e",
   "metadata": {},
   "source": [
    "- 超参数搜索中超参数选择依据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81cd0d-92f7-434f-8d12-a639e8b693a7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来的问题就是，既然模型3和模型4其实都是超参数不完全的搜索流程，并且二者表现出了不同的泛化能力，那要如何才能确定搜索哪几个超参数才能达到更好效果呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829857a-19db-4c92-85e4-3d3f789bac03",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们需要确定一点的是，从理论层面来说，超参数搜索最有效的方法一定是全参数大范围高精度搜索，如此得到的模型一定是能够最大程度学习全局规律的模型。但遗憾的是，在实际执行过程中，尤其是回归类问题，受限于当前算力条件，该方案几乎没有可执行性。因此需要考虑挑选出部分超参数进行搜索，而只要是挑选部分超参数出来进行搜索，其实就已经是不满足理论最优解的情况了，而具体要选哪些超参数出来搜索，也不存在理论最优解。因此，关于超参数的选取，其实也是需要多次尝试并且不断积累经验的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd26d07-68d0-4cb7-bb04-2d15600ffa64",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总的来说，根据长期实践经验总结，集成相关超参数肯定需要进行搜索的，例如随机森林中的'max_features'、'n_estimators'和'max_samples'等，此外对于基础学习器的超参数来说，可以优先尝试max_depth、max_leaf_nodes这两个参数，该参数对决策树模型剪枝影响巨大。当然，是否要加入max_depth、max_leaf_nodes，甚至是其他基础分类器超参数进行搜索，可以通过少量次数迭代测试效果再决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b489b-42ff-495e-bc3a-fadeb2557e11",
   "metadata": {},
   "source": [
    "&emsp;&emsp;例如，此处带入max_depth搜索结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d731584-682e-4916-ac80-bf8c116a3578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [09:36<00:00, 11.53s/trial, best loss: 0.24188062380676975]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 95.0,\n",
       " 'max_features': 7,\n",
       " 'max_samples': 0.9919926208654151,\n",
       " 'n_estimators': 661.0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_space = {'max_features': hp.choice('max_features', max_features_range),\n",
    "            'n_estimators': hp.quniform('n_estimators', 200, 800, 1), \n",
    "            'max_samples': hp.uniform('max_samples', 0.2, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 20, 120, 1)}\n",
    "\n",
    "def RF_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    max_depth = int(params['max_depth'])\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    max_samples = params['max_samples']\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "        \n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(max_depth = max_depth, \n",
    "                                   n_estimators = n_estimators, \n",
    "                                   max_samples = max_samples,\n",
    "                                   max_features = max_features,\n",
    "                                   random_state=12)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_RF, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def RF_param_search(max_evals=500):\n",
    "    params_best = fmin(RF_param_objective,\n",
    "                       space = RF_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best\n",
    "\n",
    "RF_param_search(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f412f1e-e6df-430c-99f5-f595c0867af6",
   "metadata": {},
   "source": [
    "比不带入情况交叉验证平均得分0.2414更差，因此这里不宜带入max_depth进行搜索。接下来继续考虑带入max_leaf_nodes的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adb88c69-444a-4fc2-9754-f380a79cc497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [05:52<00:00,  7.05s/trial, best loss: 0.26150200005559976]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 86.0,\n",
       " 'max_features': 7,\n",
       " 'max_leaf_nodes': 579.0,\n",
       " 'max_samples': 0.7548212077512358,\n",
       " 'n_estimators': 464.0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_space = {'max_features': hp.choice('max_features', max_features_range),\n",
    "            'n_estimators': hp.quniform('n_estimators', 200, 800, 1), \n",
    "            'max_samples': hp.uniform('max_samples', 0.2, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 20, 120, 1), \n",
    "            'max_leaf_nodes': hp.quniform('max_leaf_nodes', 200, 600, 1)}\n",
    "\n",
    "def RF_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    max_depth = int(params['max_depth'])\n",
    "    max_leaf_nodes = int(params['max_leaf_nodes'])\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    max_samples = params['max_samples']\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "        \n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(max_depth = max_depth, \n",
    "                                   max_leaf_nodes = max_leaf_nodes, \n",
    "                                   n_estimators = n_estimators, \n",
    "                                   max_samples = max_samples,\n",
    "                                   max_features = max_features,\n",
    "                                   random_state=12)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_RF, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def RF_param_search(max_evals=500):\n",
    "    params_best = fmin(RF_param_objective,\n",
    "                       space = RF_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best\n",
    "\n",
    "RF_param_search(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43711433-dfe1-43a4-9328-986595bdd60d",
   "metadata": {},
   "source": [
    "同样，带入更多超参数后，交叉验证得分反而降低，因此对于该数据集，按照模型4的训练方法即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366c451-3de6-420a-aea5-3a6ed13eb9d5",
   "metadata": {},
   "source": [
    "- 从实践经验对理论的补位"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fadeee-679b-43a2-8ca5-a8756b346c04",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其实不仅是超参数搜索，在机器学习的很多技术实践不是对理论完美复刻，而是基于实际情况，在理论的基础上、结合实际经验，寻找更优的实现方法。例如此前我们一直强调过拟合会威胁模型泛化能力，但这里发现适度放宽模型学习能力，容忍一定过拟合倾向，在借助交叉验证平均分为判别条件情况下，反而能提升模型的泛化能力，因此无比要很多时候活学活用，才能起到更好的效果。在实践过程中我们不是一定追求一个完美的流程，而是追求一个更高效更准确的最终结果。正所谓黑猫白猫、抓住老鼠就是好猫。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edca2c-c609-4dd2-bf81-1052aeaec2eb",
   "metadata": {},
   "source": [
    "- 高效搜索流程优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4cce3-b800-4793-b15b-fa2050bf4745",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，既然是诞生于实践过程的搜索策略，往往就会有长期实践过程中逐渐摸索出来的优化流程。对于这里我们看到的基于集成参数的高效搜索流程也不例外。整体来说，由于4号模型经过优化的超参数个数较少，因此模型在不同数据集上进行训练和预测时偏差较小但方差较大（例如max_depth取值为None时，就会根据不同数据集情况训练得到不同深度的基础树模型），目前来说，较为通用的围绕高效搜索流程算法特性设计的优化方法有以下两种，这两种方法的基本思路都源于交叉训练，希望通过类似交叉训练的过程来降低输出结果方差，提升结果稳定性，进而提高模型泛化能力："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fcab-84fc-4597-b9ec-0301372ab89b",
   "metadata": {},
   "source": [
    "方法一：在搜索得到一组最优超参数后，通过交叉训练的方式同时输出多组测试集的预测结果，然后求均值作为最终的预测结果；      \n",
    "方法二：在超参数搜索过程中，不再使用验证集的平均得分作为搜索依据，换成验证集拼接而成的预测数据和标签之间的MSE作为搜索依据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b5d2d-5881-4e6f-b28a-366b4c87c933",
   "metadata": {},
   "source": [
    "&emsp;&emsp;很明显，这两种方法都有非常浓厚的“模型融合”的意味在里面。其实很多方法是可以同时适用于很多场景中的，只要能解决当前问题，其实就是值得尝试的方法。其实尽管课程顺序是先介绍了模型融合中的oof数据集概念和交叉训练输出平均结果等流程，但实际上这些方法大多诞生于回归问题的单模优化过程中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61c6fd-f2a0-4567-9b88-9a0d20895c65",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过需要注意的是，这些方法并不如此前介绍的超参数搜索策略效果那么明显，只能作为备选的一种策略，或许能起到优化结果的效果。建议在实际建模过程多加尝试，多训练几组结果然后择优输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fdcbc-5f11-47b1-8770-5854653eb79d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先测试方案一：这里我们选择五折交叉训练并输出最终结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8693d85-36d8-4d05-8bc8-d6f344e11cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features='log2', max_samples=0.9990956320729892,\n",
       "                      n_estimators=619, random_state=12)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_reg_B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "735ca3ad-fac6-4aed-bbb6-5f5dee19f523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features='log2', max_samples=0.9990956320729892,\n",
       "                      n_estimators=619, n_jobs=15, random_state=12)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_reg_B1.set_params(n_jobs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8afe6d7b-859d-4daf-963f-3e200154c2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RF_test_predict = 0\n",
    "RF_train_predict = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "    # 在训练集上训练\n",
    "    X_train_part = X_train.loc[train_part_index]\n",
    "    y_train_part = y_train[train_part_index]\n",
    "    RF_reg_B1.fit(X_train_part, y_train_part)\n",
    "    RF_train_predict += RF_reg_B1.predict(X_train) / 5\n",
    "    RF_test_predict += RF_reg_B1.predict(X_test) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5a5f2823-40bd-4970-bdd1-6f0e7639e90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05882594631689255"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_train_predict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "061b21c9-2b3f-42f1-95a0-5e44bd6e14a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2351970974351355"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5eaa30ee-2eb9-4988-8ef0-28eca438b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03217159873008787, 0.23059680210021352)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_reg_B1.fit(X_train, y_train)\n",
    "mean_squared_error(RF_reg_B1.predict(X_train), y_train), mean_squared_error(RF_reg_B1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29af0e1-9358-4ab6-b316-88c8fcc26994",
   "metadata": {},
   "source": [
    "然后测试方案二，方案二的测试过程较为复杂，需要修改原始搜索流程中的目标函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14841ab5-f86b-415c-b40b-947606ba12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {'max_features': hp.choice('max_features', max_features_range),\n",
    "            'n_estimators': hp.quniform('n_estimators', 20, 700, 1), \n",
    "            'max_samples': hp.uniform('max_samples', 0.2, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "466a4811-40c2-4c5b-8ae5-ddb68e26631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    max_samples = params['max_samples']\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "        \n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(n_estimators = n_estimators, \n",
    "                                   max_samples = max_samples, \n",
    "                                   max_features = max_features,\n",
    "                                   random_state=12, \n",
    "                                   n_jobs=15)\n",
    "\n",
    "    oof_series = pd.Series(np.empty(X_train.shape[0]))\n",
    "    \n",
    "    if train == True:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "        for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X_train.loc[train_part_index]\n",
    "            y_train_part = y_train[train_part_index]\n",
    "            reg_RF.fit(X_train_part, y_train_part)\n",
    "            X_eval_part = X_train.loc[eval_index]\n",
    "            # 将验证集上预测结果拼接入oof数据集\n",
    "            oof_series.loc[eval_index] = reg_RF.predict(X_eval_part)\n",
    "    \n",
    "        res = mean_squared_error(oof_series, y_train)\n",
    "    \n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5ff602fc-7204-4bfa-9574-c7698c1f87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    params_best = fmin(RF_param_objective,\n",
    "                       space = RF_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ba2d9-79c4-4e05-bb12-8a9543777b98",
   "metadata": {},
   "source": [
    "然后进行超参数搜索，仍然是进行50次搜索，得到结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cd780d03-9c16-4871-a666-7b0b4a139f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 50/50 [03:21<00:00,  4.04s/trial, best loss: 0.2449185928067511]\n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e0d18ba-88d3-48e1-a9d3-896289a0d531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 1, 'max_samples': 0.9403838253830878, 'n_estimators': 328.0}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ce3fb0b6-1514-4bc2-b1fb-31c65e14fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg_B2 = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f9171ace-badf-43d3-86ed-d087391e01a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.037033392939370864, 0.23450777599546047)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg_B2.predict(X_train), y_train), mean_squared_error(RF_reg_B2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cd229-f22e-4507-8f92-a659ea6debb6",
   "metadata": {},
   "source": [
    "能够发现，这两种方法在当前模型中未能发挥优化效果。最终随机森林最优预测结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f734020-abcf-4669-8540-9049e3faa3d4",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| <center>随机森林 | <center>0.0351 |<center> 0.2599 | <center>0.2508 |\n",
    "| <center>随机森林_OPT | <center>0.0321 |<center>0.2414 | <center>0.2305 |\n",
    "| <center>极端随机树 | <center>0 |<center> 0.2581 | <center>0.2436 |\n",
    "| <center>GBDT | <center>0.2605 |<center> 0.2870 | <center>0.2777 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a6e61-bead-4848-87ba-6c7bd23eb56d",
   "metadata": {},
   "source": [
    "### 2.回归极端随机树超参数优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3316499-8b9c-4ac5-bcaa-17a7de9bc16c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来是极端随机树的回归模型的超参数优化过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "16663d07-b833-4eab-87fe-60f6dd27721f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mExtraTreesRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'squared_error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "An extra-trees regressor.\n",
       "\n",
       "This class implements a meta estimator that fits a number of\n",
       "randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
       "of the dataset and uses averaging to improve the predictive accuracy\n",
       "and control over-fitting.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : int, default=100\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``n_estimators`` changed from 10 to 100\n",
       "       in 0.22.\n",
       "\n",
       "criterion : {\"squared_error\", \"absolute_error\"}, default=\"squared_error\"\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"squared_error\" for the mean squared error, which is equal to\n",
       "    variance reduction as feature selection criterion, and \"absolute_error\"\n",
       "    for the mean absolute error.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Mean Absolute Error (MAE) criterion.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
       "        version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
       "        version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `round(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "bootstrap : bool, default=False\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole dataset is used to build each tree.\n",
       "\n",
       "oob_score : bool, default=False\n",
       "    Whether to use out-of-bag samples to estimate the generalization score.\n",
       "    Only available if bootstrap=True.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
       "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
       "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors. See :term:`Glossary\n",
       "    <n_jobs>` for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls 3 sources of randomness:\n",
       "\n",
       "    - the bootstrapping of the samples used when building trees\n",
       "      (if ``bootstrap=True``)\n",
       "    - the sampling of the features to consider when looking for the best\n",
       "      split at each node (if ``max_features < n_features``)\n",
       "    - the draw of the splits for each of the `max_features`\n",
       "\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "max_samples : int or float, default=None\n",
       "    If bootstrap is True, the number of samples to draw from X\n",
       "    to train each base estimator.\n",
       "\n",
       "    - If None (default), then draw `X.shape[0]` samples.\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
       "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "base_estimator_ : ExtraTreeRegressor\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "estimators_ : list of DecisionTreeRegressor\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
       "    Prediction computed with out-of-bag estimate on the training set.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ExtraTreesClassifier : An extra-trees classifier with random splits.\n",
       "RandomForestClassifier : A random forest classifier with optimal splits.\n",
       "RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
       "       Machine Learning, 63(1), 3-42, 2006.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_diabetes\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> from sklearn.ensemble import ExtraTreesRegressor\n",
       ">>> X, y = load_diabetes(return_X_y=True)\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, random_state=0)\n",
       ">>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n",
       "...    X_train, y_train)\n",
       ">>> reg.score(X_test, y_test)\n",
       "0.2708...\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ExtraTreesRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02639b4a-2317-40f9-8b52-37ea44491701",
   "metadata": {},
   "source": [
    "极端随机树模型参数和随机森林参数完全一致，只有criterion参数取值不同，但该参数并不是超参数优化的对象，因此极端随机树和随机森林的超参数优化流程高度类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6848c-7346-43df-ae80-a761c93f3a92",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是超参数空间的设置过程，对于极端随机树来说，由于其模型训练的特殊性（随机选取子节点进行criterion计算），往往树的复杂度要高于随机森林中的树，因此可以考虑稍微放大各超参数搜索空间。当然对应的需要增加迭代次数。不过到极端随机树单模型训练时间会更短，可以对冲一部分由于超参数搜索空间所带来的搜索时间的增加。此外需要注意的是，在默认情况下ExtraTreesRegressor是不包含bootstrap过程的，并且由于极端随机树的建模流程本身随机性就比较强，再进行bootstrap可能会对模型结果的稳定性造成影响，因此超参数空间中需要删除max_samples参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "24c17231-23ea-4f68-8c06-8b9a506705cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_space = {'max_depth': hp.quniform('max_depth', 2, 50, 1), \n",
    "            'n_estimators': hp.quniform('n_estimators', 20, 700, 1), \n",
    "            'max_features': hp.choice('max_features', max_features_range)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370afb73-b34c-4282-8ed4-c599564f10c7",
   "metadata": {},
   "source": [
    "然后定义目标函数和优化函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "038a7526-c819-4a30-9280-479b4eac32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    max_depth = int(params['max_depth'])\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    \n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "    \n",
    "    # 模型创建\n",
    "    reg_ET = ExtraTreesRegressor(max_depth = max_depth, \n",
    "                                 n_estimators = n_estimators, \n",
    "                                 max_features = max_features, \n",
    "                                 random_state=12)\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_ET, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_ET.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f3b92a4c-537a-4457-8fc2-cc12b9646c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_search(max_evals=500):\n",
    "    params_best = fmin(ET_param_objective,\n",
    "                       space = ET_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6dd99-44f5-4783-8a7c-728086900523",
   "metadata": {},
   "source": [
    "然后进行超参数搜索优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1f4405a3-c9d9-4ab0-8de8-cade38268567",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 50/50 [04:27<00:00,  5.35s/trial, best loss: 0.2346537351978845]\n"
     ]
    }
   ],
   "source": [
    "ET_best_param = ET_param_search(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe708eb7-2372-43cd-a258-acd3b75e6ee8",
   "metadata": {},
   "source": [
    "查看最优超参数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3d40a566-0d55-43f4-a0d2-0b5085d31f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 39.0, 'max_features': 9, 'n_estimators': 516.0}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83871b1-2284-44e0-94b7-2d7d7f0b452b",
   "metadata": {},
   "source": [
    "然后输出最佳超参数组在训练集上训练得到的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e0c88244-a906-457d-9097-913d870ad80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_reg = ET_param_objective(ET_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "250e1f4c-985f-4a63-8047-05eb1f0d680b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(max_depth=39, max_features=0.6, n_estimators=516,\n",
       "                    random_state=12)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b8e2c-00f0-45a6-8c2c-874d5eb648ad",
   "metadata": {},
   "source": [
    "测试模型在训练集和测试集上表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "377e1603-05c9-4b0a-a6f0-c7c0b52a21c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3687030793344334e-07, 0.22259777187847052)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_reg.predict(X_train), y_train), mean_squared_error(ET_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3103a-105d-4656-b708-e41ba987ba8c",
   "metadata": {},
   "source": [
    "同样，通过超参数优化，极端随机树也获得了一组好的预测结果，整体模型性能有所提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c7915-ec3a-47d8-ad87-d560ddd2d9a0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来根据训练集平均得分测试两种优化策略是否有效："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a10826-a6c6-49a9-9110-35a5b9512482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(max_depth=39, max_features=0.6, n_estimators=516, n_jobs=15,\n",
       "                    random_state=12)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_reg.set_params(n_jobs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246acb0-5727-443d-b47c-40fd6bcfe9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ET_test_predict = 0\n",
    "ET_train_predict = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "    # 在训练集上训练\n",
    "    X_train_part = X_train.loc[train_part_index]\n",
    "    y_train_part = y_train[train_part_index]\n",
    "    ET_reg.fit(X_train_part, y_train_part)\n",
    "    ET_train_predict += ET_reg.predict(X_train) / 5\n",
    "    ET_test_predict += ET_reg.predict(X_test) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105f4b6-5ce9-4f2c-be5a-3d2d4c4eee1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.009386272529438165, 0.22576287904950001)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_train_predict, y_train), mean_squared_error(ET_test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f9c2a-3621-431c-b47d-42c2e8f62e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_objective(params, train=True):\n",
    "    \n",
    "    # 超参数读取\n",
    "    max_depth = int(params['max_depth'])\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    \n",
    "    if train == True:\n",
    "        max_features = params['max_features']\n",
    "        \n",
    "    else:\n",
    "        max_features = max_features_range[params['max_features']]\n",
    "    \n",
    "    # 模型创建\n",
    "    reg_ET = ExtraTreesRegressor(max_depth = max_depth, \n",
    "                                 n_estimators = n_estimators, \n",
    "                                 max_features = max_features, \n",
    "                                 random_state=12, \n",
    "                                 n_jobs=15)\n",
    "\n",
    "    oof_series = pd.Series(np.empty(X_train.shape[0]))\n",
    "    \n",
    "    if train == True:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "        for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X_train.loc[train_part_index]\n",
    "            y_train_part = y_train[train_part_index]\n",
    "            reg_ET.fit(X_train_part, y_train_part)\n",
    "            X_eval_part = X_train.loc[eval_index]\n",
    "            # 将验证集上预测结果拼接入oof数据集\n",
    "            oof_series.loc[eval_index] = reg_ET.predict(X_eval_part)\n",
    "    \n",
    "        res = mean_squared_error(oof_series, y_train)\n",
    "    else:\n",
    "        res = reg_ET.fit(X_train, y_train)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a01fd4-7390-4e64-a1ba-ac9efa39785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [03:12<00:00,  3.85s/trial, best loss: 0.23321127263835334]\n"
     ]
    }
   ],
   "source": [
    "ET_best_param = ET_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdddb8a-ecff-43c1-bf73-2513a9b7b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_reg2 = ET_param_objective(ET_best_param2, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268571a-a27c-4f2a-b682-670b64b4fa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.803886411200801e-05, 0.22332734286955555)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_reg2.predict(X_train), y_train), mean_squared_error(ET_reg2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bb56a-cfff-465f-9150-0b6f2e42c03a",
   "metadata": {},
   "source": [
    "最终最优结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6923a-1f58-46db-ba0f-7dbf2f11b9be",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| <center>随机森林 | <center>0.0351 |<center> 0.2599 | <center>0.2508 |\n",
    "| <center>随机森林_OPT | <center>0.0321 |<center>0.2414 | <center>0.2305 |\n",
    "| <center>极端随机树 | <center>0 |<center> 0.2581 | <center>0.2436 |\n",
    "| <center>极端随机树_OPT | <center>1.36e-07 |<center> 0.2346 | <center>0.2257 |\n",
    "| <center>GBDT | <center>0.2605 |<center> 0.2870 | <center>0.2777 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13808f11-02f9-4398-ba55-414832dfe7ba",
   "metadata": {},
   "source": [
    "### 3.回归GBDT超参数优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0f7f6-aed3-4dc6-b1f9-78eb9f02e6f7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后是GBDT的回归模型的超参数优化过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "938d3948-ed0a-4c11-8797-fa403323788e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'squared_error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'friedman_mse'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Gradient Boosting for regression.\n",
       "\n",
       "GB builds an additive model in a forward stage-wise fashion;\n",
       "it allows for the optimization of arbitrary differentiable loss functions.\n",
       "In each stage a regression tree is fit on the negative gradient of the\n",
       "given loss function.\n",
       "\n",
       "Read more in the :ref:`User Guide <gradient_boosting>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'\n",
       "    Loss function to be optimized. 'squared_error' refers to the squared\n",
       "    error for regression. 'absolute_error' refers to the absolute error of\n",
       "    regression and is a robust loss function. 'huber' is a\n",
       "    combination of the two. 'quantile' allows quantile regression (use\n",
       "    `alpha` to specify the quantile).\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        The loss 'ls' was deprecated in v1.0 and will be removed in\n",
       "        version 1.2. Use `loss='squared_error'` which is equivalent.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        The loss 'lad' was deprecated in v1.0 and will be removed in\n",
       "        version 1.2. Use `loss='absolute_error'` which is equivalent.\n",
       "\n",
       "learning_rate : float, default=0.1\n",
       "    Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
       "    There is a trade-off between learning_rate and n_estimators.\n",
       "\n",
       "n_estimators : int, default=100\n",
       "    The number of boosting stages to perform. Gradient boosting\n",
       "    is fairly robust to over-fitting so a large number usually\n",
       "    results in better performance.\n",
       "\n",
       "subsample : float, default=1.0\n",
       "    The fraction of samples to be used for fitting the individual base\n",
       "    learners. If smaller than 1.0 this results in Stochastic Gradient\n",
       "    Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
       "    Choosing `subsample < 1.0` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "\n",
       "criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'},             default='friedman_mse'\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"friedman_mse\" for the mean squared error with improvement\n",
       "    score by Friedman, \"squared_error\" for mean squared error, and \"mae\"\n",
       "    for the mean absolute error. The default value of \"friedman_mse\" is\n",
       "    generally the best as it can provide a better approximation in some\n",
       "    cases.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "    .. deprecated:: 0.24\n",
       "        `criterion='mae'` is deprecated and will be removed in version\n",
       "        1.1 (renaming of 0.26). The correct way of minimizing the absolute\n",
       "        error is to use `loss='absolute_error'` instead.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Criterion 'mse' was deprecated in v1.0 and will be removed in\n",
       "        version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_depth : int, default=3\n",
       "    Maximum depth of the individual regression estimators. The maximum\n",
       "    depth limits the number of nodes in the tree. Tune this parameter\n",
       "    for best performance; the best value depends on the interaction\n",
       "    of the input variables.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "init : estimator or 'zero', default=None\n",
       "    An estimator object that is used to compute the initial predictions.\n",
       "    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
       "    initial raw predictions are set to zero. By default a\n",
       "    ``DummyEstimator`` is used, predicting either the average target value\n",
       "    (for loss='squared_error'), or a quantile for the other losses.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the random seed given to each Tree estimator at each\n",
       "    boosting iteration.\n",
       "    In addition, it controls the random permutation of the features at\n",
       "    each split (see Notes for more details).\n",
       "    It also controls the random splitting of the training data to obtain a\n",
       "    validation set if `n_iter_no_change` is not None.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `int(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Choosing `max_features < n_features` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "alpha : float, default=0.9\n",
       "    The alpha-quantile of the huber loss function and the quantile\n",
       "    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Enable verbose output. If 1 then it prints progress and performance\n",
       "    once in a while (the more trees the lower the frequency). If greater\n",
       "    than 1 then it prints progress and performance for every tree.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Must be between 0 and 1.\n",
       "    Only used if ``n_iter_no_change`` is set to an integer.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "n_iter_no_change : int, default=None\n",
       "    ``n_iter_no_change`` is used to decide if early stopping will be used\n",
       "    to terminate training when validation score is not improving. By\n",
       "    default it is set to None to disable early stopping. If set to a\n",
       "    number, it will set aside ``validation_fraction`` size of the training\n",
       "    data as validation and terminate training when validation score is not\n",
       "    improving in all of the previous ``n_iter_no_change`` numbers of\n",
       "    iterations.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the early stopping. When the loss is not improving\n",
       "    by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
       "    number), the training stops.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_improvement_ : ndarray of shape (n_estimators,)\n",
       "    The improvement in loss (= deviance) on the out-of-bag samples\n",
       "    relative to the previous iteration.\n",
       "    ``oob_improvement_[0]`` is the improvement in\n",
       "    loss of the first stage over the ``init`` estimator.\n",
       "    Only available if ``subsample < 1.0``\n",
       "\n",
       "train_score_ : ndarray of shape (n_estimators,)\n",
       "    The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
       "    model at iteration ``i`` on the in-bag sample.\n",
       "    If ``subsample == 1`` this is the deviance on the training data.\n",
       "\n",
       "loss_ : LossFunction\n",
       "    The concrete ``LossFunction`` object.\n",
       "\n",
       "init_ : estimator\n",
       "    The estimator that provides the initial predictions.\n",
       "    Set via the ``init`` argument or ``loss.init_estimator``.\n",
       "\n",
       "estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "n_classes_ : int\n",
       "    The number of classes, set to 1 for regressors.\n",
       "\n",
       "    .. deprecated:: 0.24\n",
       "        Attribute ``n_classes_`` was deprecated in version 0.24 and\n",
       "        will be removed in 1.1 (renaming of 0.26).\n",
       "\n",
       "n_estimators_ : int\n",
       "    The number of estimators as selected by early stopping (if\n",
       "    ``n_iter_no_change`` is specified). Otherwise it is set to\n",
       "    ``n_estimators``.\n",
       "\n",
       "n_features_ : int\n",
       "    The number of data features.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n",
       "    Classification Tree.\n",
       "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
       "sklearn.ensemble.RandomForestRegressor : A random forest regressor.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data and\n",
       "``max_features=n_features``, if the improvement of the criterion is\n",
       "identical for several splits enumerated during the search of the best\n",
       "split. To obtain a deterministic behaviour during fitting,\n",
       "``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       "J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
       "Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
       "\n",
       "J. Friedman, Stochastic Gradient Boosting, 1999\n",
       "\n",
       "T. Hastie, R. Tibshirani and J. Friedman.\n",
       "Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import make_regression\n",
       ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = make_regression(random_state=0)\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, random_state=0)\n",
       ">>> reg = GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.fit(X_train, y_train)\n",
       "GradientBoostingRegressor(random_state=0)\n",
       ">>> reg.predict(X_test[1:2])\n",
       "array([-61...])\n",
       ">>> reg.score(X_test, y_test)\n",
       "0.4...\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\vdmion\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GradientBoostingRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a44214-2065-4777-b3f2-88bc18373b6b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于GBDT来说（包括其他Boosting算法也类似），最重要是的需要考虑计算时长和优化效果如何权衡的问题，很多时候GBDT的训练或超参数搜索时长并不和模型效果呈正比，并且由于无法设置n_jobs，部分参数的微调会极大程度影响运算速度，为了尽量避免无效或者低效计算，这里有几点模型训练时的参数设置建议："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78826f-573d-40a9-b34d-80a4799b1460",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先基础模型的复杂度会较大程度影响模型训练速度，并且伴随n_estimators增加呈叠加效应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e4c9ac4-2ecd-4465-aeb7-0438e418154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.353135585784912\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "GradientBoostingRegressor().fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f4ecab5-5c67-496c-b796-52d4d2dcd716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.385526657104492\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "GradientBoostingRegressor(max_depth=20).fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e15067b-4b65-4ee1-862e-7811a3ba0fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.930529832839966\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "GradientBoostingRegressor(n_estimators=200, max_depth=20).fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802f332-e36e-4c23-baf2-864cd63c078c",
   "metadata": {},
   "source": [
    "因此需要合理设置控制基础模型复杂度的相关参数。此外学习率也会较大程度影响计算时间，学习率越大、单模训练时长越小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3dad81b-9d84-45cc-b7f3-78611566bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.78476572036743\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "GradientBoostingRegressor(n_estimators=200, max_depth=20, learning_rate=0.05).fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a577d2ef-8272-41a4-8983-c5f68199edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4038450717926025\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "GradientBoostingRegressor(n_estimators=200, max_depth=20, learning_rate=0.5).fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526b1e4-6559-4cc1-996a-57da861485d6",
   "metadata": {},
   "source": [
    "而最关键的是，一般来说不建议调整或搜索loss和criterion两个参数。这两个参数在默认参数设置下就能起到较好效果，而若更换两个参数的取值，尤其是涉及到绝对值计算（也就是loss选择'absolute_error'或criterion选择'mae'）时，在模型较为复杂时，计算时长将呈指数级上升："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d8a8d730-3594-474f-b8de-8bc37f377fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5370254516601562\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "GradientBoostingRegressor(loss='absolute_error').fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aba598-dfa1-4e84-bbab-e3ee4afaa0a9",
   "metadata": {},
   "source": [
    "loss选取'absolute_error'在简单模型的情况下计算时间的延长并不明显："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d53c96d-cba4-47f7-88a9-6cd34a35d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.54344344139099\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "GradientBoostingRegressor(n_estimators=200, max_depth=20, loss='absolute_error').fit(X_train, y_train)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce69adc-c547-42bf-96ee-6c2e71df5194",
   "metadata": {},
   "source": [
    "但当模型复杂度提升后，loss选取'absolute_error'时的计算时间是默认参数'squared_error'的3-4倍，而若是在模型更加复杂的时候、或者多次建模进行超参数搜索时，loss选取'absolute_error'带来的计算时长将呈指数级上升。类似的情况还发生在criterion选取'mae'时。因此，在大多数情况下，loss和criterion可以考虑保留默认参数进行后续的超参数搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2a7d0bd-5234-4a7d-baff-629c21a360d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3209"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a11e6-046e-4a73-86f8-9c706fdc5faf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行GBDT超参数搜索优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f7b763-96d1-46b2-89f1-4887956f1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_space = {'n_estimators': hp.quniform('n_estimators', 20, 701, 1),\n",
    "             'learning_rate': hp.uniform('learning_rate', 0.02, 0.2),\n",
    "             'subsample': hp.uniform('subsample', 0.1, 1.0),\n",
    "             'max_depth': hp.quniform('max_depth', 2, 20, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce473bda-bf1f-4691-baab-a94888855051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_objective(params, train=True):\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    learning_rate = params['learning_rate']\n",
    "    subsample = params['subsample']\n",
    "    max_depth = int(params['max_depth'])\n",
    "    \n",
    "    reg_GBR = GradientBoostingRegressor(n_estimators = n_estimators, \n",
    "                                        learning_rate = learning_rate, \n",
    "                                        subsample = subsample, \n",
    "                                        max_depth = max_depth, \n",
    "                                        random_state=12)\n",
    "    if train == True:\n",
    "        res = -cross_val_score(reg_GBR, X_train, y_train, scoring='neg_mean_squared_error', n_jobs=15).mean()\n",
    "    else:\n",
    "        res = reg_GBR.fit(X_train, y_train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92225fa6-cb1b-4485-8150-fa257a51d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_search(max_evals=500):\n",
    "    params_best = fmin(GBR_param_objective,\n",
    "                       space = GBR_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals)\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b9f2ab18-6dc2-4f33-943b-32fa4d775ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [09:45<00:00, 11.71s/trial, best loss: 0.20911175863049003]\n"
     ]
    }
   ],
   "source": [
    "GBR_best_param = GBR_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "41af152e-89ab-4840-bd14-12728b30304d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.07948277691156985,\n",
       " 'max_depth': 7.0,\n",
       " 'n_estimators': 499.0,\n",
       " 'subsample': 0.9053472864274166}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a38e9-d40e-48bf-b046-61d7191e2253",
   "metadata": {},
   "source": [
    "然后输出最优超参数模型，并测试模型在训练集和测试集上的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "87e91cbd-aeea-47e3-a313-8e73f74658d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_reg = GBR_param_objective(GBR_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dbc8f1ca-8ec6-4932-beb1-aff4db1e63c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.07948277691156985, max_depth=7,\n",
       "                          n_estimators=499, random_state=12,\n",
       "                          subsample=0.9053472864274166)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "10d9ee54-976a-42cb-8b6f-6df0d5b000c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02536793820105726, 0.19804404020292288)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_reg.predict(X_train), y_train), mean_squared_error(GBR_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe026fde-1a2d-4b48-b976-918252c31154",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来根据训练集平均得分测试两种优化策略是否有效："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "efdbee34-4b33-4573-8b27-c2d3e224af54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GBR_test_predict = 0\n",
    "GBR_train_predict = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "    # 在训练集上训练\n",
    "    X_train_part = X_train.loc[train_part_index]\n",
    "    y_train_part = y_train[train_part_index]\n",
    "    GBR_reg.fit(X_train_part, y_train_part)\n",
    "    GBR_train_predict += GBR_reg.predict(X_train) / 5\n",
    "    GBR_test_predict += GBR_reg.predict(X_test) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cbbc4378-b952-4b21-a8b6-7b1241dafc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03541031209862538, 0.1916646210811945)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_train_predict, y_train), mean_squared_error(GBR_test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f8743d92-4c99-4db2-8b61-04fe8f6a0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_objective(params, train=True):\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    learning_rate = params['learning_rate']\n",
    "    subsample = params['subsample']\n",
    "    max_depth = int(params['max_depth'])\n",
    "    \n",
    "    reg_GBR = GradientBoostingRegressor(n_estimators = n_estimators, \n",
    "                                        learning_rate = learning_rate, \n",
    "                                        subsample = subsample, \n",
    "                                        max_depth = max_depth, \n",
    "                                        random_state = 12)\n",
    "    \n",
    "    oof_series = pd.Series(np.empty(X_train.shape[0]))\n",
    "    \n",
    "    if train == True:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "        for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X_train.loc[train_part_index]\n",
    "            y_train_part = y_train[train_part_index]\n",
    "            reg_GBR.fit(X_train_part, y_train_part)\n",
    "            X_eval_part = X_train.loc[eval_index]\n",
    "            # 将验证集上预测结果拼接入oof数据集\n",
    "            oof_series.loc[eval_index] = reg_GBR.predict(X_eval_part)\n",
    "    \n",
    "        res = mean_squared_error(oof_series, y_train)\n",
    "    else:\n",
    "        res = reg_GBR.fit(X_train, y_train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "22c0ab7e-deb9-4ffb-a476-f03ddbf0b4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [42:54<00:00, 51.49s/trial, best loss: 0.21289865067490696]\n"
     ]
    }
   ],
   "source": [
    "GBR_best_param = GBR_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ea326579-f5ca-4921-a769-605dfbf51eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.09171075672824927,\n",
       " 'max_depth': 5.0,\n",
       " 'n_estimators': 475.0,\n",
       " 'subsample': 0.7500351292461755}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "787ace60-fe29-4fb1-9f32-88b27ab51000",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_reg2 = GBR_param_objective(GBR_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a28deec0-9ceb-41c7-812c-af58d1455d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.09171075672824927, max_depth=5,\n",
       "                          n_estimators=475, random_state=12,\n",
       "                          subsample=0.7500351292461755)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_reg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2d748cc3-81f2-4e06-a7c0-16d6e011d237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07978385844498767, 0.20153019448532147)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_reg2.predict(X_train), y_train), mean_squared_error(GBR_reg2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000dc061-7572-4ffc-840d-dd20a123fc71",
   "metadata": {},
   "source": [
    "最终模型最优结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ea93a-8913-4826-a3d1-5d30b8d363b8",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| <center>随机森林 | <center>0.0351 |<center> 0.2599 | <center>0.2508 |\n",
    "| <center>随机森林_OPT | <center>0.0321 |<center>0.2414 | <center>0.2305 |\n",
    "| <center>极端随机树 | <center>0 |<center> 0.2581 | <center>0.2436 |\n",
    "| <center>极端随机树_OPT | <center>1.36e-07 |<center> 0.2346 | <center>0.2257 |\n",
    "| <center>GBDT | <center>0.2605 |<center> 0.2870 | <center>0.2777 |\n",
    "| <center>GBDT_OPT | <center>0.03545 |<center> 0.2091 | <center>0.1916 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e825fb-4282-4fd6-b89c-790e1d123401",
   "metadata": {},
   "source": [
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba22ee-cc93-4f23-9914-d2945d45e284",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就准备好了接下来进行模型融合的基础模型，这里我们将这些模型进行本地保存，方便后续调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6b8eb24c-84e4-45a5-9d80-665e00e31870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/GBR_reg.joblib']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(RF_reg, './models/RF_reg_B1.joblib')\n",
    "dump(ET_reg, './models/ET_reg.joblib')\n",
    "dump(GBR_reg, './models/GBR_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7a7cb89e-2ec2-4791-b0f7-8723c9591c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用时可使用如下语句\n",
    "#RF_reg = load('./models/RF_reg.joblib')\n",
    "#ET_reg = load('./models/ET_reg.joblib')\n",
    "#GBR_reg = load('./models/GBR_reg.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
