{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13425201-0793-4a25-94a6-28ffe968867c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> 【Kaggle】Telco Customer Churn 电信用户流失预测案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf37b4-3ea8-4f02-85c5-c6c0292a5e05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58fa73-581c-42e1-8e8c-e365c7e3aef1",
   "metadata": {},
   "source": [
    "## <font face=\"仿宋\">第四部分导读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209080b9-5c3b-4d32-b643-5e65bfc156d1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">在案例的第二、三部分中，我们详细介绍了关于特征工程的各项技术，特征工程技术按照大类来分可以分为数据预处理、特征衍生、特征筛选三部分，其中特征预处理的目的是为了将数据集整理、清洗到可以建模的程度，具体技术包括缺失值处理、异常值处理、数据重编码等，是建模之前必须对数据进行的处理和操作；而特征衍生和特征筛选则更像是一类优化手段，能够帮助模型突破当前数据集建模的效果上界。并且我们在第二部分完整详细的介绍机器学习可解释性模型的训练、优化和解释方法，也就是逻辑回归和决策树模型。并且此前我们也一直以这两种算法为主，来进行各个部分的模型测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824c2f2-3d38-4d04-bcf5-4a15414da179",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而第四部分，我们将开始介绍集成学习的训练和优化的实战技巧，尽管从可解释性角度来说，集成学习的可解释性并不如逻辑回归和决策树，但在大多数建模场景下，集成学习都将获得一个更好的预测结果，这也是目前效果优先的建模场景下最常使用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18eb12-6c5d-4739-a059-d78483e45084",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">总的来说，本部分内容只有一个目标，那就是借助各类优化方法，抵达每个主流集成学习的效果上界。换而言之，本部分我们将围绕单模优化策略展开详细的探讨，涉及到的具体集成学习包括随机森林、XGBoost、LightGBM、和CatBoost等目前最主流的集成学习算法，而具体的优化策略则包括超参数优化器的使用、特征衍生和筛选方法的使用、单模型自融合方法的使用，这些优化方法也是截至目前，提升单模效果最前沿、最有效、同时也是最复杂的方法。其中有很多较为艰深的理论，也有很多是经验之谈，但无论如何，我们希望能够围绕当前数据集，让每个集成学习算法优化到极限。值得注意的是，在这个过程中，我们会将此前介绍的特征衍生和特征筛选视作是一种模型优化方法，衍生和筛选的效果，一律以模型的最终结果来进行评定。而围绕集成学习进行海量特征衍生和筛选，也才是特征衍生和筛选技术能发挥巨大价值的主战场。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a86ea0-cccf-4de5-a126-b5aee687876e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而在抵达了单模的极限后，我们就会进入到下一阶段，也就是模型融合阶段。需要知道的是，只有单模的效果到达了极限，进一步的多模型融合、甚至多层融合，才是有意义的，才是有效果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e94793-9c2e-40d9-936c-f3574b40e197",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851f898-1ff8-4bc0-9e4b-1b67e97a6428",
   "metadata": {},
   "source": [
    "# <center>Part 4.集成算法的训练与优化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc2ca29-af96-4a59-a9bb-1273e1d0506c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# 导入模型融合模块\n",
    "import manual_ensemble as me\n",
    "from manual_ensemble import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3da92-0c2b-40a8-9771-a8b6814684b0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后执行Part 1中的数据清洗相关工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714fd109-7f9c-4e56-9514-8a2e29f56e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "tcc = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# 标注连续/离散字段\n",
    "# 离散字段\n",
    "category_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "                'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "                'PaymentMethod']\n",
    "\n",
    "# 连续字段\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    " \n",
    "# 标签\n",
    "target = 'Churn'\n",
    "\n",
    "# ID列\n",
    "ID_col = 'customerID'\n",
    "\n",
    "# 验证是否划分能完全\n",
    "assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]\n",
    "\n",
    "# 连续字段转化\n",
    "tcc['TotalCharges']= tcc['TotalCharges'].apply(lambda x: x if x!= ' ' else np.nan).astype(float)\n",
    "tcc['MonthlyCharges'] = tcc['MonthlyCharges'].astype(float)\n",
    "\n",
    "# 缺失值填补\n",
    "tcc['TotalCharges'] = tcc['TotalCharges'].fillna(0)\n",
    "\n",
    "# 标签值手动转化 \n",
    "tcc['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "tcc['Churn'].replace(to_replace='No',  value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f1dac9-42a7-4205-b4d5-128411ddc0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tcc.drop(columns=[ID_col, target]).copy()\n",
    "labels = tcc['Churn'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6b0d2-04c2-456b-a5f2-c6be60658993",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时，创建自然编码后的数据集以及经过时序特征衍生的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c8d217-99cd-4ae5-8169-ee5f74dbc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train, test = train_test_split(tcc, random_state=22)\n",
    "\n",
    "X_train = train.drop(columns=[ID_col, target]).copy()\n",
    "X_test = test.drop(columns=[ID_col, target]).copy()\n",
    "\n",
    "y_train = train['Churn'].copy()\n",
    "y_test = test['Churn'].copy()\n",
    "\n",
    "X_train_seq = pd.DataFrame()\n",
    "X_test_seq = pd.DataFrame()\n",
    "\n",
    "# 年份衍生\n",
    "X_train_seq['tenure_year'] = ((72 - X_train['tenure']) // 12) + 2014\n",
    "X_test_seq['tenure_year'] = ((72 - X_test['tenure']) // 12) + 2014\n",
    "\n",
    "# 月份衍生\n",
    "X_train_seq['tenure_month'] = (72 - X_train['tenure']) % 12 + 1\n",
    "X_test_seq['tenure_month'] = (72 - X_test['tenure']) % 12 + 1\n",
    "\n",
    "# 季度衍生\n",
    "X_train_seq['tenure_quarter'] = ((X_train_seq['tenure_month']-1) // 3) + 1\n",
    "X_test_seq['tenure_quarter'] = ((X_test_seq['tenure_month']-1) // 3) + 1\n",
    "\n",
    "# 独热编码\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_train_seq)\n",
    "\n",
    "seq_new = list(X_train_seq.columns)\n",
    "\n",
    "# 创建带有列名称的独热编码之后的df\n",
    "X_train_seq = pd.DataFrame(enc.transform(X_train_seq).toarray(), \n",
    "                           columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "X_test_seq = pd.DataFrame(enc.transform(X_test_seq).toarray(), \n",
    "                          columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "# 调整index\n",
    "X_train_seq.index = X_train.index\n",
    "X_test_seq.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de3adf1-cf79-459b-b859-07f6f8a62fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(X_train[category_cols])\n",
    "\n",
    "X_train_OE = pd.DataFrame(ord_enc.transform(X_train[category_cols]), columns=category_cols)\n",
    "X_train_OE.index = X_train.index\n",
    "X_train_OE = pd.concat([X_train_OE, X_train[numeric_cols]], axis=1)\n",
    "\n",
    "X_test_OE = pd.DataFrame(ord_enc.transform(X_test[category_cols]), columns=category_cols)\n",
    "X_test_OE.index = X_test.index\n",
    "X_test_OE = pd.concat([X_test_OE, X_test[numeric_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ffe1a-6b58-46eb-b1fb-1a30c53cceb3",
   "metadata": {},
   "source": [
    "然后是模型融合部分所需的第三方库、准备的数据以及训练好的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b711d101-d89e-4c05-bb7f-768a36dfd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本节新增第三方库\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4bcb60-7dc8-4a61-8aeb-201c1fb9d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化KFold评估器\n",
    "kf = KFold(n_splits=5, random_state=12, shuffle=True)\n",
    "\n",
    "# 重置训练集和测试集的index\n",
    "X_train_OE = X_train_OE.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "train_part_index_l = []\n",
    "eval_index_l = []\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train_OE, y_train):\n",
    "    train_part_index_l.append(train_part_index)\n",
    "    eval_index_l.append(eval_index)\n",
    "    \n",
    "# 训练集特征\n",
    "X_train1 = X_train_OE.loc[train_part_index_l[0]]\n",
    "X_train2 = X_train_OE.loc[train_part_index_l[1]]\n",
    "X_train3 = X_train_OE.loc[train_part_index_l[2]]\n",
    "X_train4 = X_train_OE.loc[train_part_index_l[3]]\n",
    "X_train5 = X_train_OE.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集特征\n",
    "X_eval1 = X_train_OE.loc[eval_index_l[0]]\n",
    "X_eval2 = X_train_OE.loc[eval_index_l[1]]\n",
    "X_eval3 = X_train_OE.loc[eval_index_l[2]]\n",
    "X_eval4 = X_train_OE.loc[eval_index_l[3]]\n",
    "X_eval5 = X_train_OE.loc[eval_index_l[4]]\n",
    "\n",
    "# 训练集标签\n",
    "y_train1 = y_train.loc[train_part_index_l[0]]\n",
    "y_train2 = y_train.loc[train_part_index_l[1]]\n",
    "y_train3 = y_train.loc[train_part_index_l[2]]\n",
    "y_train4 = y_train.loc[train_part_index_l[3]]\n",
    "y_train5 = y_train.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集标签\n",
    "y_eval1 = y_train.loc[eval_index_l[0]]\n",
    "y_eval2 = y_train.loc[eval_index_l[1]]\n",
    "y_eval3 = y_train.loc[eval_index_l[2]]\n",
    "y_eval4 = y_train.loc[eval_index_l[3]]\n",
    "y_eval5 = y_train.loc[eval_index_l[4]]\n",
    "\n",
    "train_set = [(X_train1, y_train1), \n",
    "             (X_train2, y_train2), \n",
    "             (X_train3, y_train3), \n",
    "             (X_train4, y_train4), \n",
    "             (X_train5, y_train5)]\n",
    "\n",
    "eval_set = [(X_eval1, y_eval1), \n",
    "            (X_eval2, y_eval2), \n",
    "            (X_eval3, y_eval3), \n",
    "            (X_eval4, y_eval4), \n",
    "            (X_eval5, y_eval5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da0e273-1880-4b64-b7ff-e31bbf86cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型组\n",
    "grid_RF_1 = load('./models/grid_RF_1.joblib') \n",
    "grid_RF_2 = load('./models/grid_RF_2.joblib') \n",
    "grid_RF_3 = load('./models/grid_RF_3.joblib') \n",
    "grid_RF_4 = load('./models/grid_RF_4.joblib') \n",
    "grid_RF_5 = load('./models/grid_RF_5.joblib') \n",
    "\n",
    "RF_1 = grid_RF_1.best_estimator_\n",
    "RF_2 = grid_RF_2.best_estimator_\n",
    "RF_3 = grid_RF_3.best_estimator_\n",
    "RF_4 = grid_RF_4.best_estimator_\n",
    "RF_5 = grid_RF_5.best_estimator_\n",
    "\n",
    "RF_l = [RF_1, RF_2, RF_3, RF_4, RF_5]\n",
    "\n",
    "# 决策树模型组\n",
    "grid_tree_1 = load('./models/grid_tree_1.joblib')\n",
    "grid_tree_2 = load('./models/grid_tree_2.joblib')\n",
    "grid_tree_3 = load('./models/grid_tree_3.joblib')\n",
    "grid_tree_4 = load('./models/grid_tree_4.joblib')\n",
    "grid_tree_5 = load('./models/grid_tree_5.joblib')\n",
    "\n",
    "tree_1 = grid_tree_1.best_estimator_\n",
    "tree_2 = grid_tree_2.best_estimator_\n",
    "tree_3 = grid_tree_3.best_estimator_\n",
    "tree_4 = grid_tree_4.best_estimator_\n",
    "tree_5 = grid_tree_5.best_estimator_\n",
    "\n",
    "tree_l = [tree_1, tree_2, tree_3, tree_4, tree_5]\n",
    "\n",
    "# 逻辑回归模型组\n",
    "grid_lr_1 = load('./models/grid_lr_1.joblib')\n",
    "grid_lr_2 = load('./models/grid_lr_2.joblib')\n",
    "grid_lr_3 = load('./models/grid_lr_3.joblib')\n",
    "grid_lr_4 = load('./models/grid_lr_4.joblib')\n",
    "grid_lr_5 = load('./models/grid_lr_5.joblib')\n",
    "\n",
    "lr_1 = grid_lr_1.best_estimator_\n",
    "lr_2 = grid_lr_2.best_estimator_\n",
    "lr_3 = grid_lr_3.best_estimator_\n",
    "lr_4 = grid_lr_4.best_estimator_\n",
    "lr_5 = grid_lr_5.best_estimator_\n",
    "\n",
    "lr_l = [lr_1, lr_2, lr_3, lr_4, lr_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4836033-8708-4b20-9c6a-770eb69c9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1_predict_proba_RF = pd.Series(RF_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_RF = pd.Series(RF_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_RF = pd.Series(RF_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_RF = pd.Series(RF_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_RF = pd.Series(RF_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_RF = pd.concat([eval1_predict_proba_RF, \n",
    "                                   eval2_predict_proba_RF, \n",
    "                                   eval3_predict_proba_RF, \n",
    "                                   eval4_predict_proba_RF, \n",
    "                                   eval5_predict_proba_RF]).sort_index()\n",
    "\n",
    "eval1_predict_proba_tree = pd.Series(tree_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_tree = pd.Series(tree_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_tree = pd.Series(tree_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_tree = pd.Series(tree_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_tree = pd.Series(tree_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_tree = pd.concat([eval1_predict_proba_tree, \n",
    "                                     eval2_predict_proba_tree, \n",
    "                                     eval3_predict_proba_tree, \n",
    "                                     eval4_predict_proba_tree, \n",
    "                                     eval5_predict_proba_tree]).sort_index()\n",
    "\n",
    "eval1_predict_proba_lr = pd.Series(lr_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_lr = pd.Series(lr_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_lr = pd.Series(lr_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_lr = pd.Series(lr_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_lr = pd.Series(lr_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_lr = pd.concat([eval1_predict_proba_lr, \n",
    "                                   eval2_predict_proba_lr, \n",
    "                                   eval3_predict_proba_lr, \n",
    "                                   eval4_predict_proba_lr, \n",
    "                                   eval5_predict_proba_lr]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d25ee4c-5b60-49a0-9beb-fa3f51604aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_proba_RF = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF.append(RF_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_RF = np.array(test_predict_proba_RF)\n",
    "test_predict_proba_RF = test_predict_proba_RF.mean(0)\n",
    "\n",
    "test_predict_proba_tree = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree.append(tree_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_tree = np.array(test_predict_proba_tree)\n",
    "test_predict_proba_tree = test_predict_proba_tree.mean(0)\n",
    "\n",
    "test_predict_proba_lr = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr.append(lr_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_lr = np.array(test_predict_proba_lr)\n",
    "test_predict_proba_lr = test_predict_proba_lr.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f613b-2cac-45b4-8da7-e624be870226",
   "metadata": {},
   "source": [
    "## <center>Ch.3 模型融合基础方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed2e28-d2be-4f7f-bde6-c828622f96ea",
   "metadata": {},
   "source": [
    "- Stacking模型融合优化方法综述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e4c3c-1c18-4102-8807-bd6737881c36",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在此前的Stacking模型融合过程中，我们不难发现，其实无论每个流程的各个环节如何优化，最终结果还是存在一定过拟合倾向，从根本上来说，这其实是由于数据重复训练导致的必然结果。所谓的数据重复训练，指的是一级学习器和元学习器都在同一个数据集上反复训练，整个训练过程并没有纳入额外的信息，导致整体过拟合问题较为明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ca44f-5a76-415e-9a85-af45e440b5c6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而如何解决Stacking的过拟合问题进而提升融合效果，总的来说有两种思路，其一是从训练方法来说，若能通过修改训练流程使得不同模型的学习倾向各有不同，则可以提升一级学习器彼此之间的独立性，进而减少过拟合倾向、提升泛化能力。这种方法也被称为级联优化。其二则是从数据和特征的分配上来进行调整，例如，我们可以考虑给不同层的模型分配不同数据，这个过程也被称为Blending融合（混合融合），此外，还可以针对不同层的模型分配不同特征，则被称为特征增强方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8491b-da54-41a5-a937-fcb06eaf9ca9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;两种优化策略都是人们长期实践总结得到的、有可能对提升融合结果的方法。这里需要注意，这两大类策略都只是有可能能优化融合效果，而不是一定有效（如果一定有效，则会像交叉训练一样被列为必要操作）。因此，Stacking的优化方法实践其实还是需要结合当前情况来进行灵活调整，必要时也可以考虑暴力计算的思路，即各种方法都进行尝试，然后择优输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d51576-5f36-491b-a8e1-f9d3639a3398",
   "metadata": {},
   "source": [
    "&emsp;&emsp;无论如何，对这些方法的掌握都是必须的。这一小节我们先介绍流程相对简单清晰的Blending融合方法，下一小节将重点介绍级联优化。而特征增强其实是较为复杂的一块内容，我们将在课程偏后部分介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791ede9-2c59-4955-b565-83799cb61dc6",
   "metadata": {},
   "source": [
    "## 十一、Blending融合原理与实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6776f14-2961-47f1-a26f-9f4c5356c0a7",
   "metadata": {},
   "source": [
    "### 1.Blending融合的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835225e-77f1-468e-a734-f6dd82f64d5a",
   "metadata": {},
   "source": [
    "- Blending融合的训练和预测流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaadc1-ab3d-4326-9de6-2de1fa623e93",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Blending融合的基本过程和Stacking融合较为类似，都是两层模型的基本架构，即都是一级学习器进行训练，然后一级学习器的训练结果带入元学习器进行学习和训练。而和Stacking有所不同的是，为了避免一组数据重复训练导致的过拟合，Blending会在训练集中进一步划分训练集和留出集，一般比例为5：5到9：1不等。其中训练集用于一级学习器的训练，然后一级学习器围绕留出集进行预测，预测结果拼接成类似Stacking中的oof数据集，再将其带入元学习器进行模型训练。至此，即完成了两层模型的训练，其基本过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd3c0b-39cd-4917-acf1-83f096d2e886",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221008213642678.png\" alt=\"image-20221008213642678\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98143aef-adae-440e-9168-e9db84acc307",
   "metadata": {},
   "source": [
    "不难发现，Blending和Stacking的主要区别就在于一级学习器和元学习器带入训练的数据完全不同，由此即可避免由于数据重复训练导致的模型过拟合问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6908f-1ec4-4f0b-9425-f90353835276",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过需要注意的是，尽管拆分数据能够避免数据的重复训练，但由于此时两层模型带入训练的数据都有所减少，因此每一层模型本身的准确率会有所下降。二在每个独立的模型准确率有所下降的情况下，是否在融合后还能有更好的效果，其实并不确定。也就是说，Blending方法并不一定能获得一个比Stacking更好的融合结果，在大多数情况下，只能说Blending也是一种很有潜力的融合过程。若是追求融合效果极限，往往需要两种方法都进行尝试，然后择优输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee56bda-621b-465d-a8b0-123069a7f441",
   "metadata": {},
   "source": [
    "> 甚至有的时候，我们会同时采用多种不同融合流程训练多组元学习器，再来进行融合。该类方法属于融合结构上的拓展，这部分内容会在后续的案例中进行更深度的探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897955e9-f758-4fd3-b693-9cfee55fcb73",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而对于训练集和留出集的划分比例，一般来说是在5:5到9:1的范围内划分。训练集划分比例过高（留出集比例较低）则会导致元学习器偏差较大，而如果训练集划分比例较小（留出集比例较大），则会严重影响一级学习器的学习效果，而一级学习器是保障模型融合效果的核心，因此训练集比例较小对模型融合的结果是致命的。当然，在若不确定最佳比例情况下，通用做法是按照8:2的比例进行划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62840b93-c2a0-49a7-9f0e-dc2a684b9869",
   "metadata": {},
   "source": [
    "> 需要注意的是，此时的留出集就类似于交叉验证过程中的验证集，都是从训练集中划分出来的一部分特殊用途的数据集。在很多场景下，验证集的标签是不可知的，而此时我们手头上带有标签的数据集其实全部可以视作训练集，然后围绕这个训练集来进行训练集和留出集的划分。本案例中我们提前划分出了带标签的测试集，其实更多的是出于教学目的，为的是模拟一个在线提交结果验证的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14854bd3-1dc5-49c4-bd88-afe1891ef637",
   "metadata": {},
   "source": [
    "> 此外，关于Blending中具体比例，也可以作为超参数进行超参数搜索和优化。这部分属于级联优化的内容，相关方法将在下一小节进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0afe6c4-a442-4523-a474-b784d9247b4c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当在Blending流程中训练完两层模型后，实际的新数据的预测过程则和Stacking完全一致：只需要将新数据带入一级学习器输出预测结果，然后将预测结果带入元学习器进行预测即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29725693-eeeb-49b8-b09e-9c8791e91415",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而对于Blending元学习器选取，尽管Blending有数据集层面的信息隔离，理论上元学习器可以有更大的学习空间、同时也可以搭配更强的学习器。但实际训练过程中，限于元学习的训练数据数量有限、且学习数据仍然为一级学习器预测结果，因此元学习器网袜给仍然会表现出严重的过拟合倾向。因此，整体来看，Blending的元学习器的选择和优化方法基本需要和Stacking过程保持一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61764c6-a0de-4120-981f-1e14c55cd3af",
   "metadata": {},
   "source": [
    "- Blending融合的优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3070289-8e3e-46c7-aa75-6115c9816db5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，就整个Blending的融合过程来说，其实也有非常多和Stacking类似的优化方法。例如一级学习器也可以交叉训练，然后围绕留出集进行预测然后取均值，再带入元学习器预测；同时，一级学习器的交叉训练过程也可以配合超参数优化，元学习器的预测也可以采用此前定义的final_model_opt流程等等。很明显，这部分功能的都可以借鉴或改写此前Stacking部分定义的函数来实现。并且，由于新的重要变量的加入——留出集的划分比例，由此也将衍生出一系列的围绕划分比例优化的策略，这部分内容我们将在下一小节进行重点讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38095fd2-d5e2-43e8-a699-fc7f409fcabc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，Blending融合也有属于自己的优势优化策略——特征增强策略，即在元学习器中加入留出集特征。我们在Part 4.3.6中曾尝试在Stacking过程进行类似的特征增强操作，但结果是放大了过拟合倾向。而相比之下，Blending融合过程中，由于一级学习器并没有直接学习留出集特征，因此这部分数据对于元学习器来说还是会有较大的学习价值的，甚至Still等人在2009年的Feature-weighted linear stacking. arXiv preprint arXiv:0911.0460.论文中表示，将留出集的特征和一级学习器在留出集上的预测结果进行多项式特征衍生，能有效提升元学习器的泛化能力。这部分内容也是后续特征增强模块内容的重点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd3f4b-3d33-4d7c-a216-a15c4567d7c8",
   "metadata": {},
   "source": [
    "### 2.Blending模型融合的手动实现方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5b751-1ea8-470f-9390-610427e05256",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解了Blending的基本原理后，接下来尝试手动执行Blending融合流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2243c-2910-4fe7-b846-80223a21ed25",
   "metadata": {},
   "source": [
    "#### 2.1 划分训练集和留出集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6b73e-f1d6-4dfc-8c10-5a839aae01da",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，是训练集和留出集的划分，这里我们按照8：2的比例划分进行划分，并将训练集命名为train1、留出集命名为train2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17c0f405-5f06-4db5-a694-5f3351f9891a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>79.60</td>\n",
       "      <td>5515.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.00</td>\n",
       "      <td>241.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>19.00</td>\n",
       "      <td>73.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>55.55</td>\n",
       "      <td>551.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>20.05</td>\n",
       "      <td>91.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  SeniorCitizen  Partner  Dependents  PhoneService  MultipleLines  \\\n",
       "0     0.0            0.0      1.0         0.0           1.0            0.0   \n",
       "1     0.0            0.0      1.0         1.0           1.0            0.0   \n",
       "2     1.0            0.0      0.0         0.0           1.0            0.0   \n",
       "3     0.0            0.0      1.0         1.0           1.0            0.0   \n",
       "4     0.0            1.0      0.0         0.0           1.0            0.0   \n",
       "\n",
       "   InternetService  OnlineSecurity  OnlineBackup  DeviceProtection  \\\n",
       "0              0.0             2.0           2.0               0.0   \n",
       "1              1.0             0.0           2.0               0.0   \n",
       "2              2.0             1.0           1.0               1.0   \n",
       "3              0.0             0.0           2.0               2.0   \n",
       "4              2.0             1.0           1.0               1.0   \n",
       "\n",
       "   TechSupport  StreamingTV  StreamingMovies  Contract  PaperlessBilling  \\\n",
       "0          2.0          2.0              2.0       2.0               1.0   \n",
       "1          2.0          0.0              0.0       0.0               1.0   \n",
       "2          1.0          1.0              1.0       0.0               0.0   \n",
       "3          0.0          0.0              0.0       0.0               1.0   \n",
       "4          1.0          1.0              1.0       0.0               0.0   \n",
       "\n",
       "   PaymentMethod  tenure  MonthlyCharges  TotalCharges  \n",
       "0            0.0      68           79.60       5515.80  \n",
       "1            2.0       3           80.00        241.30  \n",
       "2            3.0       4           19.00         73.45  \n",
       "3            3.0      10           55.55        551.30  \n",
       "4            3.0       4           20.05         91.45  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_OE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51fb5fb8-1380-4224-bbee-730b3102a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train_OE, y_train,  test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00668fa6-7d42-46f1-ad0e-b20e254bd41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4225, 19)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38117261-c9ed-4782-b87c-a47b6382adf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1057, 19)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90241c8-f2aa-4ae1-a957-15f548886bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32</td>\n",
       "      <td>35.15</td>\n",
       "      <td>1051.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "      <td>25.10</td>\n",
       "      <td>712.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16</td>\n",
       "      <td>88.45</td>\n",
       "      <td>1422.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>89.05</td>\n",
       "      <td>6185.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>74.95</td>\n",
       "      <td>151.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  SeniorCitizen  Partner  Dependents  PhoneService  MultipleLines  \\\n",
       "2303     0.0            0.0      1.0         1.0           0.0            1.0   \n",
       "1277     0.0            0.0      0.0         0.0           0.0            1.0   \n",
       "1855     0.0            0.0      0.0         0.0           1.0            2.0   \n",
       "4280     0.0            0.0      1.0         1.0           1.0            2.0   \n",
       "1316     1.0            0.0      0.0         0.0           1.0            0.0   \n",
       "\n",
       "      InternetService  OnlineSecurity  OnlineBackup  DeviceProtection  \\\n",
       "2303              0.0             2.0           2.0               0.0   \n",
       "1277              0.0             0.0           0.0               0.0   \n",
       "1855              1.0             0.0           0.0               2.0   \n",
       "4280              0.0             2.0           2.0               2.0   \n",
       "1316              1.0             0.0           2.0               0.0   \n",
       "\n",
       "      TechSupport  StreamingTV  StreamingMovies  Contract  PaperlessBilling  \\\n",
       "2303          0.0          0.0              0.0       0.0               1.0   \n",
       "1277          0.0          0.0              0.0       0.0               1.0   \n",
       "1855          0.0          0.0              2.0       0.0               1.0   \n",
       "4280          2.0          2.0              2.0       2.0               1.0   \n",
       "1316          0.0          0.0              0.0       0.0               0.0   \n",
       "\n",
       "      PaymentMethod  tenure  MonthlyCharges  TotalCharges  \n",
       "2303            2.0      32           35.15       1051.05  \n",
       "1277            1.0      29           25.10        712.85  \n",
       "1855            2.0      16           88.45       1422.10  \n",
       "4280            0.0      68           89.05       6185.80  \n",
       "1316            2.0       2           74.95        151.75  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e899b0-b085-42b0-aaf6-fca2ddbcab90",
   "metadata": {},
   "source": [
    "注意此时各数据集的index是乱序的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fc661-3c91-4fd1-bca3-f00d7148d2d0",
   "metadata": {},
   "source": [
    "#### 2.2 一级学习器单模训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da3d07-4108-45cc-98d8-b860b59ce858",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后进行一级学习器的模型训练。这里我们先尝试围绕每个模型进行网格搜索的手动调参优化，也是非常高精度的搜索过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63aa1a-4f24-40db-af5f-df108d786f8a",
   "metadata": {},
   "source": [
    "- 随机森林模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd64d9b-26bb-4228-9986-0b3e13b9252d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是随机森林。模型超参数优化过程和此前一样，此处省略从第一轮开始的若干轮搜索，只展示最后一次超参数搜索及范围，基本数据如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b959ae4-a693-490b-8650-ce0fd4172e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.82048630714417\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 设置超参数空间\n",
    "parameter_space = {\"min_samples_leaf\": range(7, 10), \n",
    "                   \"min_samples_split\": range(1, 4),\n",
    "                   \"max_depth\": range(5, 8),\n",
    "                   \"max_leaf_nodes\": [None] + list(range(32, 49, 2)), \n",
    "                   \"n_estimators\": range(9, 12), \n",
    "                   \"max_features\":['sqrt', 'log2'] + list(range(4, 8)), \n",
    "                   \"max_samples\":[None, 0.55, 0.6, 0.65]}\n",
    "\n",
    "# 实例化模型与评估器\n",
    "RF_blending = RandomForestClassifier(random_state=12)\n",
    "grid_RF_blending = GridSearchCV(RF_blending, parameter_space, n_jobs=15)\n",
    "\n",
    "# 模型训练\n",
    "grid_RF_blending.fit(X_train1, y_train1)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e98eb8e-a618-4ee4-bce2-f5d467d4ef89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6,\n",
       " 'max_features': 6,\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': 0.6,\n",
       " 'min_samples_leaf': 8,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_RF_blending.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894995d-fcf2-4609-b9ef-9ace59758528",
   "metadata": {},
   "source": [
    "然后测试模型在训练集、留出集和测试集上的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0adb19d7-ad45-470a-bda9-8cfad7fc58fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8191715976331361, 0.8136234626300851, 0.7830777967064169)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_RF_blending.score(X_train1, y_train1), grid_RF_blending.score(X_train2, y_train2), grid_RF_blending.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6a0bb-82a1-49da-ad01-f4166a317edb",
   "metadata": {},
   "source": [
    "能够发现，最终测试集的效果有所下降。不过这里比较“凑巧”的是，训练集和留出集准确率较为接近，而和留出集效果差异较大，这也说明目前数据划分，对于随机森林来说，训练集和留出集规律较为一致，而和测试集规律相差较大，这也将提升最终Blending的过拟合风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce634522-0c41-49e9-9590-0539441d0395",
   "metadata": {},
   "source": [
    "> 一般来说，如果训练集和留出集较为一致，理论上可以考虑增加留出集划分比例。不过这只是理论上的建议，实际操作起来并具备可执行性。（何种程度谓之相似？此时又该给留出集腾出多少比例的数据？并没有操作的可行性）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61704cb3-fbc5-4215-825a-556ddb0629d9",
   "metadata": {},
   "source": [
    "- 逻辑回归模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c20958-f240-4c8e-aa20-a3b60f8073b4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后是逻辑回归模型训练，仍然是按照最高精度最高规格进行优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cb915ed-6a71-4fb5-940e-4729859d8189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618.0271792411804 s\n"
     ]
    }
   ],
   "source": [
    "# 设置转化器流\n",
    "logistic_pre = ColumnTransformer([\n",
    "    ('cat', preprocessing.OneHotEncoder(drop='if_binary'), category_cols), \n",
    "    ('num', 'passthrough', numeric_cols)\n",
    "])\n",
    "\n",
    "num_pre = ['passthrough', preprocessing.StandardScaler(), preprocessing.KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')]\n",
    "\n",
    "# 实例化逻辑回归评估器\n",
    "logistic_blending = logit_threshold(max_iter=int(1e8))\n",
    "\n",
    "# 设置机器学习流\n",
    "logistic_pipe = make_pipeline(logistic_pre, logistic_blending)\n",
    "\n",
    "# 设置超参数空间\n",
    "cw_l = [None, 'balanced']\n",
    "#cw_l.extend([{1: x} for x in np.arange(1, 4, 0.2)])\n",
    "logistic_param = [\n",
    "    {'columntransformer__num':num_pre, 'logit_threshold__thr': np.arange(0.1, 1, 0.1).tolist(), 'logit_threshold__penalty': ['l1'], 'logit_threshold__C': np.arange(0.1, 1.1, 0.1).tolist(), 'logit_threshold__solver': ['saga'], 'logit_threshold__class_weight':cw_l}, \n",
    "    {'columntransformer__num':num_pre, 'logit_threshold__thr': np.arange(0.1, 1, 0.1).tolist(), 'logit_threshold__penalty': ['l2'], 'logit_threshold__C': np.arange(0.1, 1.1, 0.1).tolist(), 'logit_threshold__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'], 'logit_threshold__class_weight':cw_l}, \n",
    "]\n",
    "\n",
    "# 实例化网格搜索评估器\n",
    "grid_lr_blending = GridSearchCV(estimator = logistic_pipe,\n",
    "                                param_grid = logistic_param,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs = 15)\n",
    "\n",
    "s = time.time()\n",
    "grid_lr_blending.fit(X_train1, y_train1)\n",
    "print(time.time()-s, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ee792c6-a361-4f16-942b-0d99d237abf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8073372781065089"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lr_blending.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72929199-1ef6-4766-9763-74aaa4ed5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8073372781065089, 0.8183538315988647, 0.7825099375354913)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lr_blending.score(X_train1, y_train1), grid_lr_blending.score(X_train2, y_train2), grid_lr_blending.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bcfd1b-e292-4605-ac1a-b2515d6cd917",
   "metadata": {},
   "source": [
    "- 决策树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b74f3-2e89-4d83-821b-445e504cabf1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后是决策树模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ed264c6-9ae5-4f60-880a-aeec4d1ba2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=12)\n",
    "\n",
    "tree_param = {'max_depth': np.arange(2, 16, 1).tolist(), \n",
    "              'min_samples_split': np.arange(1, 5, 1).tolist(), \n",
    "              'min_samples_leaf': np.arange(1, 4, 1).tolist(), \n",
    "              'max_leaf_nodes':np.arange(6, 30, 1).tolist()}\n",
    "\n",
    "grid_tree_blending = GridSearchCV(estimator = tree_model,\n",
    "                                  param_grid = tree_param,\n",
    "                                  n_jobs = 12).fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "402f1aca-aa84-4c00-ad82-055a55ca5c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8011834319526627"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree_blending.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7876668-0627-40f2-b3e1-01aafe0847c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8137278106508876, 0.7994323557237465, 0.7717206132879046)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree_blending.score(X_train1, y_train1), grid_tree_blending.score(X_train2, y_train2), grid_tree_blending.score(X_test_OE, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fdbeb-7223-4dcd-8279-3a093175471b",
   "metadata": {},
   "source": [
    "|模型|训练集|留出集|测试集|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|RF单模|0.8191|0.8136|0.7830|\n",
    "|LR单模|0.8073|0.8183|0.7825|\n",
    "|Tree单模|0.8137|0.7994|0.7717|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1dc11-0ca9-4177-b6c8-3f4ef2482694",
   "metadata": {},
   "source": [
    "#### 2.3 元学习器训练和预测数据创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fa996-a8d7-4bc2-bf50-1f62cb4faf08",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在训练完一级学习器后，开始创建元学习器的训练和预测数据，也就类似Stacking融合过程中的train_oof和test_predict。为了便于记忆，我们将Blending中元学习器的训练和预测数据集命名为train_oof_blending和test_predict_blending。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d5651-7a1a-41d5-acbb-54cecbe49989",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过和Stacking过程不同的是，Blending的train_oof_blending和test_predict_blending，是在留出集上进行的预测结果。可以通过如下方式进行创建："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792e9a4-b418-46ff-8287-971e87d5a71d",
   "metadata": {},
   "source": [
    "- train_oof_blending数据集创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "698cbb4f-721c-474d-9f10-184b672fb765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08218637, 0.5260078 , 0.14814201, ..., 0.01149645, 0.00475372,\n",
       "       0.04643898])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lr_blending.predict_proba(X_train2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b5215ff5-df4f-419b-b7da-bd8abbe8c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof_blending = pd.DataFrame({'lr_oof_blending': grid_lr_blending.predict_proba(X_train2)[:, 1], \n",
    "                                   'RF_oof_blending': grid_RF_blending.predict_proba(X_train2)[:, 1],\n",
    "                                   'tree_oof_blending': grid_tree_blending.predict_proba(X_train2)[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "90f3ba76-727d-435c-8ff7-affaa8886c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_oof_blending</th>\n",
       "      <th>RF_oof_blending</th>\n",
       "      <th>tree_oof_blending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.082186</td>\n",
       "      <td>0.041656</td>\n",
       "      <td>0.051867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.526008</td>\n",
       "      <td>0.448663</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148142</td>\n",
       "      <td>0.160260</td>\n",
       "      <td>0.194373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.719639</td>\n",
       "      <td>0.747084</td>\n",
       "      <td>0.787986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.388377</td>\n",
       "      <td>0.400153</td>\n",
       "      <td>0.524138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.038907</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.556429</td>\n",
       "      <td>0.614626</td>\n",
       "      <td>0.717557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.011496</td>\n",
       "      <td>0.048237</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.004754</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.046439</td>\n",
       "      <td>0.058229</td>\n",
       "      <td>0.051867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr_oof_blending  RF_oof_blending  tree_oof_blending\n",
       "0            0.082186         0.041656           0.051867\n",
       "1            0.526008         0.448663           0.462500\n",
       "2            0.148142         0.160260           0.194373\n",
       "3            0.719639         0.747084           0.787986\n",
       "4            0.388377         0.400153           0.524138\n",
       "...               ...              ...                ...\n",
       "1052         0.023818         0.038907           0.026367\n",
       "1053         0.556429         0.614626           0.717557\n",
       "1054         0.011496         0.048237           0.026367\n",
       "1055         0.004754         0.007760           0.026367\n",
       "1056         0.046439         0.058229           0.051867\n",
       "\n",
       "[1057 rows x 3 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof_blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee9750-02b7-4453-829c-6ed786619baf",
   "metadata": {},
   "source": [
    "- test_predict_blending数据集创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "142f74f6-d03e-4d58-98ef-c21e77713e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_blending = pd.DataFrame({'lr_oof_blending': grid_lr_blending.predict_proba(X_test_OE)[:, 1], \n",
    "                                      'RF_oof_blending': grid_RF_blending.predict_proba(X_test_OE)[:, 1],\n",
    "                                      'tree_oof_blending': grid_tree_blending.predict_proba(X_test_OE)[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "754de240-9c0b-4e03-a490-afc6a2f64d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_oof_blending</th>\n",
       "      <th>RF_oof_blending</th>\n",
       "      <th>tree_oof_blending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040403</td>\n",
       "      <td>0.057965</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.221748</td>\n",
       "      <td>0.202559</td>\n",
       "      <td>0.194373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030417</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064667</td>\n",
       "      <td>0.034394</td>\n",
       "      <td>0.051867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>0.178809</td>\n",
       "      <td>0.145293</td>\n",
       "      <td>0.109890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>0.034349</td>\n",
       "      <td>0.053799</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>0.124882</td>\n",
       "      <td>0.204525</td>\n",
       "      <td>0.194373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0.502636</td>\n",
       "      <td>0.505389</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0.061399</td>\n",
       "      <td>0.076281</td>\n",
       "      <td>0.051867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr_oof_blending  RF_oof_blending  tree_oof_blending\n",
       "0            0.040403         0.057965           0.026367\n",
       "1            0.221748         0.202559           0.194373\n",
       "2            0.006364         0.003488           0.026367\n",
       "3            0.030417         0.016963           0.026367\n",
       "4            0.064667         0.034394           0.051867\n",
       "...               ...              ...                ...\n",
       "1756         0.178809         0.145293           0.109890\n",
       "1757         0.034349         0.053799           0.026367\n",
       "1758         0.124882         0.204525           0.194373\n",
       "1759         0.502636         0.505389           0.462500\n",
       "1760         0.061399         0.076281           0.051867\n",
       "\n",
       "[1761 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe03ea-838f-42a5-b287-094fb29f6ae5",
   "metadata": {},
   "source": [
    "#### 2.4 元学习器训练测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac803d-363c-47af-84ee-0152012f44bb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来是元学习器的学习与预测过程。首先，我们可以执行类似于Stacking融合过程中的对比实验，即直接带入全部常用分类模型作为元学习器，测试训练效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b142adad-31b8-4020-983a-1f0018a12cff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train2-Accuracy: 0.818354, Test-Accuracy: 0.788189\n",
      "The results of tree-final:\n",
      "Train2-Accuracy: 1.000000, Test-Accuracy: 0.732538\n",
      "The results of KNN-final:\n",
      "Train2-Accuracy: 0.850520, Test-Accuracy: 0.766042\n",
      "The results of SVM-final:\n",
      "Train2-Accuracy: 0.826868, Test-Accuracy: 0.787053\n",
      "The results of GaussianNB-final:\n",
      "Train2-Accuracy: 0.807001, Test-Accuracy: 0.780239\n",
      "The results of Bagging-final:\n",
      "Train2-Accuracy: 0.978240, Test-Accuracy: 0.758660\n",
      "The results of RandomForest-final:\n",
      "Train2-Accuracy: 1.000000, Test-Accuracy: 0.762635\n",
      "The results of AdaBoost-final:\n",
      "Train2-Accuracy: 0.838221, Test-Accuracy: 0.779671\n",
      "The results of GBDT-final:\n",
      "Train2-Accuracy: 0.886471, Test-Accuracy: 0.781374\n",
      "[15:21:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "The results of XGB-final:\n",
      "Train2-Accuracy: 0.965941, Test-Accuracy: 0.767746\n"
     ]
    }
   ],
   "source": [
    "# 逻辑回归\n",
    "lr = LogisticRegression().fit(train_oof_blending, y_train2)\n",
    "print('The results of LR-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (lr.score(train_oof_blending, y_train2), lr.score(test_predict_blending, y_test)))\n",
    "\n",
    "# 决策树\n",
    "tree = DecisionTreeClassifier().fit(train_oof_blending, y_train2)\n",
    "print('The results of tree-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (tree.score(train_oof_blending, y_train2), tree.score(test_predict_blending, y_test)))\n",
    "\n",
    "# KNN最近邻分类器\n",
    "from sklearn import neighbors\n",
    "KNN = neighbors.KNeighborsClassifier().fit(train_oof_blending, y_train2)\n",
    "print('The results of KNN-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (KNN.score(train_oof_blending, y_train2), KNN.score(test_predict_blending, y_test)))\n",
    "\n",
    "# SVM支持向量机\n",
    "from sklearn import svm\n",
    "SVM = svm.SVC().fit(train_oof_blending, y_train2)\n",
    "print('The results of SVM-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (SVM.score(train_oof_blending, y_train2), SVM.score(test_predict_blending, y_test)))\n",
    "\n",
    "# 朴素贝叶斯/高斯贝叶斯\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(train_oof_blending, y_train2)\n",
    "print('The results of GaussianNB-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (gnb.score(train_oof_blending, y_train2), gnb.score(test_predict_blending, y_test)))\n",
    "\n",
    "# Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier().fit(train_oof_blending, y_train2)\n",
    "print('The results of Bagging-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (bagging.score(train_oof_blending, y_train2), bagging.score(test_predict_blending, y_test)))\n",
    "\n",
    "# 随机森林\n",
    "RFC = RandomForestClassifier().fit(train_oof_blending, y_train2)\n",
    "print('The results of RandomForest-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (RFC.score(train_oof_blending, y_train2), RFC.score(test_predict_blending, y_test)))\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ABC = AdaBoostClassifier().fit(train_oof_blending, y_train2)\n",
    "print('The results of AdaBoost-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (ABC.score(train_oof_blending, y_train2), ABC.score(test_predict_blending, y_test)))\n",
    "\n",
    "# GBDT\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier().fit(train_oof_blending, y_train2)\n",
    "print('The results of GBDT-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (GBC.score(train_oof_blending, y_train2), GBC.score(test_predict_blending, y_test)))\n",
    "\n",
    "# XGB\n",
    "from xgboost import XGBClassifier\n",
    "XGB = XGBClassifier().fit(train_oof_blending, y_train2)\n",
    "print('The results of XGB-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (XGB.score(train_oof_blending, y_train2), XGB.score(test_predict_blending, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37529508-835c-4ff3-b4ce-5ecca0aa2812",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从上面这组模型的结果能够明显看出，Blending融合结果整体与Stacking融合结果接近，并且也同样是逻辑回归等模型表现较好，最终融合效果也超过了单模最好。从长期实践来看，在不配合特征增强的情况下，元学习器的学习空间有限，仍然还是建议采用Stacking类似的元学习器选取和优化策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fb1f1-64e9-451c-8c65-1b49ca00dfc4",
   "metadata": {},
   "source": [
    "|模型|训练集|留出集|测试集|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|RF单模|0.8191|0.8136|0.7830|\n",
    "|LR单模|0.8073|0.8183|0.7825|\n",
    "|Tree单模|0.8137|0.7994|0.7717|\n",
    "|lr_final|-|0.8183|0.7881|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5d1ea-d485-494a-b065-7b403695a463",
   "metadata": {},
   "source": [
    "### 3.Blending模型融合优化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725a90c-b983-4e97-819f-16891f9ec39b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和Stacking类似，Blending的优化策略也分为一级学习器的交叉训练（包括自动超参数优化），以及元学习器的优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c3712-9023-4644-b497-146561329316",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1 一级学习器的交叉训练优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8d912-76ca-4a0a-9857-42c0a67388b2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，对于Blending的一级学习器来说，也是可以进行交叉训练的。不过相比Stacking，Blending一级学习器的交叉训练过程会相对简单一些。由于Blending是要求一级学习器围绕新的数据进行预测，因此交叉训练过程不再需要拼接oof数据集，而仅需要训练多组模型围绕相同的新的数据进行反复预测，然后求均值即可。就类似Stacking过程一级评估器对预测数据的预测过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44ecfd-aeb0-4c36-9ff0-4376ac2464fa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而这个过程我们可以通过改写此前定义的train_cross来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac3f330f-f383-445c-8b88-0a00bf1f1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from manual_ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "521fc3d0-ccbd-46bf-8c9a-bc709b91ddaf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_cross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Stacking融合过程一级学习器交叉训练函数\n",
       "\n",
       ":param X_train: 训练集特征\n",
       ":param y_train: 训练集标签\n",
       ":param X_test: 测试集特征\n",
       ":param estimators: 一级学习器，由(名称,评估器)组成的列表\n",
       ":param n_splits: 交叉训练折数\n",
       ":param random_state: 随机数种子\n",
       "\n",
       ":return：交叉训练后创建oof训练数据和测试集平均预测结果\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\work\\jupyter\\telco\\正式课程\\manual_ensemble.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cross?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec01ed-9a83-4b1a-8bc3-2d1276d337aa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;train_cross改写过程如下。核心是在原先的train_cross过程中加入blending的过程判断，若是Blending融合，则额外执行训练集内部的留出集划分过程，并且预测过程不再是oof拼接，而是重复预测后求均值。此外，考虑到Blending过程中oof数据集不再包含完整的训练集所有条目，且训练集的进一步划分是在函数内部完成，因此该函数输出的结果需要包含oof数据集的标签，因此，此时的oof数据集而是同时输出特征和标签，且标签放在最后一列，方便后续单独提取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ea06c90-06de-4041-9acf-63869bd32b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Churn'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beece070-5279-418b-bb6e-d0787cced575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cross(X_train, y_train, X_test, estimators, test_size=0.2, n_splits=5, random_state=12, blending=False):\n",
    "    \"\"\"\n",
    "    Stacking融合过程一级学习器交叉训练函数\n",
    "    \n",
    "    :param X_train: 训练集特征\n",
    "    :param y_train: 训练集标签\n",
    "    :param X_test: 测试集特征\n",
    "    :param estimators: 一级学习器，由(名称,评估器)组成的列表\n",
    "    :param n_splits: 交叉训练折数\n",
    "    :param test_size: blending过程留出集占比\n",
    "    :param random_state: 随机数种子\n",
    "    :param blending: 是否进行blending融合\n",
    "    \n",
    "    :return：交叉训练后创建oof训练数据和测试集平均预测结果，同时包含特征和标签，标签在最后一列\n",
    "    \"\"\"    \n",
    "    # 创建一级评估器输出的训练集预测结果和测试集预测结果数据集\n",
    "    if blending == True:\n",
    "        X, X1, y, y1 = train_test_split(X_train, y_train, test_size=test_size, random_state=random_state)\n",
    "        m = X1.shape[0]\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        X1 = X1.reset_index(drop=True)\n",
    "        y1 = y1.reset_index(drop=True)\n",
    "    else:\n",
    "        m = X_train.shape[0]\n",
    "        X = X_train.reset_index(drop=True)\n",
    "        y = y_train.reset_index(drop=True)\n",
    "    \n",
    "    n = len(estimators)\n",
    "    m_test = X_test.shape[0]\n",
    "    \n",
    "    columns = []\n",
    "    for estimator in estimators:\n",
    "        columns.append(estimator[0] + '_oof')\n",
    "    \n",
    "    train_oof = pd.DataFrame(np.zeros((m, n)), columns=columns)\n",
    "    \n",
    "    columns = []\n",
    "    for estimator in estimators:\n",
    "        columns.append(estimator[0] + '_predict')\n",
    "    \n",
    "    test_predict = pd.DataFrame(np.zeros((m_test, n)), columns=columns)\n",
    "    \n",
    "    # 实例化重复交叉验证评估器\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # 执行交叉训练\n",
    "    for estimator in estimators:\n",
    "        model = estimator[1]\n",
    "        oof_colName = estimator[0] + '_oof'\n",
    "        predict_colName = estimator[0] + '_predict'\n",
    "        \n",
    "        for train_part_index, eval_index in kf.split(X, y):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X.loc[train_part_index]\n",
    "            y_train_part = y.loc[train_part_index]\n",
    "            model.fit(X_train_part, y_train_part)\n",
    "            if blending == True:\n",
    "                # 在留出集上进行预测并求均值\n",
    "                train_oof[oof_colName] += model.predict_proba(X1)[:, 1] / n_splits\n",
    "                # 在测试集上进行预测并求均值\n",
    "                test_predict[predict_colName] += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "            else:\n",
    "                # 在验证集上进行验证\n",
    "                X_eval_part = X.loc[eval_index]\n",
    "                # 将验证集上预测结果拼接入oof数据集\n",
    "                train_oof[oof_colName].loc[eval_index] = model.predict_proba(X_eval_part)[:, 1]\n",
    "                # 将测试集上预测结果填入predict数据集\n",
    "                test_predict[predict_colName] += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "    \n",
    "    # 添加标签列\n",
    "    if blending == True:\n",
    "        train_oof[y1.name] = y1\n",
    "    else:\n",
    "        train_oof[y.name] = y\n",
    "        \n",
    "    return train_oof, test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791ced9-4791-4587-834c-708c3f58824f",
   "metadata": {},
   "source": [
    "然后测试函数效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6766b31e-cedb-4b07-ab3a-e4ffb3e2e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('lr', grid_lr_blending.best_estimator_), ('tree', grid_tree_blending.best_estimator_), ('rf', grid_RF_blending.best_estimator_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3db27de4-ae54-4cfa-9377-d616ab2d060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, y_train, X_test_OE, estimators, blending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "882b20e3-f864-4419-9a1f-4c6f4f810e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_oof</th>\n",
       "      <th>tree_oof</th>\n",
       "      <th>rf_oof</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065765</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.044780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.471787</td>\n",
       "      <td>0.349819</td>\n",
       "      <td>0.407269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150529</td>\n",
       "      <td>0.233834</td>\n",
       "      <td>0.172493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.713892</td>\n",
       "      <td>0.684692</td>\n",
       "      <td>0.735245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.426013</td>\n",
       "      <td>0.516487</td>\n",
       "      <td>0.436409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.046418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.593308</td>\n",
       "      <td>0.647285</td>\n",
       "      <td>0.590749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.043482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.052917</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.093293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lr_oof  tree_oof    rf_oof  Churn\n",
       "0     0.065765  0.042398  0.044780      0\n",
       "1     0.471787  0.349819  0.407269      1\n",
       "2     0.150529  0.233834  0.172493      0\n",
       "3     0.713892  0.684692  0.735245      1\n",
       "4     0.426013  0.516487  0.436409      0\n",
       "...        ...       ...       ...    ...\n",
       "1052  0.022700  0.030196  0.046418      0\n",
       "1053  0.593308  0.647285  0.590749      0\n",
       "1054  0.010672  0.030196  0.043482      0\n",
       "1055  0.003120  0.030196  0.015776      0\n",
       "1056  0.052917  0.042398  0.093293      0\n",
       "\n",
       "[1057 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof_blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f82f-f1f5-4f85-ac5e-64f1e13b10b1",
   "metadata": {},
   "source": [
    "而oof的特征标签可以按照如下方式分别提取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10b8a828-39de-42cc-81f5-29a036bddd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_oof</th>\n",
       "      <th>tree_oof</th>\n",
       "      <th>rf_oof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065765</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.044780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.471787</td>\n",
       "      <td>0.349819</td>\n",
       "      <td>0.407269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150529</td>\n",
       "      <td>0.233834</td>\n",
       "      <td>0.172493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.713892</td>\n",
       "      <td>0.684692</td>\n",
       "      <td>0.735245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.426013</td>\n",
       "      <td>0.516487</td>\n",
       "      <td>0.436409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.046418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.593308</td>\n",
       "      <td>0.647285</td>\n",
       "      <td>0.590749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.043482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.015776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.052917</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.093293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lr_oof  tree_oof    rf_oof\n",
       "0     0.065765  0.042398  0.044780\n",
       "1     0.471787  0.349819  0.407269\n",
       "2     0.150529  0.233834  0.172493\n",
       "3     0.713892  0.684692  0.735245\n",
       "4     0.426013  0.516487  0.436409\n",
       "...        ...       ...       ...\n",
       "1052  0.022700  0.030196  0.046418\n",
       "1053  0.593308  0.647285  0.590749\n",
       "1054  0.010672  0.030196  0.043482\n",
       "1055  0.003120  0.030196  0.015776\n",
       "1056  0.052917  0.042398  0.093293\n",
       "\n",
       "[1057 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oof数据集特征\n",
    "train_oof_blending.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd64ab94-f40c-4cc7-bce3-e0a3d46883ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "1052    0\n",
       "1053    0\n",
       "1054    0\n",
       "1055    0\n",
       "1056    0\n",
       "Name: Churn, Length: 1057, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oof数据集标签\n",
    "train_oof_blending.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d97612e-91db-41b1-9830-2a222261fd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_predict</th>\n",
       "      <th>tree_predict</th>\n",
       "      <th>rf_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.018056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246439</td>\n",
       "      <td>0.233834</td>\n",
       "      <td>0.285498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023823</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.007995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058757</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.016456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>0.162301</td>\n",
       "      <td>0.102205</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>0.023821</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.023841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>0.149621</td>\n",
       "      <td>0.203031</td>\n",
       "      <td>0.145774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0.493815</td>\n",
       "      <td>0.492869</td>\n",
       "      <td>0.456376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0.078041</td>\n",
       "      <td>0.233834</td>\n",
       "      <td>0.128576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr_predict  tree_predict  rf_predict\n",
       "0       0.028461      0.030196    0.018056\n",
       "1       0.246439      0.233834    0.285498\n",
       "2       0.003332      0.030196    0.009496\n",
       "3       0.023823      0.030196    0.007995\n",
       "4       0.058757      0.042398    0.016456\n",
       "...          ...           ...         ...\n",
       "1756    0.162301      0.102205    0.206897\n",
       "1757    0.023821      0.030196    0.023841\n",
       "1758    0.149621      0.203031    0.145774\n",
       "1759    0.493815      0.492869    0.456376\n",
       "1760    0.078041      0.233834    0.128576\n",
       "\n",
       "[1761 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09c57a-cf3c-47b8-afd1-d46babced9c2",
   "metadata": {},
   "source": [
    "然后带入逻辑回归元学习器测试效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1fa312db-9b40-4623-819f-e729c7e49986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train2-Accuracy: 0.825922, Test-Accuracy: 0.793299\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "print('The results of LR-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (lr.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]), lr.score(test_predict_blending, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663aaef6-c5b7-416a-8e23-0b6a9680983b",
   "metadata": {},
   "source": [
    "能够发现最终结果有较大提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553050b-1c71-4883-802c-88aa0af5e37f",
   "metadata": {},
   "source": [
    "> 当然，模型融合的结果也存在一定的偶然性，大多数情况下，无论是Stacking还是Blending，都是在千分位左右提升模型效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82874e-f24c-4067-999a-9811d055efb9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，此时改写后train_cross函数将能够同时执行Stacking和Blending融合的一级学习器交叉训练+train_oof数据集创建。后续需要将改写的函数替换manual_ensemble.py中原始的train_cross函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73664b61-c38d-461c-a41f-cc9ab1e3c0d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.2 元学习器优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76ac54-b11a-484c-8010-1b38a1e4fd10",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来继续讨论Blending中元学习器的优化过程。这部分优化和Stacking几乎一致，都是选择决策树模型或者逻辑回归模型，进行单模优化—交叉训练优化或者Bagging集成，然后从三个环节中的6个结果中择优输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024e12a-1e6b-4f53-8781-fc74564ca938",
   "metadata": {},
   "source": [
    "- 手动实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e1d48-0f29-47ae-a6b5-4be643032694",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和此前一样，首先我们可以尝试手动进行优化。首先是逻辑回归模型的超参数优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7483de18-4f1f-444c-bb45-373f3c9cc0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8278145695364238, 0.787052810902896)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置超参数空间\n",
    "logistic_param = [\n",
    "    {'thr': np.arange(0.1, 1, 0.1).tolist(), 'penalty': ['l1'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['saga']}, \n",
    "    {'thr': np.arange(0.1, 1, 0.1).tolist(), 'penalty': ['l2'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']}, \n",
    "]\n",
    "\n",
    "# 实例化相关评估器\n",
    "logistic_final = logit_threshold(max_iter=int(1e6))\n",
    "    \n",
    "# 执行网格搜索\n",
    "lfg = GridSearchCV(estimator = logistic_final,\n",
    "                   param_grid = logistic_param,\n",
    "                   scoring='accuracy',\n",
    "                   n_jobs = 15).fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "\n",
    "lfg.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]), lfg.score(test_predict_blending, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f7174972-24bb-4ab5-a3cb-1adfa28d0721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.2, 'penalty': 'l2', 'solver': 'lbfgs', 'thr': 0.4}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0080e8-3b16-4e7e-83ae-24c7c833782a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来是决策树模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f72adb1d-d6e2-47c4-8c05-33891d314cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.826868495742668, 0.7859170925610448)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化决策树评估器\n",
    "tree_final = DecisionTreeClassifier()\n",
    "\n",
    "tree_param = {'max_depth': np.arange(2, 16, 1).tolist(), \n",
    "              'min_samples_split': np.arange(1, 5, 1).tolist(), \n",
    "              'min_samples_leaf': np.arange(1, 4, 1).tolist(), \n",
    "              'max_leaf_nodes':np.arange(6, 30, 1).tolist()}\n",
    "\n",
    "# 实例化网格搜索评估器\n",
    "tfg = GridSearchCV(estimator = tree_final,\n",
    "                   param_grid = tree_param,\n",
    "                   n_jobs = 12).fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "\n",
    "tfg.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]), tfg.score(test_predict_blending, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "75823570-980e-4e8b-a062-6a15f0454e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2,\n",
       " 'max_leaf_nodes': 6,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c76f4-2c71-47df-94c7-09498ac8d5b7",
   "metadata": {},
   "source": [
    "然后是交叉训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40226bc4-09a7-401a-a324-9716f327fb23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7881885292447472\n"
     ]
    }
   ],
   "source": [
    "# 逻辑回归交叉训练\n",
    "res = np.zeros(test_predict_blending.shape[0])\n",
    "\n",
    "folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=12)\n",
    "\n",
    "for trn_idx, val_idx in folds.split(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]):\n",
    "    lfg = GridSearchCV(estimator = logit_threshold(max_iter=int(1e6)),\n",
    "                       param_grid = logistic_param,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs = 15)\n",
    "    lfg.fit(train_oof_blending.iloc[:, :-1].loc[trn_idx], train_oof_blending.iloc[:, -1].loc[trn_idx])\n",
    "    res += lfg.predict_proba(test_predict_blending)[:, 1] / 10\n",
    "    \n",
    "print(accuracy_score((res >= 0.5) * 1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6d9f10f-583f-46bf-89c0-2959b6b86402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7830777967064169\n"
     ]
    }
   ],
   "source": [
    "# 决策树交叉训练过程\n",
    "res = np.zeros(test_predict_blending.shape[0])\n",
    "\n",
    "folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=12)\n",
    "\n",
    "for trn_idx, val_idx in folds.split(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]):\n",
    "    tfg = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
    "                       param_grid = tree_param,\n",
    "                       n_jobs = 12)\n",
    "    tfg.fit(train_oof_blending.iloc[:, :-1].loc[trn_idx], train_oof_blending.iloc[:, -1].loc[trn_idx])\n",
    "    res += tfg.predict_proba(test_predict_blending)[:, 1] / 10\n",
    "\n",
    "print(accuracy_score((res >= 0.5) * 1, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa12ec6-b7a1-4eca-b856-011eb37f4ee9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后，尝试带入经过超参数优化的基础评估器到Bagging中来进行训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "678b91b5-2277-41d2-b8bc-e3819a3a56ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8221381267738883, 0.7864849517319704)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 设置超参数空间\n",
    "parameter_space = {\n",
    "    \"n_estimators\": range(10, 21), \n",
    "    \"max_samples\": np.arange(0.1, 1.1, 0.1).tolist(),\n",
    "    'max_features':np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "# 实例化模型与评估器\n",
    "bagging_final = BaggingClassifier(LogisticRegression(C=0.3, \n",
    "                                                     penalty='l1',\n",
    "                                                     solver='saga'))\n",
    "BG = GridSearchCV(bagging_final, parameter_space, n_jobs=15).fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "\n",
    "BG.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]), BG.score(test_predict_blending, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6e29b978-8327-47eb-a7d9-0853f1e48d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8334910122989593, 0.7932992617830777)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置超参数空间\n",
    "parameter_space = {\n",
    "    \"n_estimators\": range(10, 21), \n",
    "    \"max_samples\": np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "# 实例化模型与评估器\n",
    "bagging_final = BaggingClassifier(DecisionTreeClassifier(max_depth=3,\n",
    "                                                         max_leaf_nodes=7,\n",
    "                                                         min_samples_leaf=1,\n",
    "                                                         min_samples_split=2))\n",
    "BG = GridSearchCV(bagging_final, parameter_space, n_jobs=15).fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "\n",
    "BG.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]), BG.score(test_predict_blending, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cdaaf-8c3a-4572-8e40-acf0d12d8b7e",
   "metadata": {},
   "source": [
    "能够发现优化流程仍然能输出一个较好的结果。当然，通过上述流程不难发现，这里仍然可以调用此前定义的final_model_opt函数来完成整个优化流程，只需要按需调整输入数据对象即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a1c7b-b853-4033-a5f3-bda69622dd1a",
   "metadata": {},
   "source": [
    "- 借助final_model_opt实现Blending元学习器优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7af5cbad-66b1-49b7-9062-c8820e7276ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mfinal_model_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_model_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_space_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Stacking元学习器自动优化与预测函数\n",
       "\n",
       ":param final_model_l: 备选元学习器组成的列表\n",
       ":param param_space_l: 备选元学习器各自超参数搜索空间组成的列表\n",
       ":param X: oof_train训练集特征\n",
       ":param y: oof_train训练集标签\n",
       ":param test_predict: 一级评估器输出的测试集预测结果\n",
       "\n",
       ":return：多组元学习器在oof_train上的最佳评分，以及最佳元学习器在test_predict上的预测结果\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\vdmion\\appdata\\local\\temp\\ipykernel_16160\\67447601.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model_opt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6adc31ac-88b8-421a-89d3-33b8c49b7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8403cccf-f468-4aba-a842-0e9ec819354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_param = [{'thr': np.arange(0.1, 1.1, 0.1).tolist(), 'penalty': ['l1'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['saga']}, \n",
    "                  {'thr': np.arange(0.1, 1.1, 0.1).tolist(), 'penalty': ['l2'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']}]\n",
    "\n",
    "tree_final_param = {'max_depth': np.arange(2, 16, 1).tolist(), \n",
    "                    'min_samples_split': np.arange(1, 5, 1).tolist(), \n",
    "                    'min_samples_leaf': np.arange(1, 4, 1).tolist(), \n",
    "                    'max_leaf_nodes':np.arange(6, 30, 1).tolist()}\n",
    "\n",
    "param_space_l = [lr_final_param, tree_final_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6c19f4d1-3c30-45dc-a742-b70961c3c5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_res_final, best_test_predict_final = final_model_opt(final_model_l, \n",
    "                                                          param_space_l, \n",
    "                                                          train_oof_blending.iloc[:, :-1], \n",
    "                                                          train_oof_blending.iloc[:, -1], \n",
    "                                                          test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "08c39555-cd70-478d-be4b-708f34396070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7950028392958546"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score((best_test_predict_final >= 0.5) * 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4937a53-2844-4609-9ee0-d71bc10896e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3 一级学习器交叉训练过程的自动超参数优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc75246-a22c-43ac-82a0-c99daf5a085a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后就是关于一级交叉训练过程中的自动超参数优化实现。由于Blending和Stacking只是在训练数据的选取上有所区别，实际的训练流程是完全一样的，因此这部分功能实现也完全可以复用上一小节定义的优化流程，仍然还是实例化能够自动进行超参数搜索的评估器，然后带入此前修改过的train_cross函数即可。首先实例化各评估器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2690ab88-353b-480c-9c6e-8e6f632f752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40003242-e3a3-446f-a2bf-577aacf70dd0",
   "metadata": {},
   "source": [
    "然后执行自动超参数优化的交叉训练，并创建train_oof_blending, test_predict_blending数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e2929ef9-81d9-4491-bad1-ae1a90647bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 20/20 [02:34<00:00,  7.74s/trial, best loss: -0.7867455621301775]\n",
      "100%|███████████████████████████████████████████████| 20/20 [03:11<00:00,  9.58s/trial, best loss: -0.7881656804733728]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:13<00:00,  6.66s/trial, best loss: -0.7898714451578608]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:08<00:00,  6.43s/trial, best loss: -0.7934197826178186]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:04<00:00,  6.21s/trial, best loss: -0.7870298096157342]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.59trial/s, best loss: -0.7988165680473374]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 24.03trial/s, best loss: -0.7917159763313609]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 23.95trial/s, best loss: -0.7986249248115043]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.63trial/s, best loss: -0.8033578133087135]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.45trial/s, best loss: -0.7912879264761424]\n",
      "100%|█████████████████████████████████████████████| 500/500 [06:29<00:00,  1.28trial/s, best loss: -0.8049704142011833]\n",
      "100%|██████████████████████████████████████████████| 500/500 [07:05<00:00,  1.17trial/s, best loss: -0.810414201183432]\n",
      "100%|█████████████████████████████████████████████| 500/500 [06:12<00:00,  1.34trial/s, best loss: -0.8125855050568636]\n",
      "100%|██████████████████████████████████████████████| 500/500 [06:10<00:00,  1.35trial/s, best loss: -0.811875445885266]\n",
      "100%|█████████████████████████████████████████████| 500/500 [07:06<00:00,  1.17trial/s, best loss: -0.8090354889700226]\n"
     ]
    }
   ],
   "source": [
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, y_train, X_test_OE, estimators=estimators, blending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a3ccb-d867-4ba1-8f1e-46246e04ab27",
   "metadata": {},
   "source": [
    "当然，接下来我们就可以将其带入元学习器优化函数，测试最终的优化效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9b06485f-1ae5-4211-8d12-114dfbbb7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_res_final, best_test_predict_final = final_model_opt(final_model_l, \n",
    "                                                          param_space_l, \n",
    "                                                          train_oof_blending.iloc[:, :-1], \n",
    "                                                          train_oof_blending.iloc[:, -1], \n",
    "                                                          test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "519bb68b-d466-4ef3-8b29-b281260da7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7950028392958546"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score((best_test_predict_final >= 0.5) * 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a1de4-d492-4389-9203-f0fd6c235362",
   "metadata": {},
   "source": [
    "至此，我们就完成了自动训练和优化的Blending模型融合。同样，这里需要注意计算时间，对于Blending融合来说，一级学习器的训练、优化以及oof数据集的产出，约需要50分钟左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3727ed4-72f8-4048-b087-621b296e6d89",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4 Blending极限效果测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f613f-4e80-4ec9-917a-6a938c634a39",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后，和上一小节类似，让我们增加单模型在交叉训练过程中的超参数搜索迭代次数，来测试当前流程下Blending的效果极限："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a689716-b244-4ed8-81ea-2c308e0b2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyper = lr_cascade(lr_params_space, max_evals=50)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space, max_evals=1000)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "72d675ad-8cbb-4831-bd9e-a4b97ee49c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [05:16<00:00,  6.32s/trial, best loss: -0.7924260355029585]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:28<00:00,  6.58s/trial, best loss: -0.7912426035502959]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:13<00:00,  6.28s/trial, best loss: -0.7922346720382727]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:07<00:00,  6.15s/trial, best loss: -0.7964955866101529]\n",
      "100%|███████████████████████████████████████████████| 50/50 [05:18<00:00,  6.37s/trial, best loss: -0.7893947151230293]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 23.90trial/s, best loss: -0.7988165680473374]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 24.02trial/s, best loss: -0.7917159763313609]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:41<00:00, 24.12trial/s, best loss: -0.7986249248115043]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.69trial/s, best loss: -0.8033578133087135]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:42<00:00, 23.68trial/s, best loss: -0.7912879264761424]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:13<00:00,  1.36trial/s, best loss: -0.8047337278106509]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:49<00:00,  1.30trial/s, best loss: -0.8123076923076924]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:38<00:00,  1.32trial/s, best loss: -0.8128216319051017]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [13:33<00:00,  1.23trial/s, best loss: -0.8111659462559626]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [12:51<00:00,  1.30trial/s, best loss: -0.8095083022087932]\n"
     ]
    }
   ],
   "source": [
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, y_train, X_test_OE, estimators=estimators, blending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bc65bfd5-fe96-4902-afaa-2732fa538fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_res_final, best_test_predict_final = final_model_opt(final_model_l, \n",
    "                                                          param_space_l, \n",
    "                                                          train_oof_blending.iloc[:, :-1], \n",
    "                                                          train_oof_blending.iloc[:, -1], \n",
    "                                                          test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a71e8bc7-d40b-4b65-bd9e-065e06ea398e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7961385576377058"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score((best_test_predict_final >= 0.5) * 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c568c2-d28f-4f4c-8121-f3d804f3e3d8",
   "metadata": {},
   "source": [
    "极限效果测试下，一次运行约需要1个半小时。并且，最终得到的融合结果略好于单模建模结果，略差于上一小节的自动Stacking融合结果（0.7978）。至此，我们就完整执行了手动和自动Blending融合及优化各流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc5eec-a54f-464a-84f0-0b8db8bafbf8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;尽管此处Blending融合结果不如Stacking，但这只是在当前数据集划分比例和随机数种子情况下融合得到的结果，倘若这些参数发生变化，则最终融合结果也会随之变化。而对于留出集划分及使用方法层面的调整，则是Blending融合优化的进阶内容、同时也是Blending融合独有的优化策略，这部分内容将在下一小节进行详细讨论。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
