{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6a40fa-91de-4089-b4ca-923a9e9c4a52",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> 【Kaggle】Telco Customer Churn 电信用户流失预测案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23daa954-a44a-4a88-a35a-af873a929a59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4c6be-85ef-44d3-94a1-72f86c5571e5",
   "metadata": {},
   "source": [
    "## <font face=\"仿宋\">第四部分导读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c525fe7-13fe-454d-9497-5361cd0e8850",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">在案例的第二、三部分中，我们详细介绍了关于特征工程的各项技术，特征工程技术按照大类来分可以分为数据预处理、特征衍生、特征筛选三部分，其中特征预处理的目的是为了将数据集整理、清洗到可以建模的程度，具体技术包括缺失值处理、异常值处理、数据重编码等，是建模之前必须对数据进行的处理和操作；而特征衍生和特征筛选则更像是一类优化手段，能够帮助模型突破当前数据集建模的效果上界。并且我们在第二部分完整详细的介绍机器学习可解释性模型的训练、优化和解释方法，也就是逻辑回归和决策树模型。并且此前我们也一直以这两种算法为主，来进行各个部分的模型测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e42ac-1504-4a3f-a7c1-0a9619743f2f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而第四部分，我们将开始介绍集成学习的训练和优化的实战技巧，尽管从可解释性角度来说，集成学习的可解释性并不如逻辑回归和决策树，但在大多数建模场景下，集成学习都将获得一个更好的预测结果，这也是目前效果优先的建模场景下最常使用的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadcd2e-0796-45ee-bb50-bae5602cb6b5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">总的来说，本部分内容只有一个目标，那就是借助各类优化方法，抵达每个主流集成学习的效果上界。换而言之，本部分我们将围绕单模优化策略展开详细的探讨，涉及到的具体集成学习包括随机森林、XGBoost、LightGBM、和CatBoost等目前最主流的集成学习算法，而具体的优化策略则包括超参数优化器的使用、特征衍生和筛选方法的使用、单模型自融合方法的使用，这些优化方法也是截至目前，提升单模效果最前沿、最有效、同时也是最复杂的方法。其中有很多较为艰深的理论，也有很多是经验之谈，但无论如何，我们希望能够围绕当前数据集，让每个集成学习算法优化到极限。值得注意的是，在这个过程中，我们会将此前介绍的特征衍生和特征筛选视作是一种模型优化方法，衍生和筛选的效果，一律以模型的最终结果来进行评定。而围绕集成学习进行海量特征衍生和筛选，也才是特征衍生和筛选技术能发挥巨大价值的主战场。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98698944-ea58-4ea2-bc3f-e92691f35428",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">而在抵达了单模的极限后，我们就会进入到下一阶段，也就是模型融合阶段。需要知道的是，只有单模的效果到达了极限，进一步的多模型融合、甚至多层融合，才是有意义的，才是有效果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b59d4-9452-44c4-8161-f0dca0f8b625",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18148a4-e7dc-4d53-814c-ff8ee0fe0247",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center>Part 4.集成算法的训练与优化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a197484-55ab-4eb1-a02c-8a3124fc047e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# 导入模型融合模块\n",
    "import manual_ensemble as me\n",
    "from manual_ensemble import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa9ec4-1069-4e65-8fe0-0e0f1d52b17e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后执行Part 1中的数据清洗相关工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06e2985-0144-49d0-a42d-e85d5ebba4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "tcc = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# 标注连续/离散字段\n",
    "# 离散字段\n",
    "category_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "                'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "                'PaymentMethod']\n",
    "\n",
    "# 连续字段\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    " \n",
    "# 标签\n",
    "target = 'Churn'\n",
    "\n",
    "# ID列\n",
    "ID_col = 'customerID'\n",
    "\n",
    "# 验证是否划分能完全\n",
    "assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]\n",
    "\n",
    "# 连续字段转化\n",
    "tcc['TotalCharges']= tcc['TotalCharges'].apply(lambda x: x if x!= ' ' else np.nan).astype(float)\n",
    "tcc['MonthlyCharges'] = tcc['MonthlyCharges'].astype(float)\n",
    "\n",
    "# 缺失值填补\n",
    "tcc['TotalCharges'] = tcc['TotalCharges'].fillna(0)\n",
    "\n",
    "# 标签值手动转化 \n",
    "tcc['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "tcc['Churn'].replace(to_replace='No',  value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541cd098-48b3-4dd3-847f-741febcccb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tcc.drop(columns=[ID_col, target]).copy()\n",
    "labels = tcc['Churn'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483e47a-02be-491c-af8f-0623c42834ba",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时，创建自然编码后的数据集以及经过时序特征衍生的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aae6ded-31c0-47d1-bde5-b54371cc36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train, test = train_test_split(tcc, random_state=22)\n",
    "\n",
    "X_train = train.drop(columns=[ID_col, target]).copy()\n",
    "X_test = test.drop(columns=[ID_col, target]).copy()\n",
    "\n",
    "y_train = train['Churn'].copy()\n",
    "y_test = test['Churn'].copy()\n",
    "\n",
    "X_train_seq = pd.DataFrame()\n",
    "X_test_seq = pd.DataFrame()\n",
    "\n",
    "# 年份衍生\n",
    "X_train_seq['tenure_year'] = ((72 - X_train['tenure']) // 12) + 2014\n",
    "X_test_seq['tenure_year'] = ((72 - X_test['tenure']) // 12) + 2014\n",
    "\n",
    "# 月份衍生\n",
    "X_train_seq['tenure_month'] = (72 - X_train['tenure']) % 12 + 1\n",
    "X_test_seq['tenure_month'] = (72 - X_test['tenure']) % 12 + 1\n",
    "\n",
    "# 季度衍生\n",
    "X_train_seq['tenure_quarter'] = ((X_train_seq['tenure_month']-1) // 3) + 1\n",
    "X_test_seq['tenure_quarter'] = ((X_test_seq['tenure_month']-1) // 3) + 1\n",
    "\n",
    "# 独热编码\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_train_seq)\n",
    "\n",
    "seq_new = list(X_train_seq.columns)\n",
    "\n",
    "# 创建带有列名称的独热编码之后的df\n",
    "X_train_seq = pd.DataFrame(enc.transform(X_train_seq).toarray(), \n",
    "                           columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "X_test_seq = pd.DataFrame(enc.transform(X_test_seq).toarray(), \n",
    "                          columns = cate_colName(enc, seq_new, drop=None))\n",
    "\n",
    "# 调整index\n",
    "X_train_seq.index = X_train.index\n",
    "X_test_seq.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b7c22a-09cb-46ca-86e2-153ba000fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(X_train[category_cols])\n",
    "\n",
    "X_train_OE = pd.DataFrame(ord_enc.transform(X_train[category_cols]), columns=category_cols)\n",
    "X_train_OE.index = X_train.index\n",
    "X_train_OE = pd.concat([X_train_OE, X_train[numeric_cols]], axis=1)\n",
    "\n",
    "X_test_OE = pd.DataFrame(ord_enc.transform(X_test[category_cols]), columns=category_cols)\n",
    "X_test_OE.index = X_test.index\n",
    "X_test_OE = pd.concat([X_test_OE, X_test[numeric_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e3570-c069-4e37-9e22-29b0984b1704",
   "metadata": {},
   "source": [
    "然后是模型融合部分所需的第三方库、准备的数据以及训练好的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "befa9eac-b12e-4063-8830-ad0541c8f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化KFold评估器\n",
    "kf = KFold(n_splits=5, random_state=12, shuffle=True)\n",
    "\n",
    "# 重置训练集和测试集的index\n",
    "X_train_OE = X_train_OE.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "train_part_index_l = []\n",
    "eval_index_l = []\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train_OE, y_train):\n",
    "    train_part_index_l.append(train_part_index)\n",
    "    eval_index_l.append(eval_index)\n",
    "    \n",
    "# 训练集特征\n",
    "X_train1 = X_train_OE.loc[train_part_index_l[0]]\n",
    "X_train2 = X_train_OE.loc[train_part_index_l[1]]\n",
    "X_train3 = X_train_OE.loc[train_part_index_l[2]]\n",
    "X_train4 = X_train_OE.loc[train_part_index_l[3]]\n",
    "X_train5 = X_train_OE.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集特征\n",
    "X_eval1 = X_train_OE.loc[eval_index_l[0]]\n",
    "X_eval2 = X_train_OE.loc[eval_index_l[1]]\n",
    "X_eval3 = X_train_OE.loc[eval_index_l[2]]\n",
    "X_eval4 = X_train_OE.loc[eval_index_l[3]]\n",
    "X_eval5 = X_train_OE.loc[eval_index_l[4]]\n",
    "\n",
    "# 训练集标签\n",
    "y_train1 = y_train.loc[train_part_index_l[0]]\n",
    "y_train2 = y_train.loc[train_part_index_l[1]]\n",
    "y_train3 = y_train.loc[train_part_index_l[2]]\n",
    "y_train4 = y_train.loc[train_part_index_l[3]]\n",
    "y_train5 = y_train.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集标签\n",
    "y_eval1 = y_train.loc[eval_index_l[0]]\n",
    "y_eval2 = y_train.loc[eval_index_l[1]]\n",
    "y_eval3 = y_train.loc[eval_index_l[2]]\n",
    "y_eval4 = y_train.loc[eval_index_l[3]]\n",
    "y_eval5 = y_train.loc[eval_index_l[4]]\n",
    "\n",
    "train_set = [(X_train1, y_train1), \n",
    "             (X_train2, y_train2), \n",
    "             (X_train3, y_train3), \n",
    "             (X_train4, y_train4), \n",
    "             (X_train5, y_train5)]\n",
    "\n",
    "eval_set = [(X_eval1, y_eval1), \n",
    "            (X_eval2, y_eval2), \n",
    "            (X_eval3, y_eval3), \n",
    "            (X_eval4, y_eval4), \n",
    "            (X_eval5, y_eval5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cd868aa-f9b1-4118-ab4d-efe483f56977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型组\n",
    "grid_RF_1 = load('./models/grid_RF_1.joblib') \n",
    "grid_RF_2 = load('./models/grid_RF_2.joblib') \n",
    "grid_RF_3 = load('./models/grid_RF_3.joblib') \n",
    "grid_RF_4 = load('./models/grid_RF_4.joblib') \n",
    "grid_RF_5 = load('./models/grid_RF_5.joblib') \n",
    "\n",
    "RF_1 = grid_RF_1.best_estimator_\n",
    "RF_2 = grid_RF_2.best_estimator_\n",
    "RF_3 = grid_RF_3.best_estimator_\n",
    "RF_4 = grid_RF_4.best_estimator_\n",
    "RF_5 = grid_RF_5.best_estimator_\n",
    "\n",
    "RF_l = [RF_1, RF_2, RF_3, RF_4, RF_5]\n",
    "\n",
    "# 决策树模型组\n",
    "grid_tree_1 = load('./models/grid_tree_1.joblib')\n",
    "grid_tree_2 = load('./models/grid_tree_2.joblib')\n",
    "grid_tree_3 = load('./models/grid_tree_3.joblib')\n",
    "grid_tree_4 = load('./models/grid_tree_4.joblib')\n",
    "grid_tree_5 = load('./models/grid_tree_5.joblib')\n",
    "\n",
    "tree_1 = grid_tree_1.best_estimator_\n",
    "tree_2 = grid_tree_2.best_estimator_\n",
    "tree_3 = grid_tree_3.best_estimator_\n",
    "tree_4 = grid_tree_4.best_estimator_\n",
    "tree_5 = grid_tree_5.best_estimator_\n",
    "\n",
    "tree_l = [tree_1, tree_2, tree_3, tree_4, tree_5]\n",
    "\n",
    "# 逻辑回归模型组\n",
    "grid_lr_1 = load('./models/grid_lr_1.joblib')\n",
    "grid_lr_2 = load('./models/grid_lr_2.joblib')\n",
    "grid_lr_3 = load('./models/grid_lr_3.joblib')\n",
    "grid_lr_4 = load('./models/grid_lr_4.joblib')\n",
    "grid_lr_5 = load('./models/grid_lr_5.joblib')\n",
    "\n",
    "lr_1 = grid_lr_1.best_estimator_\n",
    "lr_2 = grid_lr_2.best_estimator_\n",
    "lr_3 = grid_lr_3.best_estimator_\n",
    "lr_4 = grid_lr_4.best_estimator_\n",
    "lr_5 = grid_lr_5.best_estimator_\n",
    "\n",
    "lr_l = [lr_1, lr_2, lr_3, lr_4, lr_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f9cbe6c-5f06-4d0d-b7c1-e5ba068a1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1_predict_proba_RF = pd.Series(RF_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_RF = pd.Series(RF_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_RF = pd.Series(RF_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_RF = pd.Series(RF_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_RF = pd.Series(RF_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_RF = pd.concat([eval1_predict_proba_RF, \n",
    "                                   eval2_predict_proba_RF, \n",
    "                                   eval3_predict_proba_RF, \n",
    "                                   eval4_predict_proba_RF, \n",
    "                                   eval5_predict_proba_RF]).sort_index()\n",
    "\n",
    "eval1_predict_proba_tree = pd.Series(tree_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_tree = pd.Series(tree_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_tree = pd.Series(tree_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_tree = pd.Series(tree_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_tree = pd.Series(tree_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_tree = pd.concat([eval1_predict_proba_tree, \n",
    "                                     eval2_predict_proba_tree, \n",
    "                                     eval3_predict_proba_tree, \n",
    "                                     eval4_predict_proba_tree, \n",
    "                                     eval5_predict_proba_tree]).sort_index()\n",
    "\n",
    "eval1_predict_proba_lr = pd.Series(lr_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index)\n",
    "eval2_predict_proba_lr = pd.Series(lr_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index)\n",
    "eval3_predict_proba_lr = pd.Series(lr_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index)\n",
    "eval4_predict_proba_lr = pd.Series(lr_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index)\n",
    "eval5_predict_proba_lr = pd.Series(lr_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index)\n",
    "\n",
    "eval_predict_proba_lr = pd.concat([eval1_predict_proba_lr, \n",
    "                                   eval2_predict_proba_lr, \n",
    "                                   eval3_predict_proba_lr, \n",
    "                                   eval4_predict_proba_lr, \n",
    "                                   eval5_predict_proba_lr]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "985dad6f-5cdc-44be-8018-7977bab015e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_proba_RF = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF.append(RF_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_RF = np.array(test_predict_proba_RF)\n",
    "test_predict_proba_RF = test_predict_proba_RF.mean(0)\n",
    "\n",
    "test_predict_proba_tree = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree.append(tree_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_tree = np.array(test_predict_proba_tree)\n",
    "test_predict_proba_tree = test_predict_proba_tree.mean(0)\n",
    "\n",
    "test_predict_proba_lr = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr.append(lr_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_lr = np.array(test_predict_proba_lr)\n",
    "test_predict_proba_lr = test_predict_proba_lr.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc2161-13dd-4a17-b8e7-3bfe635243d4",
   "metadata": {},
   "source": [
    "## <center>Ch.3 模型融合基础方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e690c0-6978-40b4-8def-d3f6c8bd1e03",
   "metadata": {},
   "source": [
    "## 十一、Blending融合进阶优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03889a-615e-4a88-8bb8-c19cf423c12c",
   "metadata": {},
   "source": [
    "### 1.基本优化思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc88556-3f7b-4b64-a30f-7c6f5cd1e78e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在上一小节，我们介绍Blending融合的手动执行方法及借助manual_ensemble函数库快速实现方法。接下来，我们进一步介绍Blending融合的进阶优化策略。相比Stacking融合，Blending模型融合的核心区别就在于留出集的划分，以及由此导致的一级学习器和元学习器之间的训练数据隔离。而如何围绕留出集划分策略来进行优化，也成了Blending融合优化的最核心的突破口。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625df9b-516c-41b9-9d6f-8e43169d3167",
   "metadata": {},
   "source": [
    "- 方案一：搜索最佳留出集划分比例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13f6a4-6559-418f-a811-272ea83648fc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据上一小节的介绍，我们知道了留出集“成于斯者毁于斯”：尽管留出集的划分能带来数据隔离从而提升融合结果的泛化能力，但留出集比例过大或者过小都会影响Blending融合效果：留出集比例越大、一级学习器越弱、元学习器越强；而如果留出集比例较小，则一级学习器较强、但元学习器过拟合风险会很大（极端情况就是留出集为0的情况，融合流程由Blending退化为Stacking）。因此，最容易想到的Blending融合的优化策略就是寻找到一个比较适中的留出集划分比例，尽可能平衡一级学习器和元学习器之间学习能力互斥的关系，从而提升最终Blending融合效果。这个也是Blending融合优化的第一种思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab1424-1e42-4fa2-8dbb-29055d4680bd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过，要将这个思路落地成具体可执行的方案却并不简单，其困难之处并不在于代码层面难以实现，而是算力不足条件约束。如何找到合适的留出集的划分比例，对机器学习这类后验的技术来说，免不了需要海量的尝试，典型的方案就是将留出集划分比例视作超参数，带入优化器来搜索出一个可靠的结果。但是，通过上一小节我们发现，一个Blending的过程动辄需要耗费半小时乃至数个小时，“海量的尝试”对于个人用户来说基本是个不可能实现的过程，哪怕是用相对较少尝试来估计最佳划分比例的贝叶斯优化，对于50%-90%这个区间的搜索任务来说，至少也需要100-500次的计算。因此，若要实现最佳划分比例搜索策略，就需要尽可能缩短单次Blending融合所需要的时间，例如可以考虑一级学习器在交叉训练过程中不再进行单独模型的超参数优化，此举尽管会降低单次Blending融合精度，但通过缩短单次Blending融合时间，前期可以帮助最外层优化器快速搜索得到一个最佳划分比例，然后再确定比例之后再训练一个效果更好Blending融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd4734-8382-4061-9e76-a42274654f28",
   "metadata": {},
   "source": [
    "- 方案二：多次划分，构建基于Blending结果的（加权）平均融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8530ca-0cea-4256-8106-bbd0a4b94c0d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，除此之外，根据长期模型融合的经验，其实早就帮助我们奠定了对待差异性结果的另外一种不同观点，那就是：不同留出集比例造成的结果差异性，或许本身也是通往更好结果的阶梯。例如，我们其实也可以通过设置多组不同比例留出集数据、来训练多个不同的Blending融合过程，然后让这些训练好的Blending融合流程对相同的测试集进行预测，并最终围绕这些预测结果来进行（加权）平均融合，如此，就相当于是执行了多层的模型融合。例如我们可以设置5：5-9：1五组不同留出集划分的数据集、训练5个Blending融合流程、再对其结果进行（加权）平均融合，其基本流程如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a3034-7e3a-46d8-8ada-1b1ebcee282f",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221019190652003.png\" alt=\"image-20221019190652003\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acceec-14da-41a8-822e-cda06939140a",
   "metadata": {},
   "source": [
    "相比方案一，第二个方案其实会更加省时省力，并且往往也能得到一个还不错的结果。从理论上来说，强而不同是保障（加权）平均融合效果之根本，在上述流程中，Blending融合结果是“强”的保证，而不同留出集比例的划分，又将严重影响Blending融合结果，因此也保障了“不同”，这也就是该方案具备可执行性的理论基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40be97-6e56-4406-804e-d981f9a12777",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其实投票法或者平均法也是可以看成是两层的模型在进行计算，第一层模型是一级学习器，第二层的计算过程其实就是简单的投票或者平均计算过程，为了方便解释，此后我们统一称呼投票法&均值法的基础学习器为一级学习器。基于此，上述过程其实就是一个一级学习器是Blending的（加权）平均融合过程。此时，由于只有5个模型参与融合，可以尽可能训练得到更优结果；而对于第二层的（加权）平均融合过程，既可以尝试简单的平均融合，也可以尝试手动设置权重的加权融合方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82378473-1769-41dd-b349-61f51b2f7119",
   "metadata": {},
   "source": [
    "> 再复杂的机器学习算法，本质上也是一个计算过程，和求平均这一计算过程无疑。而算法的本质、其基本定义，也就是一个计算过程，因此求均值这一计算过程就是一个算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286827e6-2b7a-4933-8dbe-2c85f2edb3d5",
   "metadata": {},
   "source": [
    "- 优化效果预估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356771b2-a097-4050-ab38-0b84536c8d1c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其实无论哪种方案，毫无疑问都是一整套更加复杂的融合流程。而正如Stacking融合开篇说的那样，越是复杂的流程越容易过拟合，本小节介绍的优化策略也不例外，伴随着融合流程变得更加复杂，融合结果的过拟合倾向也会更加明显。不过，在真实的实践场景中，复杂融合过程的过拟合倾向其实也是和数据本身息息相关，一般来说数据越简单（样本数量越少、特征越少），复杂融合过程的过拟合倾向就越高，而如果应对的是更加复杂的数据集，则复杂融合过程背后的强学习能力，往往能够提升最后的预测结果。因此，鉴于当前数据集较为简单的数据情况，这些模型融合优化策略大概率将出现过拟合的问题，但在后续更加复杂的数据集上，这些方法将起到非常核心的“提分”的效果。因此，本小节重点介绍各方法背后的理论依据及实现过程，并不会侧重预测结果的对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063cb96-78e6-42e0-9fab-38ab61e3b47a",
   "metadata": {},
   "source": [
    "> 更多方法带来更多样的结果、带来更多的可能性，这也是模型融合阶段最核心的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81c8af-29d9-4a47-aacc-93beb1f542c0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们尝试实现这两种不同优化策略，并测试最终能否提升融合效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41714af-ed80-418a-9981-b321867b94a1",
   "metadata": {},
   "source": [
    "### 2.方案一：搜索最佳划分比例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f820ae5-bd5b-40de-860b-5cf2da8c0571",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先尝试实现第一种方案，即通过贝叶斯优化器，搜索最佳留出集划分比例。为实现此方案，按照此前说明，我们需要先创建一个计算耗时更少的Blending融合流程，然后再将其封装为一个目标函数，用于搜索最佳留出集划分比例，然后据此再来进行最后一次高精度的Blending融合，以最终达到优化预测结果的目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efa0b7-a38c-4c07-b321-e24a12653b86",
   "metadata": {},
   "source": [
    "- 简化Blending融合过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a52038-774e-4d58-9413-0bed81e95fcb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们首先创建一个更高效快捷的Blending融合过程，这里我们直接采用此前在全部数据集上搜索得到最优超参数的三个模型作为一级学习器，并免去一级学习器超参数搜索的交叉训练，简单固定超参数进行交叉训练即可；此外，精简元学习器的优化流程，直接带入逻辑回归作为元学习器，由此可构建更快速Blending融合过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a739dc3e-ae1d-422e-9189-338abe3adbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = load('./models/tree_model.joblib')\n",
    "RF = load('./models/RF_0.joblib')\n",
    "logistic_search = load('./models/logistic_search.joblib')\n",
    "\n",
    "lr = logistic_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c80749c-dc27-49c0-b5a3-07f883fbc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('lr', lr), ('tree', tree), ('RF', RF)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ed360c9-aae3-4d53-ad76-98f9e059a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.351806879043579\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                        y_train, \n",
    "                                                        X_test_OE,\n",
    "                                                        estimators, \n",
    "                                                        blending=True)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85a3fd22-f838-43e4-ba33-df6c0f1b8d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train2-Accuracy: 0.822138, Test-Accuracy: 0.788756\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "print('The results of LR-final:')\n",
    "print('Train2-Accuracy: %f, Test-Accuracy: %f' % (lr.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]), lr.score(test_predict_blending, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3a9a9-f809-4f59-9cae-22a21b848003",
   "metadata": {},
   "source": [
    "能够发现，单次计算仅需要1s左右。在默认的训练集：留出集=8：2的情况下，最终准确率为0.788。此外，我们可以观察此时训练和预测数据如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c56394f1-4523-4568-a1c7-61d889447e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_oof</th>\n",
       "      <th>tree_oof</th>\n",
       "      <th>RF_oof</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067367</td>\n",
       "      <td>0.063791</td>\n",
       "      <td>0.024685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.470657</td>\n",
       "      <td>0.388356</td>\n",
       "      <td>0.350602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147403</td>\n",
       "      <td>0.131717</td>\n",
       "      <td>0.173163</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.712578</td>\n",
       "      <td>0.779534</td>\n",
       "      <td>0.721254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.427978</td>\n",
       "      <td>0.484931</td>\n",
       "      <td>0.421293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.023757</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.048487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.597838</td>\n",
       "      <td>0.552773</td>\n",
       "      <td>0.559185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.012680</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.029577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.014272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.056799</td>\n",
       "      <td>0.063791</td>\n",
       "      <td>0.103838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lr_oof  tree_oof    RF_oof  Churn\n",
       "0     0.067367  0.063791  0.024685      0\n",
       "1     0.470657  0.388356  0.350602      1\n",
       "2     0.147403  0.131717  0.173163      0\n",
       "3     0.712578  0.779534  0.721254      1\n",
       "4     0.427978  0.484931  0.421293      0\n",
       "...        ...       ...       ...    ...\n",
       "1052  0.023757  0.044198  0.048487      0\n",
       "1053  0.597838  0.552773  0.559185      0\n",
       "1054  0.012680  0.044198  0.029577      0\n",
       "1055  0.004052  0.044198  0.014272      0\n",
       "1056  0.056799  0.063791  0.103838      0\n",
       "\n",
       "[1057 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof_blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ed511a6-58a1-4e90-8b4a-ad955f45ca67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_predict</th>\n",
       "      <th>tree_predict</th>\n",
       "      <th>RF_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031213</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.004815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.235520</td>\n",
       "      <td>0.131717</td>\n",
       "      <td>0.339551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.002016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026678</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.009825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061598</td>\n",
       "      <td>0.063791</td>\n",
       "      <td>0.043481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>0.158728</td>\n",
       "      <td>0.211204</td>\n",
       "      <td>0.179236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>0.026341</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.068920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>0.148644</td>\n",
       "      <td>0.131717</td>\n",
       "      <td>0.167651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0.493872</td>\n",
       "      <td>0.473148</td>\n",
       "      <td>0.565447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0.075108</td>\n",
       "      <td>0.131717</td>\n",
       "      <td>0.106657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr_predict  tree_predict  RF_predict\n",
       "0       0.031213      0.044198    0.004815\n",
       "1       0.235520      0.131717    0.339551\n",
       "2       0.004348      0.044198    0.002016\n",
       "3       0.026678      0.044198    0.009825\n",
       "4       0.061598      0.063791    0.043481\n",
       "...          ...           ...         ...\n",
       "1756    0.158728      0.211204    0.179236\n",
       "1757    0.026341      0.044198    0.068920\n",
       "1758    0.148644      0.131717    0.167651\n",
       "1759    0.493872      0.473148    0.565447\n",
       "1760    0.075108      0.131717    0.106657\n",
       "\n",
       "[1761 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637e842-c47e-482c-9d6a-b104ce5604e2",
   "metadata": {},
   "source": [
    "> 从理论上来说，借助完整数据集上训练的模型来进行留出集划分比例的验证，可能会存在留出集信息提前泄露的问题。不过由于后续将会多次反复划分留出集，而同时又不太可能重复多次训练模型，因此留出集信息泄露问题不可避免；此外，全训练数据集上的最优单模训练过程也是一定会在模型融合之前执行的，因此在实际建模过程中，上述Blending流程会非常容易实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2591d-ff58-4053-82a0-13e7ce81c957",
   "metadata": {},
   "source": [
    "- 搜索最佳留出集划分比例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da2e68-f053-48ff-b4a2-9fb4b78f48a9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，即可创建TPE搜索流程搜索最佳留出集划分比例，首先是搜索空间和目标函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a84f8d46-1729-4d1a-8c84-47729393bd80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_cross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mblending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Stacking融合过程一级学习器交叉训练函数\n",
       "\n",
       ":param X_train: 训练集特征\n",
       ":param y_train: 训练集标签\n",
       ":param X_test: 测试集特征\n",
       ":param estimators: 一级学习器，由(名称,评估器)组成的列表\n",
       ":param n_splits: 交叉训练折数\n",
       ":param test_size: blending过程留出集占比\n",
       ":param random_state: 随机数种子\n",
       ":param blending: 是否进行blending融合\n",
       "\n",
       ":return：交叉训练后创建oof训练数据和测试集平均预测结果，同时包含特征和标签，标签在最后一列\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\work\\jupyter\\telco\\正式课程\\manual_ensemble.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cross?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eddba4c7-e503-4624-af03-d6a0bbedb865",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_space = {'test_size': hp.uniform('test_size', 0.1, 0.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e258856b-e63c-4dd5-ad97-0583997ee1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_res(params, train=True):\n",
    "    test_size = params['test_size']\n",
    "    train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                            y_train, \n",
    "                                                            X_test_OE,\n",
    "                                                            estimators, \n",
    "                                                            blending=True, \n",
    "                                                            test_size=test_size)\n",
    "    lr = LogisticRegression().fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "    if train == True:\n",
    "        res = -lr.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "    else:\n",
    "        res = (train_oof_blending, test_predict_blending)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce111304-2755-4b8d-ab8e-1a81ab86f224",
   "metadata": {},
   "source": [
    "和此前类似，我们在创建目标函数的时候分别设置了训练模式和测试模式，训练模式最终输出元学习器在留出集上的预测准确率，而测试模式下最终输出在给定某个划分比例时oof训练数据集和测试集预测结果。当然，这里也可以将测试模式的输出结果设置为留出集和测试集上最终预测概率结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cf8c3-b499-42a9-8a6b-b5798e5877b2",
   "metadata": {},
   "source": [
    "最后是优化函数定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1fcd67e7-0441-4dfd-8ed5-a8b2893c1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_split_res(max_evals):\n",
    "    params_best = fmin(fn = split_res,\n",
    "                       space = split_space,\n",
    "                       algo = tpe.suggest,\n",
    "                       max_evals = max_evals, \n",
    "                       rstate=np.random.RandomState(11))    \n",
    "    \n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8d22b-3d18-438f-b003-4de944083451",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，测试优化能否顺利运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0413897b-b7a8-4168-92ab-5aba014f0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [02:14<00:00,  1.34s/trial, best loss: -0.8321167883211679]\n"
     ]
    }
   ],
   "source": [
    "best_split = param_split_res(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d166b01a-3e42-41f0-9c75-29d029fea4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_size': 0.1295881342481578}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690f613-8ee2-45d7-8697-370e0f53d1ea",
   "metadata": {},
   "source": [
    "> 这里若对搜索结果的精度进行限制，则可以进一步提升搜索效率。考虑到数据集本身数量有限，确实不用如此高精度的搜索结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40c49b-ad09-4650-a284-0ca799e58836",
   "metadata": {},
   "source": [
    "最终搜索得到的留出集划分最佳比例为13%。接下来测试Blending融合效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04c40ef6-d15c-448f-a5ab-5b9e0d57b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof_blending, test_predict_blending = split_res(best_split,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d75d53f-cfae-4957-a8e6-9cedff7973d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_oof</th>\n",
       "      <th>tree_oof</th>\n",
       "      <th>RF_oof</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074883</td>\n",
       "      <td>0.069631</td>\n",
       "      <td>0.026061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.475950</td>\n",
       "      <td>0.409022</td>\n",
       "      <td>0.347158</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.137676</td>\n",
       "      <td>0.090934</td>\n",
       "      <td>0.193092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.713064</td>\n",
       "      <td>0.763057</td>\n",
       "      <td>0.715137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.415286</td>\n",
       "      <td>0.560982</td>\n",
       "      <td>0.385007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.763057</td>\n",
       "      <td>0.743935</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.005990</td>\n",
       "      <td>0.050797</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>0.388037</td>\n",
       "      <td>0.294272</td>\n",
       "      <td>0.307472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0.656585</td>\n",
       "      <td>0.235635</td>\n",
       "      <td>0.612801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0.485777</td>\n",
       "      <td>0.763057</td>\n",
       "      <td>0.545376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>685 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr_oof  tree_oof    RF_oof  Churn\n",
       "0    0.074883  0.069631  0.026061      0\n",
       "1    0.475950  0.409022  0.347158      1\n",
       "2    0.137676  0.090934  0.193092      0\n",
       "3    0.713064  0.763057  0.715137      1\n",
       "4    0.415286  0.560982  0.385007      0\n",
       "..        ...       ...       ...    ...\n",
       "680  0.607143  0.763057  0.743935      1\n",
       "681  0.005990  0.050797  0.008541      0\n",
       "682  0.388037  0.294272  0.307472      0\n",
       "683  0.656585  0.235635  0.612801      1\n",
       "684  0.485777  0.763057  0.545376      1\n",
       "\n",
       "[685 rows x 4 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof_blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "191938ec-f9be-41c2-b741-ba5ca0837f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train-oof-Accuracy: 0.832117, Test-Accuracy: 0.789324\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "print('The results of LR-final:')\n",
    "print('Train-oof-Accuracy: %f, Test-Accuracy: %f' % (lr.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]), lr.score(test_predict_blending, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc58051-a83e-436b-8591-c11486eafea9",
   "metadata": {},
   "source": [
    "能够发现，通过调整留出集比例，最终融合结果略有提升，这也说明通过合理调配一级学习器和元学习器的学习能力，将有助于Blending融合效果提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288835f-b792-45e3-badf-1c7a39a58b26",
   "metadata": {},
   "source": [
    "- 更高精度的Blending融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bcda7d-2db3-48ce-8fdf-5beaabdbd84e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在得到最佳划分比例后，接下来，我们更换一个更高精度的Blending融合过程，测试效果是否会进一步提升："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d821897f-a961-4641-a67c-0e04210a8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一级学习器\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f9c5fc9-78ea-4d9b-a423-dfaf6949da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 20/20 [02:05<00:00,  6.28s/trial, best loss: -0.7916762792073351]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:08<00:00,  6.44s/trial, best loss: -0.7837928867199053]\n",
      "100%|████████████████████████████████████████████████| 20/20 [02:09<00:00,  6.49s/trial, best loss: -0.788195800059154]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:09<00:00,  6.45s/trial, best loss: -0.7936372375036971]\n",
      "100%|███████████████████████████████████████████████| 20/20 [02:07<00:00,  6.37s/trial, best loss: -0.7960854776693285]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:40<00:00, 24.44trial/s, best loss: -0.8006555013309671]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:40<00:00, 24.76trial/s, best loss: -0.7965746081041111]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:40<00:00, 24.77trial/s, best loss: -0.7944531943212068]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:39<00:00, 25.06trial/s, best loss: -0.7955412599822538]\n",
      "100%|███████████████████████████████████████████| 1000/1000 [00:40<00:00, 24.78trial/s, best loss: -0.8020670659568175]\n",
      "100%|█████████████████████████████████████████████| 500/500 [06:05<00:00,  1.37trial/s, best loss: -0.8118053090801538]\n",
      "100%|██████████████████████████████████████████████| 500/500 [05:55<00:00,  1.40trial/s, best loss: -0.806637089618456]\n",
      "100%|█████████████████████████████████████████████| 500/500 [05:54<00:00,  1.41trial/s, best loss: -0.8031551316178647]\n",
      "100%|█████████████████████████████████████████████| 500/500 [06:07<00:00,  1.36trial/s, best loss: -0.8091363501922508]\n",
      "100%|█████████████████████████████████████████████| 500/500 [05:45<00:00,  1.45trial/s, best loss: -0.8118581780538303]\n"
     ]
    }
   ],
   "source": [
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                        y_train, \n",
    "                                                        X_test_OE, \n",
    "                                                        estimators=estimators, \n",
    "                                                        test_size=0.12958,\n",
    "                                                        blending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d521d-fae5-4d42-8469-d99a20000822",
   "metadata": {},
   "source": [
    "当然，接下来我们就可以将其带入元学习器优化函数，测试最终的优化效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "58f59d53-b490-4278-ba73-865704ff3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义元学习器搜索空间\n",
    "lr_final_param = [{'thr': np.arange(0.1, 1.1, 0.1).tolist(), 'penalty': ['l1'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['saga']}, \n",
    "                  {'thr': np.arange(0.1, 1.1, 0.1).tolist(), 'penalty': ['l2'], 'C': np.arange(0.1, 1.1, 0.1).tolist(), 'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']}]\n",
    "\n",
    "tree_final_param = {'max_depth': np.arange(2, 16, 1).tolist(), \n",
    "                    'min_samples_split': np.arange(1, 5, 1).tolist(), \n",
    "                    'min_samples_leaf': np.arange(1, 4, 1).tolist(), \n",
    "                    'max_leaf_nodes':np.arange(6, 30, 1).tolist()}\n",
    "\n",
    "param_space_l = [lr_final_param, tree_final_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "df49711c-29d5-4163-965b-054bda69e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义元学习器列表\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "859cb76d-c5bc-480a-960b-2ffd593cff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行元学习器训练搜索\n",
    "best_res_final, best_test_predict_final = final_model_opt(final_model_l, \n",
    "                                                          param_space_l, \n",
    "                                                          train_oof_blending.iloc[:, :-1], \n",
    "                                                          train_oof_blending.iloc[:, -1], \n",
    "                                                          test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3444602-32a4-4a07-b271-fe044b83d105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8335766423357664"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_res_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53b1ec91-846f-4ce6-b8d2-638eb9b5dc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.787052810902896"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score((best_test_predict_final >= 0.5) * 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa67dc-e9e4-47d7-89e0-cc7532f58a66",
   "metadata": {},
   "source": [
    "能够发现，在执行了一个更强的Blending融合后，训练集准确率有所提升，但测试集准确率下降，说明最终融合结果出现了一定过拟合倾向。当然，由此也说明，对于简单的数据集，并非一定要使用学习能力最强的融合流程。在融合阶段稍微“保留些余地”，或许将更有助于最终模型结果的提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf71de5-56c1-4ab4-b2d3-ce4a982b07ff",
   "metadata": {},
   "source": [
    "### 3.方案二：多次划分，构建基于Blending结果的（加权）平均融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8ae8d-2d8f-4a65-b276-f3d2050390cb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们尝试第二种优化策略，即通过多次划分不同比例的留出集来训练多个Blending融合结果，然后再进行平均融合或加权平均融合。在已经定义了诸多辅助函数的情况下，该过程的代码实现流程并不复杂，唯一需要考虑的问题就是计算时间。在此前的Blending融合过程中，完整执行一级学习器交叉搜索训练+元学习器优化，在一级学习器搜索次数较少的情况下估计用时45min，此时要完整训练5个Blending融合结果，则至少需要3个半小时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5c870291-53d4-4a45-a0b5-9d722ead9101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.75"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(45 * 5) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4520f5-bf53-4bd7-9599-8c73561a9741",
   "metadata": {},
   "source": [
    "回顾完整计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a75fc-3be7-4dff-9f69-149d1e8d1b02",
   "metadata": {},
   "source": [
    "<center><img src=\"http://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20221019190652003.png\" alt=\"image-20221019190652003\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efae467-10b1-4f5a-bcc7-5e57f29ef5d2",
   "metadata": {},
   "source": [
    "- 一阶段Blending融合过程如下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cb054-e0a7-4eb1-b0f5-08b2fd39abf2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是5个不同比例留出集的Blending过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134baf86-c0c6-4249-ba3b-90b376228559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 20/20 [01:46<00:00,  5.33s/trial, best loss: -0.7935282522996058]\n",
      "100%|███████████████████████████████| 20/20 [01:43<00:00,  5.17s/trial, best loss: -0.7856397399543538]\n",
      "100%|███████████████████████████████| 20/20 [01:40<00:00,  5.03s/trial, best loss: -0.7890580261428868]\n",
      "100%|███████████████████████████████| 20/20 [01:39<00:00,  4.96s/trial, best loss: -0.7972643336330314]\n",
      "100%|███████████████████████████████| 20/20 [01:31<00:00,  4.59s/trial, best loss: -0.7909509647970122]\n",
      "100%|███████████████████████████| 1000/1000 [00:30<00:00, 32.62trial/s, best loss: -0.7956362819005464]\n",
      "100%|███████████████████████████| 1000/1000 [00:30<00:00, 32.91trial/s, best loss: -0.7861650183276853]\n",
      "100%|███████████████████████████| 1000/1000 [00:30<00:00, 32.58trial/s, best loss: -0.7887986721073379]\n",
      "100%|███████████████████████████| 1000/1000 [00:30<00:00, 32.38trial/s, best loss: -0.7999000622449685]\n",
      "100%|███████████████████████████| 1000/1000 [00:30<00:00, 32.84trial/s, best loss: -0.7993720174285911]\n",
      "100%|█████████████████████████████| 500/500 [04:27<00:00,  1.87trial/s, best loss: -0.8143097724600595]\n",
      "100%|█████████████████████████████| 500/500 [04:39<00:00,  1.79trial/s, best loss: -0.8043132305138668]\n",
      "100%|█████████████████████████████| 500/500 [05:13<00:00,  1.60trial/s, best loss: -0.8011591396362128]\n",
      "100%|█████████████████████████████| 500/500 [04:48<00:00,  1.73trial/s, best loss: -0.8093654471263573]\n",
      "100%|█████████████████████████████| 500/500 [05:07<00:00,  1.62trial/s, best loss: -0.8098896880835467]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                        y_train, \n",
    "                                                        X_test_OE, \n",
    "                                                        estimators=estimators, \n",
    "                                                        test_size=0.1,\n",
    "                                                        blending=True)\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final1, best_test_predict_final1 = final_model_opt(final_model_l, \n",
    "                                                            param_space_l, \n",
    "                                                            train_oof_blending.iloc[:, :-1], \n",
    "                                                            train_oof_blending.iloc[:, -1], \n",
    "                                                            test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf68e4e-73d5-460c-965a-69dc0fec6c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 20/20 [01:38<00:00,  4.95s/trial, best loss: -0.7855029585798816]\n",
      "100%|███████████████████████████████| 20/20 [01:37<00:00,  4.88s/trial, best loss: -0.7884615384615385]\n",
      "100%|███████████████████████████████| 20/20 [01:36<00:00,  4.84s/trial, best loss: -0.7834319526627219]\n",
      "100%|███████████████████████████████| 20/20 [01:42<00:00,  5.15s/trial, best loss: -0.7931952662721894]\n",
      "100%|███████████████████████████████| 20/20 [01:37<00:00,  4.88s/trial, best loss: -0.7884615384615384]\n",
      "100%|███████████████████████████| 1000/1000 [00:28<00:00, 35.20trial/s, best loss: -0.7973372781065089]\n",
      "100%|███████████████████████████| 1000/1000 [00:28<00:00, 34.94trial/s, best loss: -0.8041420118343193]\n",
      "100%|███████████████████████████| 1000/1000 [00:28<00:00, 34.54trial/s, best loss: -0.7899408284023668]\n",
      "100%|███████████████████████████| 1000/1000 [00:29<00:00, 34.33trial/s, best loss: -0.8023668639053254]\n",
      "100%|███████████████████████████| 1000/1000 [00:28<00:00, 34.92trial/s, best loss: -0.7911242603550297]\n",
      "100%|█████████████████████████████| 500/500 [04:41<00:00,  1.78trial/s, best loss: -0.8053254437869823]\n",
      "100%|█████████████████████████████| 500/500 [04:53<00:00,  1.70trial/s, best loss: -0.8085798816568047]\n",
      "100%|█████████████████████████████| 500/500 [04:27<00:00,  1.87trial/s, best loss: -0.8029585798816568]\n",
      "100%|█████████████████████████████| 500/500 [04:52<00:00,  1.71trial/s, best loss: -0.8115384615384615]\n",
      "100%|█████████████████████████████| 500/500 [04:16<00:00,  1.95trial/s, best loss: -0.8044378698224852]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                        y_train, \n",
    "                                                        X_test_OE, \n",
    "                                                        estimators=estimators, \n",
    "                                                        test_size=0.2,\n",
    "                                                        blending=True)\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final2, best_test_predict_final2 = final_model_opt(final_model_l, \n",
    "                                                            param_space_l, \n",
    "                                                            train_oof_blending.iloc[:, :-1], \n",
    "                                                            train_oof_blending.iloc[:, -1], \n",
    "                                                            test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd7a26f-c541-4572-9113-2fbafce9aee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 20/20 [01:26<00:00,  4.33s/trial, best loss: -0.7893132345543513]\n",
      "100%|███████████████████████████████| 20/20 [01:34<00:00,  4.73s/trial, best loss: -0.7916832441578635]\n",
      "100%|███████████████████████████████| 20/20 [01:26<00:00,  4.30s/trial, best loss: -0.7789077148214204]\n",
      "100%|███████████████████████████████| 20/20 [01:23<00:00,  4.18s/trial, best loss: -0.7802550647093793]\n",
      "100%|███████████████████████████████| 20/20 [01:29<00:00,  4.47s/trial, best loss: -0.7890451365070655]\n",
      "100%|███████████████████████████| 1000/1000 [00:26<00:00, 37.87trial/s, best loss: -0.8052179082635936]\n",
      "100%|███████████████████████████| 1000/1000 [00:26<00:00, 37.42trial/s, best loss: -0.7994632322678008]\n",
      "100%|███████████████████████████| 1000/1000 [00:26<00:00, 37.16trial/s, best loss: -0.8015559976219876]\n",
      "100%|███████████████████████████| 1000/1000 [00:27<00:00, 36.83trial/s, best loss: -0.7866825581927104]\n",
      "100%|███████████████████████████| 1000/1000 [00:26<00:00, 37.12trial/s, best loss: -0.8123713815338182]\n",
      "100%|█████████████████████████████| 500/500 [03:58<00:00,  2.10trial/s, best loss: -0.8062268486760873]\n",
      "100%|█████████████████████████████| 500/500 [04:01<00:00,  2.07trial/s, best loss: -0.8072449352906205]\n",
      "100%|█████████████████████████████| 500/500 [04:28<00:00,  1.86trial/s, best loss: -0.8076462249051083]\n",
      "100%|█████████████████████████████| 500/500 [04:52<00:00,  1.71trial/s, best loss: -0.8002063611835185]\n",
      "100%|█████████████████████████████| 500/500 [04:42<00:00,  1.77trial/s, best loss: -0.8113578680203044]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                        y_train, \n",
    "                                                        X_test_OE, \n",
    "                                                        estimators=estimators, \n",
    "                                                        test_size=0.3,\n",
    "                                                        blending=True)\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final3, best_test_predict_final3 = final_model_opt(final_model_l, \n",
    "                                                            param_space_l, \n",
    "                                                            train_oof_blending.iloc[:, :-1], \n",
    "                                                            train_oof_blending.iloc[:, -1], \n",
    "                                                            test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e9c4549-5712-4657-966a-f4b115b09ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 20/20 [01:21<00:00,  4.09s/trial, best loss: -0.7838264299802762]\n",
      "100%|███████████████████████████████| 20/20 [01:21<00:00,  4.09s/trial, best loss: -0.7838264299802762]\n",
      "100%|████████████████████████████████| 20/20 [01:23<00:00,  4.16s/trial, best loss: -0.790138067061144]\n",
      "100%|███████████████████████████████| 20/20 [01:21<00:00,  4.08s/trial, best loss: -0.7865877712031557]\n",
      "100%|███████████████████████████████| 20/20 [01:26<00:00,  4.31s/trial, best loss: -0.7858780226436193]\n",
      "100%|███████████████████████████| 1000/1000 [00:25<00:00, 39.70trial/s, best loss: -0.7897435897435898]\n",
      "100%|███████████████████████████| 1000/1000 [00:25<00:00, 39.31trial/s, best loss: -0.7960552268244576]\n",
      "100%|███████████████████████████| 1000/1000 [00:25<00:00, 39.90trial/s, best loss: -0.7917159763313609]\n",
      "100%|███████████████████████████| 1000/1000 [00:25<00:00, 39.84trial/s, best loss: -0.7956607495069032]\n",
      "100%|███████████████████████████| 1000/1000 [00:25<00:00, 39.72trial/s, best loss: -0.7969264936557486]\n",
      "100%|█████████████████████████████| 500/500 [03:54<00:00,  2.13trial/s, best loss: -0.8035502958579881]\n",
      "100%|█████████████████████████████| 500/500 [04:13<00:00,  1.98trial/s, best loss: -0.8078895463510847]\n",
      "100%|█████████████████████████████| 500/500 [04:02<00:00,  2.06trial/s, best loss: -0.8039447731755424]\n",
      "100%|█████████████████████████████| 500/500 [03:53<00:00,  2.14trial/s, best loss: -0.8047337278106509]\n",
      "100%|██████████████████████████████| 500/500 [03:57<00:00,  2.11trial/s, best loss: -0.807173585550327]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                        y_train, \n",
    "                                                        X_test_OE, \n",
    "                                                        estimators=estimators, \n",
    "                                                        test_size=0.4,\n",
    "                                                        blending=True)\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final4, best_test_predict_final4 = final_model_opt(final_model_l, \n",
    "                                                            param_space_l, \n",
    "                                                            train_oof_blending.iloc[:, :-1], \n",
    "                                                            train_oof_blending.iloc[:, -1], \n",
    "                                                            test_predict_blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f849386-e0d2-4a3c-9d5b-59d008dea0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 20/20 [01:17<00:00,  3.89s/trial, best loss: -0.7940349343999642]\n",
      "100%|███████████████████████████████| 20/20 [01:06<00:00,  3.34s/trial, best loss: -0.7823154403773543]\n",
      "100%|███████████████████████████████| 20/20 [01:09<00:00,  3.48s/trial, best loss: -0.7775772242949818]\n",
      "100%|███████████████████████████████| 20/20 [01:09<00:00,  3.46s/trial, best loss: -0.7889393073622175]\n",
      "100%|███████████████████████████████| 20/20 [01:14<00:00,  3.71s/trial, best loss: -0.7913033735560709]\n",
      "100%|███████████████████████████| 1000/1000 [00:23<00:00, 42.29trial/s, best loss: -0.7945178313334006]\n",
      "100%|███████████████████████████| 1000/1000 [00:23<00:00, 42.56trial/s, best loss: -0.7922523612651675]\n",
      "100%|███████████████████████████| 1000/1000 [00:23<00:00, 42.48trial/s, best loss: -0.7908227174436714]\n",
      "100%|███████████████████████████| 1000/1000 [00:23<00:00, 42.49trial/s, best loss: -0.8078608001971922]\n",
      "100%|███████████████████████████| 1000/1000 [00:23<00:00, 42.97trial/s, best loss: -0.7917649826896576]\n",
      "100%|█████████████████████████████| 500/500 [03:50<00:00,  2.17trial/s, best loss: -0.8158213169305233]\n",
      "100%|█████████████████████████████| 500/500 [04:03<00:00,  2.06trial/s, best loss: -0.8125978958690464]\n",
      "100%|█████████████████████████████| 500/500 [04:04<00:00,  2.05trial/s, best loss: -0.8007697220261504]\n",
      "100%|█████████████████████████████| 500/500 [03:32<00:00,  2.35trial/s, best loss: -0.8144913896451659]\n",
      "100%|█████████████████████████████| 500/500 [03:40<00:00,  2.26trial/s, best loss: -0.8121206009882022]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [('lr', lr_hyper), ('tree', tree_hyper), ('rf', RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(X_train_OE, \n",
    "                                                        y_train, \n",
    "                                                        X_test_OE, \n",
    "                                                        estimators=estimators, \n",
    "                                                        test_size=0.5,\n",
    "                                                        blending=True)\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final5, best_test_predict_final5 = final_model_opt(final_model_l, \n",
    "                                                            param_space_l, \n",
    "                                                            train_oof_blending.iloc[:, :-1], \n",
    "                                                            train_oof_blending.iloc[:, -1], \n",
    "                                                            test_predict_blending)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38384220-4b69-40b5-baa8-13bf193dbfb5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们保存各组Blending的融合结果，方便后续调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8350839c-8a01-4018-bece-9f7fb48497e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0647482 , 0.28947368, 0.0647482 , ..., 0.0647482 , 0.65714286,\n",
       "       0.0647482 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_test_predict_final1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688695c4-eb1a-4652-910d-199f060832c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Blending_res = pd.DataFrame({'res1':best_test_predict_final1, \n",
    "                             'res2':best_test_predict_final2, \n",
    "                             'res3':best_test_predict_final3, \n",
    "                             'res4':best_test_predict_final4, \n",
    "                             'res5':best_test_predict_final5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a83832e2-a2b8-4a34-8305-cca5f010b433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res1</th>\n",
       "      <th>res2</th>\n",
       "      <th>res3</th>\n",
       "      <th>res4</th>\n",
       "      <th>res5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.072438</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.277228</td>\n",
       "      <td>0.234146</td>\n",
       "      <td>0.230654</td>\n",
       "      <td>0.263626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.041876</td>\n",
       "      <td>0.048764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.072704</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.048764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.137391</td>\n",
       "      <td>0.110875</td>\n",
       "      <td>0.156221</td>\n",
       "      <td>0.141935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.110875</td>\n",
       "      <td>0.129351</td>\n",
       "      <td>0.112634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.630565</td>\n",
       "      <td>0.579210</td>\n",
       "      <td>0.476724</td>\n",
       "      <td>0.500612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.100716</td>\n",
       "      <td>0.095260</td>\n",
       "      <td>0.072663</td>\n",
       "      <td>0.061001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          res1      res2      res3      res4      res5\n",
       "0     0.064748  0.051689  0.072438  0.052375  0.056628\n",
       "1     0.289474  0.277228  0.234146  0.230654  0.263626\n",
       "2     0.064748  0.051689  0.042431  0.041876  0.048764\n",
       "3     0.064748  0.051689  0.042431  0.052375  0.056628\n",
       "4     0.064748  0.072704  0.042431  0.052375  0.048764\n",
       "...        ...       ...       ...       ...       ...\n",
       "1756  0.289474  0.137391  0.110875  0.156221  0.141935\n",
       "1757  0.064748  0.051689  0.042431  0.052375  0.056628\n",
       "1758  0.064748  0.118314  0.110875  0.129351  0.112634\n",
       "1759  0.657143  0.630565  0.579210  0.476724  0.500612\n",
       "1760  0.064748  0.100716  0.095260  0.072663  0.061001\n",
       "\n",
       "[1761 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Blending_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "14154416-2e53-4a4c-9e64-762f2a11f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入本地\n",
    "Blending_res.to_csv('Blending_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5318415-9b3c-4fea-9faf-02a5df676b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后续可以使用如下方式调用\n",
    "# Blending_res = pd.read_csv('Blending_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ecbd33a3-d8cc-4402-8af9-7a07f028c1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res1</th>\n",
       "      <th>res2</th>\n",
       "      <th>res3</th>\n",
       "      <th>res4</th>\n",
       "      <th>res5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.072438</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.277228</td>\n",
       "      <td>0.234146</td>\n",
       "      <td>0.230654</td>\n",
       "      <td>0.263626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.041876</td>\n",
       "      <td>0.048764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.072704</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.048764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.137391</td>\n",
       "      <td>0.110875</td>\n",
       "      <td>0.156221</td>\n",
       "      <td>0.141935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.118314</td>\n",
       "      <td>0.110875</td>\n",
       "      <td>0.129351</td>\n",
       "      <td>0.112634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.630565</td>\n",
       "      <td>0.579210</td>\n",
       "      <td>0.476724</td>\n",
       "      <td>0.500612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.100716</td>\n",
       "      <td>0.095260</td>\n",
       "      <td>0.072663</td>\n",
       "      <td>0.061001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          res1      res2      res3      res4      res5\n",
       "0     0.064748  0.051689  0.072438  0.052375  0.056628\n",
       "1     0.289474  0.277228  0.234146  0.230654  0.263626\n",
       "2     0.064748  0.051689  0.042431  0.041876  0.048764\n",
       "3     0.064748  0.051689  0.042431  0.052375  0.056628\n",
       "4     0.064748  0.072704  0.042431  0.052375  0.048764\n",
       "...        ...       ...       ...       ...       ...\n",
       "1756  0.289474  0.137391  0.110875  0.156221  0.141935\n",
       "1757  0.064748  0.051689  0.042431  0.052375  0.056628\n",
       "1758  0.064748  0.118314  0.110875  0.129351  0.112634\n",
       "1759  0.657143  0.630565  0.579210  0.476724  0.500612\n",
       "1760  0.064748  0.100716  0.095260  0.072663  0.061001\n",
       "\n",
       "[1761 rows x 5 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Blending_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b50b8-3fac-4afe-aafa-b21ef139941c",
   "metadata": {},
   "source": [
    "在得到了5组测试集预测结果后，接下来围绕对测试集的预测结果进行融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b643224-2f3f-47db-83fd-80e081a5616e",
   "metadata": {},
   "source": [
    "- 二阶段平均融合&手动设置权重的加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8ade2-4665-401b-9711-240b2dca0cbc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;二阶段融合需要注意，由于每个Blending融合过程中元学习器都是针对不同数据集进行的预测，所以无法先在训练集上测试融合效果再对测试集结果进行融合，此阶段的融合只能一次性的围绕测试集的预测结果进行融合并接提交结果，对于测试集标签未知的情况（如竞赛中），我们也只能通过在线提交结果后的结果评估看到最终预测效果。因此，这里只能选择平均融合或者手动设置权重的加权融合。这里我们首先尝试简单的均值融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8c568ba4-5058-47d3-a276-30db42b202b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1756    False\n",
       "1757    False\n",
       "1758    False\n",
       "1759     True\n",
       "1760    False\n",
       "Length: 1761, dtype: bool"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Blending_res.mean(axis=1) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e6ecd08f-8892-485b-90f9-8afd7df31586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7932992617830777"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Blending_res.mean(axis=1) > 0.5, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144ab71-7407-4065-bb49-5df3c207da3d",
   "metadata": {},
   "source": [
    "能够发现，最终融合结果较方案一，有非常明显的提升。接下来进一步考虑手动加权平均融合。这里我们采用Part 4.3.3中介绍的权重设置策略，即按照训练集上的评分进行排序，然后以排序结果作为权重进行加权平均融合，执行过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f19325a0-10b4-457c-9473-246b34db9288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "best_res_final1    0.829868\n",
       "best_res_final2    0.834437\n",
       "best_res_final3    0.816404\n",
       "best_res_final4    0.825367\n",
       "best_res_final5    0.814086\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([best_res_final1, \n",
    "           best_res_final2, \n",
    "           best_res_final3, \n",
    "           best_res_final4, \n",
    "           best_res_final5], index=['best_res_final1', \n",
    "                                    'best_res_final2', \n",
    "                                    'best_res_final3', \n",
    "                                    'best_res_final4', \n",
    "                                    'best_res_final5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6691f773-8cb8-4f4f-a490-ddfd18cee087",
   "metadata": {},
   "outputs": [],
   "source": [
    "Blending_res1 = ((Blending_res['res1'] * 4) + \n",
    "                 (Blending_res['res2'] * 5) + \n",
    "                 (Blending_res['res3'] * 2) + \n",
    "                 (Blending_res['res4'] * 3) + \n",
    "                 (Blending_res['res5'] * 1)) / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "74b6c7d4-052a-4a0e-8605-1bdb330d9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7938671209540034"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Blending_res1 > 0.5, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a7970-8bad-4b24-9f58-61efd3c6a4eb",
   "metadata": {},
   "source": [
    "能够发现，融合结果有了进一步提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60cecf-41c1-494b-8741-1fa5693d1e59",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完整介绍了Blending融合的两种高阶优化策略。其实在实践过程中，这两种方法的基本表现也和本节展示结果类似，在大多数情况下方案二的效果都会优于方案一，并且方案二也是为数极少的、非常实用的二阶融合策略。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
