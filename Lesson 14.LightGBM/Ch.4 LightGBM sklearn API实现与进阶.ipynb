{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5b759b-1812-4ad0-a418-002f804c199a",
   "metadata": {},
   "source": [
    "# <center> LightGBM原理与实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718670d1-c71a-480d-8b5a-e63173f3b60d",
   "metadata": {},
   "source": [
    "- 高阶集成学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24e337-edb5-4161-bcd2-d0343b1e431e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在学习了一系列梯度提升树的改进算法后，接下来，我们将进入到更加前沿的集成学习算法中，即LightGBM算法（全称为Light Gradient Boosting Machine，以下简称LGBM算法）和CatBoost算法（全称为Categorical Boosting）的学习中。和XGBoost算法（以下简称XGB算法）类似，这两个算法也是GBDT的改进算法，并且由于这两个算法诞生时间更晚，因此相比之下，LGBM和CatBoost拥有更多功能上的优化，以便应对更加复杂的当前机器学习应用情况。例如，相比XGB，LGBM有更高效的计算效率和更低的内存占用，并且面对高维数据，LGBM算法拥有更好的过拟合特性，这使得在建模数据量日趋增加的今天，LGBM会更适合作为前期探索性建模的模型（Baseline模型），并且在具体建模效果上，对比XGB也是不遑多让。而CatBoost算法同样在训练效率上比XGB更快，并且更加自动化——在特征衍生和超参数优化已经成为机器学习模型训练标配的今天，CatBoost能够（一定程度上）实现对输入的特征进行自动特征衍生和对超参数进行自动超参数优化。不难看出，LGBM和CatBoost是诞生于新应用环境中的新型集成学习算法，而对LGBM和CatBoost算法的学习，也成为了当今算法工程师的必修课。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1244cd-c105-456a-b7c6-407c5a180ae8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;不过同样需要说明的是，尽管LGBM和CatBoost算法对比XGB有诸多方面的优化，但这并不代表这两种算法相比XGB具有全方位的效果优势。在真实的实战应用，XGB（甚至是随机森林）仍然具有非常高的实践价值，很多时候我们需要尝试多种不同类型的算法，才能获得一个更好的结果。并且，由RF、XGB、LGBM、CatBoost属于“强而不同”的算法，这会导致这些模型结果会非常适合进行更进一步的模型融合，以达到更好的效果，因此在大多数追求极致建模效果的场景下，这些模型都需要训练，并得到一个尽可能好的结果，然后再进行融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e51b3-50fe-45b9-9702-96b28c3310fc",
   "metadata": {},
   "source": [
    "> 而相比其他集成学习算法，例如Bagging、AdaBoost等，RF、XGB、LGBM和CatB可以说是有全方位的效果优势，因此，除非是某些特殊场景，否则一般不会优先考虑使用这些算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32171271-ca57-4294-9238-9bc4cb1811a1",
   "metadata": {},
   "source": [
    "- LightGBM算法简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc099dc6-4d27-40f3-a240-73c3526cab9b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LightGBM 是一种高效的 Gradient Boosting 算法，由 Microsoft Research Asia 团队开发，早期为Microsoft内部处理海量高维数据的专用算法，并于2017年由Guolin Ke, Qi Meng, Thomas Finley等人通过论文形式正式发布。一经推出便引发业内极大关注，LGBM算法凭借其高效、快速、精准等特性，成为继XGB之后又一极具革命性的集成学习算法。        \n",
    "&emsp;&emsp;并且，和传统的GBDT相比，LGBM算法进行了非常多的模型训练层面的优化（主要是高效数值压缩方法），并且同时支持XGB中提出的直方图优化和正则化等方法，同时，作为新一代集成学习算法，LGBM支持分布式并行计算和GPU加速计算，并且能读取多种不同类型的数据进行训练，这使得LGBM在实际运行过程中更低的内存消耗、更快的计算效率和更加精准的预测结果。此外，从功能层面来看，LGBM和XGB类似，都支持多种损失函数，可以执行回归、分类和排序任务，并且内置了多种特征重要性评估方法，便于进行特征解释和筛选。        \n",
    "&emsp;&emsp;总的来说，从实践层面来看，LightGBM是一种预测精度位于第一梯队且功能多样的“次世代”集成学习算法，并且由于其出色的优化策略，使得其计算速度往往远超其他机器学习算法，根据算法提出者的论证，LightGBM在很多场景下的计算用时是GBDT的1/20，并且内存占用率极低。这使得LightGBM算法也是目前处理海量数据最为高效的机器学习算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbdcfa-5457-4489-b7c2-3282f7db753e",
   "metadata": {},
   "source": [
    "- LightGBM算法相关论文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5564b-0ebb-49b7-aac9-0bedab08935d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于新兴机器学习算法，最权威的介绍材料毫无疑问就是提出者发布的相关论文，这里我们重点推荐开发团队在2017年提出LGBM原理论文以及2019年由Essam Al Daoud提出的算法性能对比论文，两篇论文介绍及地址如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef15534-f1c1-4093-9e4c-f379d74855c4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;[LightGBM: A Highly Efficient Gradient Boosting Decision Tree (2017)](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)        \n",
    "&emsp;&emsp;作者：Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu      \n",
    "&emsp;&emsp;该论文是 LightGBM 的最初论文，详细阐述了 LightGBM 算法的设计思想、技术特点和实验结果。\n",
    "\n",
    "&emsp;&emsp;[Comparison between XGBoost, LightGBM and CatBoost using a home credit dataset (2019)](https://publications.waset.org/10009954/comparison-between-xgboost-lightgbm-and-catboost-using-a-home-credit-dataset)      \n",
    "&emsp;&emsp;作者：Essam Al Daoud      \n",
    "&emsp;&emsp;该论文详细对比了LGBM、XGB和CatB三个模型在信用卡数据上的性能差异，并提出了不同模型的超参数优化基本思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a21d91-9ab1-4d8b-9588-50970d1ab46d",
   "metadata": {},
   "source": [
    "在后续的LGBM算法原理讲解中，我们也将大量借鉴这些论文中的原理介绍相关内容和性能验证方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab80a25-15f8-4449-9737-35966980c5ca",
   "metadata": {},
   "source": [
    "> 上述论文可通过点击课件中蓝色链接在线观看，或查看课件网盘中的PDF版本论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d15cb7-a0f1-4de8-a3a1-28abf265032b",
   "metadata": {},
   "source": [
    "- LightGBM官方文档与开源项目地址"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40fb95-3412-4fa3-9aa9-e9fe2e620d50",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外官方说明文档和Github项目说明也是必不可少的学习材料。对于LightGBM来说，论文中隐藏了大量细节，这些都需要我们通过查阅官方说明文档和项目源码来进行补充。因此在后续的教学过程中，我们也将大量参考或者回归到这些全文说明文档中进行讲解和介绍，具体地址如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00bc911-b603-4162-9faf-3d21d361a667",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LightGBM的官方文档：https://lightgbm.readthedocs.io/en/v3.3.5/index.html\n",
    "\n",
    "&emsp;&emsp;LightGBM的GitHub地址：https://github.com/microsoft/LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb84bea-19f0-4247-b715-d47b815c532c",
   "metadata": {},
   "source": [
    "最后，需要导入本节课程需要用到的第三方库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a0574c-7385-43f6-b575-ee8660246dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-Learn相关模块\n",
    "# 评估器类\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 数据准备\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03a4f-38f1-46ea-9b4f-1e40e51e424a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7ab3b-0efb-4751-a6bd-c80f04cb0cae",
   "metadata": {},
   "source": [
    "## <center>Ch.2 LightGBM sklearn API实现与进阶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09265a6c-6d48-4ef9-a4fb-83b0285b73b9",
   "metadata": {},
   "source": [
    "### 1.LightGBM算法特性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bec602-0a7c-4436-898f-3ab1c4727a14",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据上一小节的LGBM优化策略的总结，我们不难发现LGBM算法具备如下特性："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c4316-6a5f-4457-9eb1-15035c01e04b",
   "metadata": {},
   "source": [
    "- 超高的计算效率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91724917-b7eb-48b0-82fd-9d191f130472",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其实从上面的和GBDT的对比中不难发现，LGBM最大的特点就是计算效率上进行了诸多优化，这方面的优化在具体建模过程中表现为两个方面，其一是建模时间极大缩短，其二则是计算过程的内存占用大幅减少，并且，LGBM同时提供多线程并行计算、分布式计算和GPU加速计算等多种计算实现方式，选择合适的计算方式，可以进一步提升计算效率；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe9d16-7781-4931-9502-06d34dad42ba",
   "metadata": {},
   "source": [
    "- 性能强劲"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b7ded-e79b-4421-9f83-c3ca5e7d9ede",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而同时，算法原理层面的“简化操作”并不会对LGBM的实际预测效率产生太大的影响，尽管从理论上来说，LGBM算法的精确性是介于RF和XGB之间（强于随机森林、弱与XGB），但实际建模过程中，这些微弱的理论层面的性能差异并不会带来太多实际影响，并且复杂度降低有时候反而会使得模型更克服过拟合问题，从而获得一个更好的预测结果。在实际使用过程中，LGBM和XGB性能层面并没有明显差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a924e1-20b6-4faf-8322-9f39108be574",
   "metadata": {},
   "source": [
    "- 功能完善"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3c11e-ede4-4d43-ae3c-3ba738e5cdb5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并且作为新兴的（次世代）集成学习算法，LGBM提供了 Python、R 和 C++ 的 API，易于与其他机器学习库集成，且支持Windos、Linux、MacOS等平台部署，实现层面也同时拥有sklearn API和原生API等多种调用方式，功能层面同样支持多种损失函数、能够应对排序任务、时间序列任务等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c640775-77a4-4dd4-9a6e-105b6328f209",
   "metadata": {},
   "source": [
    "&emsp;&emsp;功能完善、性能强劲，同时能保持高效的计算效率，这也是Light一词的精髓所在。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4810a-9963-4431-a2a1-4852ffaf6315",
   "metadata": {},
   "source": [
    "> 我们将在Ch3.LightGBM原理进阶进行更加完整深入的原理介绍。在不影响LGBM使用的情况下，接下来我们将开始介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b5721-f60d-4e7a-8af1-7f0f79f39774",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.LightGBM算法的快速使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8f610-19fd-40f0-9a7e-4d5db72907e4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解了LGBM基本原理和性能特性后，接下来让我们尝试快速使用先尝试着快速使用LGBM算法，并在实际使用过程中，逐步总结LGBM的计算效率和算法性能上的特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f84b1-b7cf-4abb-8755-b4771a5eff05",
   "metadata": {},
   "source": [
    "#### 1.1 LGBM算法库安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7d50b-22b8-4c61-b339-40c83a727973",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首次使用LGBM算法前，需要进行LGBM算法库的安装，这里推荐直接使用pip进行安装，在命令行中输入如下命令进行LGBM算法库的安装："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d65c5-1cc6-4c4b-b41f-7d02d207e9ed",
   "metadata": {},
   "source": [
    "<center> pip install lightgbm -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1912e-5c7b-4ef3-ab2c-9b9e74605ad7",
   "metadata": {},
   "source": [
    "安装完成后，即可按照如下方式进行导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffe6109-0401-4545-880a-0b243446b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "05657599-2276-4409-8ed1-dc3dbe0a5b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'lightgbm' from 'D:\\\\anaconda3\\\\lib\\\\site-packages\\\\lightgbm\\\\__init__.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        d:\\anaconda3\\lib\\site-packages\\lightgbm\\__init__.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "LightGBM, Light Gradient Boosting Machine.\n",
       "\n",
       "Contributors: https://github.com/microsoft/LightGBM/graphs/contributors.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eccb53b-bb16-45b1-ae62-afc6bb673f5f",
   "metadata": {},
   "source": [
    "目前课程采用的LGBM版本是3.3.5，可以通过如下方式查看LGBM版本号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9f95619-36ea-4421-9e6c-7183bbf245cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.5'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf322c-eef6-4cd7-8a39-0de2d66a02c7",
   "metadata": {},
   "source": [
    "如果是低于这个版本的LGBM，则可在命令行中输入以下命令对LGBM进行升级："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a120d43-64b6-4a3c-86a8-ce672f9a82a6",
   "metadata": {},
   "source": [
    "<center> pip install --upgrade lightgbm -i https://pypi.tuna.tsinghua.edu.cn/simple --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54c036-6948-4405-a4d4-bec77e951ff9",
   "metadata": {},
   "source": [
    "#### 1.2 LGBM的sklearn API快速使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c8082-204f-4b35-86a6-7f5cd20f2be3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们尝试训练LGBM模型。对于LGBM来说，支持多种不同类型的数据输入以及多种不同类型的训练方式，这里我们先从最简单的应用情况开始介绍，即围绕CSV格式数据进行DATaFrame数据格式读取，并采用类sklearn的建模风格进行模型的训练。这里我们先采用鸢尾花数据集进行简单模型测试，数据导入和数据集划分如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d1c6e8-6a5b-40f0-b8b8-fdd22b5c54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09083beb-fdca-4505-85f0-e4d3f25b6c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88446e8-c250-44df-a391-483fad70ff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f508767-ece0-4261-837a-98db85d0df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(\"target\", axis=1), data[\"target\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1428f3a-bf4b-4025-b050-d4f06d4d7ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "46                 5.1               3.8                1.6               0.2\n",
       "81                 5.5               2.4                3.7               1.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "116                6.5               3.0                5.5               1.8\n",
       "101                5.8               2.7                5.1               1.9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb6b362-a5ff-452d-a143-2a1e26dbe0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46     0.0\n",
       "81     1.0\n",
       "122    2.0\n",
       "116    2.0\n",
       "101    2.0\n",
       "      ... \n",
       "83     1.0\n",
       "92     1.0\n",
       "12     0.0\n",
       "24     0.0\n",
       "11     0.0\n",
       "Name: target, Length: 120, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e3757-c2cc-47cf-8cc0-ee286688244c",
   "metadata": {},
   "source": [
    "然后是模型实例化过程。和XGB类似，LGBM不同类型的模型（及功能）可以直接通过导入子模块的方式来调用，也可以直接通过父模块.子模块的方式直接进行调用，例如我们可以通过如下方式调用LGBM的分类模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6da1436-0a77-4d60-9668-a8ffccc3a692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = lgb.LGBMClassifier()\n",
    "gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9753f-e5a4-4ebc-b0e9-fe768f91a542",
   "metadata": {},
   "source": [
    "也可以先导入，再调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0105cb0-532e-45e1-9030-534827fc4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eba18b6-0f62-4648-985f-a93a5ad318ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = LGBMClassifier()\n",
    "gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6cbbbc-bf27-42df-b25a-a79d78cdac5f",
   "metadata": {},
   "source": [
    "在sklearn API中，模型已经设置好了默认超参数取值，可以完全不认识任何模型超参数的情况下进行建模。sklearn API中LGBM的超参数情况如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032fe0f4-fb41-4e47-af88-aa4f9bcd665f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gbdt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'split'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM classifier.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    group : array-like\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     DaskLGBMClassifier\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LGBMClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a2c07-9fc5-4987-becf-372534b72855",
   "metadata": {},
   "source": [
    "具体的模型训练过程和sklearn中其他模型一样，通过fit进行训练，并利用predict进行结果输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb67ac99-5bf1-4d55-b464-063094e616b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc5449-9c00-4ec5-83d7-a113e989112f",
   "metadata": {},
   "source": [
    "然后输出预测结果，同样可以输出类别结果和概率预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688f7396-c984-45fd-b474-730c3d17279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 2., 1., 2., 2., 1., 2., 2., 2., 2., 1., 0., 2., 0., 1., 0.,\n",
       "       0., 1., 2., 2., 0., 1., 1., 1., 0., 1., 2., 2., 2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b5c0f37-e9cd-4373-931d-8bf0fe916a11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.79636842e-04, 7.28615644e-01, 2.71204719e-01],\n",
       "       [9.99987227e-01, 1.20250690e-05, 7.47948037e-07],\n",
       "       [1.34676011e-05, 2.31251864e-05, 9.99963407e-01],\n",
       "       [2.18466583e-04, 9.99651775e-01, 1.29757936e-04],\n",
       "       [1.34824244e-06, 1.00600076e-04, 9.99898052e-01],\n",
       "       [3.13568859e-04, 3.94433204e-04, 9.99291998e-01],\n",
       "       [4.72453331e-06, 9.99967609e-01, 2.76662323e-05],\n",
       "       [2.20066038e-04, 7.11129401e-02, 9.28666994e-01],\n",
       "       [4.86834830e-05, 1.16781709e-03, 9.98783499e-01],\n",
       "       [8.07432727e-06, 1.47405138e-04, 9.99844521e-01],\n",
       "       [1.25168480e-05, 8.01178053e-05, 9.99907365e-01],\n",
       "       [5.54258177e-06, 9.99755969e-01, 2.38488661e-04],\n",
       "       [9.99983112e-01, 1.62201809e-05, 6.68162169e-07],\n",
       "       [9.75921627e-06, 1.63169828e-05, 9.99973924e-01],\n",
       "       [9.99994206e-01, 5.04369190e-06, 7.50454933e-07],\n",
       "       [5.82426993e-06, 9.99745873e-01, 2.48303116e-04],\n",
       "       [9.99992739e-01, 5.84123034e-06, 1.41954340e-06],\n",
       "       [9.99997743e-01, 1.39113965e-06, 8.65727207e-07],\n",
       "       [2.85180714e-06, 9.99979178e-01, 1.79702952e-05],\n",
       "       [1.05151634e-06, 2.10859818e-05, 9.99977863e-01],\n",
       "       [1.18514070e-05, 1.33569480e-03, 9.98652454e-01],\n",
       "       [9.99981328e-01, 1.78480174e-05, 8.24301315e-07],\n",
       "       [1.26807400e-05, 9.99921290e-01, 6.60297323e-05],\n",
       "       [5.89371662e-06, 9.99928055e-01, 6.60514676e-05],\n",
       "       [1.64935398e-04, 9.99178340e-01, 6.56724513e-04],\n",
       "       [9.99993426e-01, 5.84150550e-06, 7.32066079e-07],\n",
       "       [3.57339374e-06, 9.99990573e-01, 5.85325868e-06],\n",
       "       [1.05151634e-06, 2.10859818e-05, 9.99977863e-01],\n",
       "       [1.25522203e-05, 2.82715622e-04, 9.99704732e-01],\n",
       "       [5.01216937e-06, 3.26785073e-04, 9.99668203e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5c9d8-292a-45c7-a142-6204999ae771",
   "metadata": {},
   "source": [
    "我们可以借助argmax方法将概率预测结果转化为类别预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b138f871-0d13-4032-b17b-3820bb9c2abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 0, 2, 0, 1, 0, 0, 1, 2, 2, 0,\n",
       "       1, 1, 1, 0, 1, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(gbm.predict_proba(X_test), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcda57-67e4-4dfd-a2e1-164ce9fe3312",
   "metadata": {},
   "source": [
    "同样，我们可以调用accuracy_score函数快速查看最终模型在训练集和测试集上的准确率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d82cd895-9094-4ebc-80fc-b9f22460219b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9666666666666667)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, gbm.predict(X_train)), accuracy_score(y_test, gbm.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70602f-45fe-465a-a5d7-2feb68f009c9",
   "metadata": {},
   "source": [
    "至此，我们就完成了一次简单的LGBM算法的sklearn API调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d59dad-f4cf-4f23-bb31-73c05b042b9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.LightGBM sklearn API超参数解释与使用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b8249-4ff7-4beb-b5d5-3acc9c9b5fc8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们进一步解释LGBM的sklearn API中各评估器中的超参数及使用方法。尽管此时我们并未深入讲解LGBM的算法原理，但LGBM评估器的整体超参数构成和GBDT、XGB类似，我们可以借助此前的知识、并通过类比的方法了解LGBM的每个超参数的具体作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a691f4-4d4c-4022-8600-4818163f9b5d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在LGBM的sklearn API中，总共包含四个模型类（也就是四个评估器），分别是lightgbm.LGBMModel、LGBMClassifier 和 LGBMRegressor 以及LGBMRanker："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f812f03-a403-413e-b785-01cdcb3ce057",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202303181606760.png\" alt=\"1679126801449\" style=\"zoom:35%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823290e9-dc72-46c8-9449-5b179d4ec154",
   "metadata": {},
   "source": [
    "其中LGBMModel是 LightGBM 的基本模型类，它是一个泛型模型类，可以用于各种类型的问题（如分类、回归等）。通常，我们不直接使用 LGBMModel，而是使用针对特定任务的子类使用不同的类，即分类问题使用 LGBMClassifier 、回归问题使用 LGBMRegressor，而排序问题则使用LGBMRanker。接下来我们重点解释分类和回归模型类的超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef458c-f80a-4543-93e4-619f55ca0e8f",
   "metadata": {},
   "source": [
    "#### 2.1 LGBMClassifier超参数概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9588ae-ee3d-4a3e-aef2-1c0c3de9f446",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和sklearn中其他评估器命名规则一致，Classifier是用于分类问题的评估器，Regressor是用于回归问题的评估器，两个评估器的超参数构成类似，这里我们先看LGBMClassifier的超参数构成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbfae391-7672-44b8-a297-f1b670e34aee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gbdt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'split'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM classifier.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    group : array-like\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     DaskLGBMClassifier\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LGBMClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c441cfc-443f-428b-8207-fd23b4cea721",
   "metadata": {},
   "source": [
    "总的来看，我们可以将LGBMClassifier的超参数分为决策树剪枝超参数、Boosting过程控制超参数、特征和数据处理类超参数和其他超参数四类。其中决策树剪枝超参数包括max_depth（树的最大深度）、num_leaves（叶子节点数）、min_child_samples（子节点的最小样本数）等，都是大家非常熟悉的超参数；Boosting过程控制超参数和XGB的booster类似，包含boosting_type（提升类型，如gbdt、dart、goss、rf），n_estimators（迭代次数）、learning_rate（学习率）和reg_alpha（L1正则化系数）、reg_lambda（L2正则化系数）；而特征和数据处理类超参数则主要与数据采样有关，包含subsample（样本子集的比例）、subsample_freq（进行子采样的频率）、colsample_bytree（列采样的比例）等；而其他参数则是一些对建模效果并没有重要影响的超参数，例如random_state（随机数种子）、n_jobs（并行计算所用的CPU核心数）、class_weight（类别权重）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f61ae-2a11-4a3c-916a-a43911f7ca90",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从整体来看，LGBMClassifier的超参数构成并不复杂，对比XGB，甚至可以说是更加简洁清晰。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0ceaa-0f55-45fd-a8f8-14d1071d2b03",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们就逐个解释这些超参数，并对其使用方法进行说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81487a6b-387c-4d9b-b13c-757cf28d09a8",
   "metadata": {},
   "source": [
    "#### 2.2 LGBMClassifier的决策树剪枝超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fe779-046a-479a-9ee2-3823df753cbf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LGBM中决策树剪枝超参数和其他评估器中剪枝超参数并无区别，这类超参数已经在课程中反复解释过多次，超参数及解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d285a9b-9b78-4da9-93b2-b895a46e5266",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|num_leaves|一棵树上的叶子节点数，默认为 31| \n",
    "|max_depth|树的最大深度，默认值为 -1，表示无限制|\n",
    "|min_split_gain|相当于min_impurity_decrease，再分裂所需最小增益。默认值为 0，表示无限制|\n",
    "|min_child_weight|子节点的最小权重和。默认值为 1e-3。较大的 min_child_weight 可以防止过拟合| \n",
    "|min_child_samples|相当于min_samples_leaf，单个叶子节点上的最小样本数量。默认值为 20。较大的 min_child_samples 可以防止过拟合| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b361b-8f05-4543-8be3-c3dadf8ead78",
   "metadata": {},
   "source": [
    "能够发现，相比决策树或者随机森林，LGBM的决策树剪枝类参数有所精简，保留了最能影响决策树生长的核心超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec7141-16cb-4bdb-9499-629cc01a9e56",
   "metadata": {},
   "source": [
    "#### 2.3 LGBMClassifier的Boosting过程控制超参数解释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6473289-441b-406a-a694-6bf43cf3c680",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后来看更加重要的Boosting过程控制超参数，首先较为复杂的是boosting_type，该超参数解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fa7b5-4200-41dd-a1c6-88ccd13680d9",
   "metadata": {},
   "source": [
    "- boosting_type: 使用的梯度提升算法类型，默认为 'gbdt'，可选项包括 'gbdt'（梯度提升决策树）、'dart'（Dropouts meet Multiple Additive Regression Trees）、'goss'（Gradient-based One-Side Sampling）和 'rf'（Random Forest，随机森林）。其中GBDT是最常用、且性能最稳定的 boosting 类型，也是boosting_type默认取值；而dart (Dropouts meet Multiple Additive Regression Trees)则是一种结合了 Dropout 和多重加性回归树的方法。它在每次迭代过程中随机选择一部分树进行更新，会较大程度增加模型随机性，可以用于存在较多噪声的数据集或者数据集相对简单（需要减少过拟合风险）的场景中；GOSS 是一种基于梯度的单侧采样方法。它在每次迭代中只使用具有较大梯度的样本进行训练，从而降低计算复杂度。goss(Gradient-based One-Side Sampling)则是前文介绍的梯度的单边采样算法，可以在保持较高精度的同时加速训练过程，适用于大规模数据集，可以在保持较高精度的同时加速训练过程，有些时候精度不如GBDT；而rf则是采用随机森林来进行“Boosting过程”，或者说此时就不再是Boosting，而是替换成了Bagging过程，类似于XGBRFClassifier的计算过程，此时LGBM本质上将按照RF的计算范式进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6ec93-4aba-41cc-a3e2-5e97ce278952",
   "metadata": {},
   "source": [
    "然后与之相关的是subsample_for_bin参数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85d1ac-de45-4b17-8168-6801384e1c65",
   "metadata": {},
   "source": [
    "- subsample_for_bin：该参数表示对连续变量进行分箱时（直方图优化过程）抽取样本的个数，默认取值为200000，当实际抽样个数大于输入训练数据样本数量时，会带入全部样本进行计算。而如果boosting_type选择的是goss，则在直方图优化时会自动完成抽样，具体抽样策略是：会保留所有较小梯度的样本（即那些已经被模型很好拟合的样本），并对较大梯度的样本进行采样。这种策略能够在加速训练（大梯度样本的贡献）的同时有效防止过拟合（小梯度样本的贡献）。因此，如果boosting_type选择的是 \"goss\"，。则subsample_for_bin参数会失去作用，此时无论subsample_for_bin取值多少都不影响最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b421d-2440-41b6-94a7-0a1c30c30acf",
   "metadata": {},
   "source": [
    "> 如果需要控制goss过程，则需要借助top_rate 和 other_rate 这两个参数，但是这两个参数只存在于LGBM原生API中，在sklearn中并没有，因此在使用 LightGBM 的 sklearn API 时，GOSS 采样方法会自动进行调整。关于这两个参数的解释和使用，我们将在介绍LGBM原生API时讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f495c5-856f-4826-a369-aec6b4a4828d",
   "metadata": {},
   "source": [
    "其他参数和GBDT及XGB类似，具体解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c1a21-9244-48c4-a987-49e2e34a1371",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|learning_rate|学习率，即每次迭代中梯度提升的步长，默认值为 0.1| \n",
    "|n_estimators|迭代次数，即生成的基学习器的数量，默认值为 100|\n",
    "|reg_alpha| L1 正则化系数，默认值为 0|\n",
    "|reg_lambda| L2 正则化系数。默认值为 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ec0ee-58d6-4f0b-941b-89657ad4425a",
   "metadata": {},
   "source": [
    "#### 2.4 LGBMClassifier的特征和数据处理类超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57ae97-c80f-48cb-a8be-0c81516e9872",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LGBM中特征和数据处理类超参数主要用于每次迭代时数据和特征的分配，核心作用是控制模型训练过程的随机性，其超参数的基本解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf0dfd-bfd0-45bb-95e1-2bad459a4076",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|subsample|模型训练时抽取的样本数量，取值范围为 (0, 1]，表示抽样比例，默认为1.0| \n",
    "|subsample_freq|抽样频率，表示每隔几轮进行一次抽样，默认取值为0，表示不进行随机抽样|\n",
    "|colsample_bytree|在每次迭代（树的构建）时，随机选择特征的比例，取值范围为 (0, 1]，默认为1.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f413d5-0d3b-4b40-9961-83cad789633c",
   "metadata": {},
   "source": [
    "这里有以下几点需要注意："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cdf3e0-0a45-4682-bf98-4969e008c2bb",
   "metadata": {},
   "source": [
    "- subsample和subsample_for_bin之间的关系        \n",
    "&emsp;&emsp;这两个参数尽管从字面理解都是抽样比例，但实际上这两个参数是完全独立的，彼此之间并不影响。其中subsample_for_bin抽样结果用于直方图构建，而subsample抽样结果则是用于模型训练，这两个抽样过程彼此独立，互不影响；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a302eb-5619-452e-9198-ef1bc7777347",
   "metadata": {},
   "source": [
    "- subsample和subsample_freq之间的关系        \n",
    "&emsp;&emsp;更加关键的是subsample_freq参数，如果subsample_freq=0，则无论subsample取值为多少，模型训练时都不会进行随机抽样；换而言之，只有subsample_freq不等于0，且subsample不等于1.0时，才会进行抽样；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5ee02-e0f3-41b0-9334-9b436cfd4a1a",
   "metadata": {},
   "source": [
    "- subsample_freq和colsample_bytree之间的关系        \n",
    "&emsp;&emsp;不同于subsample是样本抽样，colsample_bytree是每次迭代（每次构建一颗树时）进行的特征抽样，并且colsample_bytree不受subsample_freq影响，即无论subsample_freq取值为多少，每次建树时都会根据colsample_bytree设置的比例进行列抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6475b9-fa75-45bb-bb22-c26ff4fb9594",
   "metadata": {},
   "source": [
    "- LGBM和RF的不同特征抽样（分配）规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2eab6-788f-4c14-b744-623693267ba8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同时需要注意的是，LGBM和随机森林不同，随机森林是每棵树的每次分裂时都随机分配特征，而LGBM是每次构建一颗树时随机分配一个特征子集，这颗树在成长过程中每次分裂都是依据这个特征子集进行生长。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fae5a1-252c-4d6b-b382-ef7aceefdf75",
   "metadata": {},
   "source": [
    "#### 2.5 LGBMClassifier的其他超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb4bd1-5785-4700-bcd1-06b74e733f42",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后则是一些用于辅助建模的超参数，具体超参数及解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32451119-bd46-4ac0-84ba-3f97d557a803",
   "metadata": {},
   "source": [
    "|Name|Description|      \n",
    "|:--:|:--:| \n",
    "|objective|指定目标函数，默认为None，会自动判断是二分类还是多分类问题，这里我们也可以手动设置 'binary'(用于二分类问题)或'multiclass'(用于多分类问题)| \n",
    "|class_weight|样本权重设置参数|\n",
    "|importance_type|特征重要性计算方式，默认为 'split'，表示使用特征在模型中被选中作为分裂特征的次数，可选 'gain'，表示使用特征在模型中的分裂收益之和作为重要性评估指标|\n",
    "|random_state|随机数种子|\n",
    "|n_jobs|并行的线程数，默认为-1，调用全部可用线程|\n",
    "|silent|是否沉默（不输出日志），默认为'warn'，仅显示警告和报错，可选'info'，用于打印全部信息|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d877a1-7515-4079-8baa-56dcb3d77069",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是关于目标函数的设置，对于LGBM的sklearn API来说，objective参数较为简单，只有'binary'和'multiclass'两种，其中'binary'表示逻辑回归损失，也就是二分类交叉熵损失，而'multiclass'则代表多分类交叉熵损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499d66a-0f4f-40b7-a4aa-fdcced18bdbc",
   "metadata": {},
   "source": [
    "> 当然，LGBM也支持softmax损失，只不过无法通过objective传入。softmax损失默认是只有原生API才支持调用，若需要传入sklearn API中，则需要借助\\*\\*kwargs参数，以原生API形式进行传输。原生API的调用方法我们将在下一小节进行探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd1fbc-5294-4e89-b9ef-cf854edb99d6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，需要注意用于计算特征重要性的importance_type参数，这也是LGBM算法的特点之一——即提供了两种不同的用于评估特征重要性的方法。很多时候我们在特征工程阶段都会创建海量特征来增强数据表现，并且通过一些数值指标或者模型来进行特征筛选，进而兼顾建模效率和预测效果。考虑到LGBM同时拥有非常高的计算效率，这使得LGBM算法成为筛选特征的最常用的模型之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4c0ef-6a9d-458a-a577-103fb726f9f7",
   "metadata": {},
   "source": [
    "> 具体特征衍生方法，我们将在特征工程阶段进行详细介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff791a-67e1-49cb-8239-4a6a9e692840",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完整解释了LGBMClassifier的全部超参数。接下来我们继续讨论LGBMRegressor超参数。LGBMRegressor和LGBMClassifier只有两点不同，其一是LGBMRegressor没有class_weight参数，其二则是LGBMRegressor的损失函数和LGBMClassifier完全不同。接下来我们就LGBMRegressor的损失函数选取进行解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ae603-2ba4-4b34-827f-6c2b13984eb0",
   "metadata": {},
   "source": [
    "#### 2.6 LGBMRegressor损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f7da8e7-825b-4808-a179-58065f224ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd344701-9f12-4ddb-a04b-1e5c21598bbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLGBMRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gbdt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'split'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM regressor.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    group : array-like\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     DaskLGBMRegressor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LGBMRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480bd221-9e72-4d63-ade7-ed3067c7e5f9",
   "metadata": {},
   "source": [
    "LGBMRegressor的损失函数包含了GBDT和XGB的各类回归类损失函数，相关计算过程及应用场景此前都有详细讨论，这里只进行简要说明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea377767-8a67-4950-9863-1b3f0c50a5ef",
   "metadata": {},
   "source": [
    "- 均方误差（MSE, Mean Squared Error）：最常用的的损失函数，此时objective='regression' 或 objective='regression_l2'；\n",
    "- 平均绝对误差（MAE, Mean Absolute Error）：通常用于标签存在异常值情况，此时objective='regression_l1'；\n",
    "- Huber损失（Huber Loss）：适用于目标值存在大量异常值或者噪声时。Huber损失在预测误差较小时表现为均方误差，在预测误差较大时表现为平均绝对误差，这使得它对异常值具有更好的鲁棒性。此时objective='quantile'；\n",
    "- Quantile损失（Quantile Loss）：用于分位数回归，最小化预测值与真实值之间的分位数损失，适用于需要对预测分布进行精细控制的场景（例如围绕某种分布进行预测）。此时objective='quantile'；\n",
    "- Poisson损失（Poisson Loss）： 适用于计数问题，即目标值是非负整数且服从泊松分布。此时objective='poisson'\n",
    "- Gamma损失（Gamma Loss）：适用于预测非负实数且服从伽马分布的目标值。此时objective='gamma'\n",
    "- Tweedie损失（Tweedie Loss）：适用于广义线性模型（Generalized Linear Models, GLMs）中的 Tweedie 分布（非对称分布）的数据集。此时objective='tweedie'。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba271a-f9d1-4fbf-99a6-d30e10d9c9a4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;能够发现，大多数损失函数都是针对标签存在某个具体分布时进行的预测，一般情况下我们都是在均方误差（MSE）、平均绝对误差（MAE）和Huber损失中进行选择，通常情况首选MSE，而当标签存在噪声或者异常点时，MAE会表现出更好的泛化能力。并且由于MAE较为特殊的计算过程，导致其在正常情况数据集下精度不如MSE。而Huber则是二者的综合，适用于标签存在少量异常值的数据集，Huber对异常值较为鲁棒，同时又可以保留较好的精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6e88e-0525-46cf-b1a8-16a1a5d6598c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.LightGBM sklearn API进阶使用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa73d2-6ea3-4206-9aa9-a87c74ae5592",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们进一步尝试进行更复杂的LGBM API的调用，以满足更加复杂的建模过程。这里我们仍然是分sklearn API和原生API两种调用方式进行尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a61490-63a6-4bad-96fd-f1ecd2a14c01",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是sklearn API进阶调用方法。在实际建模过程中，如果需要进一步借助sklearn功能模块进行建模，例如需要进行网格搜索超参数优化、构建机器学习流或者和sklearn其他评估器进行模型融合等，则可以调用LGBM的sklearn API进行快速执行。当然，大多数sklearn的相关功能都在此前的课程中有所尝试，这里我们对此快速进行回顾。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2fb84-15dc-452d-beab-07df9d81e476",
   "metadata": {},
   "source": [
    "#### 2.1 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11141f26-07e9-403e-be4c-acab6b637343",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先是交叉验证，通过调用LGBM的sklearn API，我们可以将LGBM作为评估器输入到交叉验证相关函数中进行快速计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5cb3c032-9b4a-4675-8e3a-88293b44d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "78ae61cc-1330-43c0-ba7b-4f0144a40481",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2*n_jobs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Evaluate a score by cross-validation.\n",
       "\n",
       "Read more in the :ref:`User Guide <cross_validation>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimator : estimator object implementing 'fit'\n",
       "    The object to use to fit the data.\n",
       "\n",
       "X : array-like of shape (n_samples, n_features)\n",
       "    The data to fit. Can be for example a list, or an array.\n",
       "\n",
       "y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
       "    The target variable to try to predict in the case of\n",
       "    supervised learning.\n",
       "\n",
       "groups : array-like of shape (n_samples,), default=None\n",
       "    Group labels for the samples used while splitting the dataset into\n",
       "    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
       "    instance (e.g., :class:`GroupKFold`).\n",
       "\n",
       "scoring : str or callable, default=None\n",
       "    A str (see model evaluation documentation) or\n",
       "    a scorer callable object / function with signature\n",
       "    ``scorer(estimator, X, y)`` which should return only\n",
       "    a single value.\n",
       "\n",
       "    Similar to :func:`cross_validate`\n",
       "    but only a single metric is permitted.\n",
       "\n",
       "    If `None`, the estimator's default scorer (if available) is used.\n",
       "\n",
       "cv : int, cross-validation generator or an iterable, default=None\n",
       "    Determines the cross-validation splitting strategy.\n",
       "    Possible inputs for cv are:\n",
       "\n",
       "    - `None`, to use the default 5-fold cross validation,\n",
       "    - int, to specify the number of folds in a `(Stratified)KFold`,\n",
       "    - :term:`CV splitter`,\n",
       "    - An iterable that generates (train, test) splits as arrays of indices.\n",
       "\n",
       "    For `int`/`None` inputs, if the estimator is a classifier and `y` is\n",
       "    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
       "    other cases, :class:`KFold` is used. These splitters are instantiated\n",
       "    with `shuffle=False` so the splits will be the same across calls.\n",
       "\n",
       "    Refer :ref:`User Guide <cross_validation>` for the various\n",
       "    cross-validation strategies that can be used here.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "        `cv` default value if `None` changed from 3-fold to 5-fold.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of jobs to run in parallel. Training the estimator and computing\n",
       "    the score are parallelized over the cross-validation splits.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    The verbosity level.\n",
       "\n",
       "fit_params : dict, default=None\n",
       "    Parameters to pass to the fit method of the estimator.\n",
       "\n",
       "pre_dispatch : int or str, default='2*n_jobs'\n",
       "    Controls the number of jobs that get dispatched during parallel\n",
       "    execution. Reducing this number can be useful to avoid an\n",
       "    explosion of memory consumption when more jobs get dispatched\n",
       "    than CPUs can process. This parameter can be:\n",
       "\n",
       "        - ``None``, in which case all the jobs are immediately\n",
       "          created and spawned. Use this for lightweight and\n",
       "          fast-running jobs, to avoid delays due to on-demand\n",
       "          spawning of the jobs\n",
       "\n",
       "        - An int, giving the exact number of total jobs that are\n",
       "          spawned\n",
       "\n",
       "        - A str, giving an expression as a function of n_jobs,\n",
       "          as in '2*n_jobs'\n",
       "\n",
       "error_score : 'raise' or numeric, default=np.nan\n",
       "    Value to assign to the score if an error occurs in estimator fitting.\n",
       "    If set to 'raise', the error is raised.\n",
       "    If a numeric value is given, FitFailedWarning is raised.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Returns\n",
       "-------\n",
       "scores : ndarray of float of shape=(len(list(cv)),)\n",
       "    Array of scores of the estimator for each run of the cross validation.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import datasets, linear_model\n",
       ">>> from sklearn.model_selection import cross_val_score\n",
       ">>> diabetes = datasets.load_diabetes()\n",
       ">>> X = diabetes.data[:150]\n",
       ">>> y = diabetes.target[:150]\n",
       ">>> lasso = linear_model.Lasso()\n",
       ">>> print(cross_val_score(lasso, X, y, cv=3))\n",
       "[0.33150734 0.08022311 0.03531764]\n",
       "\n",
       "See Also\n",
       "---------\n",
       "cross_validate : To run cross-validation on multiple metrics and also to\n",
       "    return train scores, fit times and score times.\n",
       "\n",
       "cross_val_predict : Get predictions from each split of cross-validation for\n",
       "    diagnostic purposes.\n",
       "\n",
       "sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
       "    loss function.\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20c2eafa-68af-402a-a5d2-b9bb0b4284c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83333333, 1.        , 1.        , 1.        , 0.875     ])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LGBMClassifier(), X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249285e6-c927-4fb2-81df-9e26a1809e88",
   "metadata": {},
   "source": [
    "#### 2.2 网格搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e90b2f-023d-4827-a717-76c2f194a0ed",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接着，我们进一步尝试借助网格搜索对LGBM评估器进行超参数优化。当然此时我们还未详细解读LGBM的超参数含义，这里我们先以测试相关功能为主，关于LGBM超参数的解读会在介绍完算法原理后进行探讨。这里我们简单设置超参数搜索空间为'n_estimators': [90, 100, 110]进行网格搜索测试，LGBM和其他GBDT框架下的集成学习一样，n_estimators都代表迭代次数或者基础学习器个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "42c3e656-e889-4548-8038-e7e9eab721f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbm = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2cca1f55-c693-44d1-a20b-ad8d1d453d9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gbdt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'split'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM classifier.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    group : array-like\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     DaskLGBMClassifier\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LGBMClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "56c1116f-a46d-4d32-9109-e45d8fdf4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [90, 100, 110]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "43979f95-448d-42eb-bfb8-8b309ee4c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_search = GridSearchCV(estimator=gbm, param_grid=param_grid, n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8db5a8e2-b8c4-4645-9256-a1bada3120f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LGBMClassifier(), n_jobs=10,\n",
       "             param_grid={'n_estimators': [90, 100, 110]})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5372afba-ffee-4367-98fd-c74d8e81ffd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416666666666668"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8425e481-bcc5-45b4-8572-25b54da8c333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 90}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0f10e-9996-4ec0-ad07-77e1c3f38659",
   "metadata": {},
   "source": [
    "#### 2.3 构建Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f4e96-718d-4a18-8f23-ac125066c66c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后我们尝试借助LGBM的sklearn API构建Pipeline。Pipeline也是sklearn特有的一项功能，通过Pipeline的构建，我们可以将数据清洗、特征衍生和机器学习封装成一个评估器，进而进行快速的数据处理和预测，甚至我们还可以围绕这个pipeline进行超参数优化，进而达到自动数据清洗、自动机器学习算法超参数优化等目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e66a4-7a56-49d0-b1d1-f6680795a69e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们尝试将多项式特征衍生、标准化和LGBM三个评估器封装为一个pipeline，在后续预测时，数据会自动按照二阶多项式衍生和Z-Score标准化的流程先进行处理，然后再带入LGBM模型中进行预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a75ec745-72fc-452c-af34-20d160f2b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_pipe = make_pipeline(PolynomialFeatures(), \n",
    "                          StandardScaler(), \n",
    "                          LGBMClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3e7559a5-278d-459d-8558-daba0a34c18d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minteraction_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minclude_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Generate polynomial and interaction features.\n",
       "\n",
       "Generate a new feature matrix consisting of all polynomial combinations\n",
       "of the features with degree less than or equal to the specified degree.\n",
       "For example, if an input sample is two dimensional and of the form\n",
       "[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
       "\n",
       "Read more in the :ref:`User Guide <polynomial_features>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "degree : int or tuple (min_degree, max_degree), default=2\n",
       "    If a single int is given, it specifies the maximal degree of the\n",
       "    polynomial features. If a tuple `(min_degree, max_degree)` is passed,\n",
       "    then `min_degree` is the minimum and `max_degree` is the maximum\n",
       "    polynomial degree of the generated features. Note that `min_degree=0`\n",
       "    and `min_degree=1` are equivalent as outputting the degree zero term is\n",
       "    determined by `include_bias`.\n",
       "\n",
       "interaction_only : bool, default=False\n",
       "    If `True`, only interaction features are produced: features that are\n",
       "    products of at most `degree` *distinct* input features, i.e. terms with\n",
       "    power of 2 or higher of the same input feature are excluded:\n",
       "\n",
       "        - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc.\n",
       "        - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc.\n",
       "\n",
       "include_bias : bool, default=True\n",
       "    If `True` (default), then include a bias column, the feature in which\n",
       "    all polynomial powers are zero (i.e. a column of ones - acts as an\n",
       "    intercept term in a linear model).\n",
       "\n",
       "order : {'C', 'F'}, default='C'\n",
       "    Order of output array in the dense case. `'F'` order is faster to\n",
       "    compute, but may slow down subsequent estimators.\n",
       "\n",
       "    .. versionadded:: 0.21\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "powers_ : ndarray of shape (`n_output_features_`, `n_features_in_`)\n",
       "    `powers_[i, j]` is the exponent of the jth input in the ith output.\n",
       "\n",
       "n_input_features_ : int\n",
       "    The total number of input features.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        This attribute is deprecated in 1.0 and will be removed in 1.2.\n",
       "        Refer to `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_output_features_ : int\n",
       "    The total number of polynomial output features. The number of output\n",
       "    features is computed by iterating over all suitably sized combinations\n",
       "    of input features.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SplineTransformer : Transformer that generates univariate B-spline bases\n",
       "    for features.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Be aware that the number of features in the output array scales\n",
       "polynomially in the number of features of the input array, and\n",
       "exponentially in the degree. High degrees can cause overfitting.\n",
       "\n",
       "See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n",
       "<sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.preprocessing import PolynomialFeatures\n",
       ">>> X = np.arange(6).reshape(3, 2)\n",
       ">>> X\n",
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])\n",
       ">>> poly = PolynomialFeatures(2)\n",
       ">>> poly.fit_transform(X)\n",
       "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5., 16., 20., 25.]])\n",
       ">>> poly = PolynomialFeatures(interaction_only=True)\n",
       ">>> poly.fit_transform(X)\n",
       "array([[ 1.,  0.,  1.,  0.],\n",
       "       [ 1.,  2.,  3.,  6.],\n",
       "       [ 1.,  4.,  5., 20.]])\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PolynomialFeatures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d4129-975f-4add-ab8f-2b0b9328c762",
   "metadata": {},
   "source": [
    "然后以Pipeline作为完整的评估器，围绕数据集进行数据处理和模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "54bf371c-b031-4d19-a2f9-9d9a97a0d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('lgbmclassifier', LGBMClassifier())])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837eacfa-f89b-4a47-b33e-bfcd321c7837",
   "metadata": {},
   "source": [
    "最终输出预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a67a33a2-8138-4335-a8f6-57c53a057cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884547f-97a6-43d0-9df3-7b511b9c81ad",
   "metadata": {},
   "source": [
    "#### 2.4 Pipeline超参数优化与自动机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5594214-98fa-49a0-b110-47c20819ef4e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后，我们尝试围绕上述Pipeline进行超参数优化。在Lesson 6中我们曾介绍，对于一个Pipeline来说，其中的每个评估器中的每个超参数，都可以放在一个超参数空间中进行搜索和优化，因此这里我们可以通过网格搜索的方法，自动搜索出（某个小范围内）最佳多项式特征衍生的阶数和LGBM的最佳基础分类器个数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c75d73f-ceb1-408c-99f6-4b6698cdfde8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('polynomialfeatures', PolynomialFeatures()),\n",
       "  ('standardscaler', StandardScaler()),\n",
       "  ('lgbmclassifier', LGBMClassifier())],\n",
       " 'verbose': False,\n",
       " 'polynomialfeatures': PolynomialFeatures(),\n",
       " 'standardscaler': StandardScaler(),\n",
       " 'lgbmclassifier': LGBMClassifier(),\n",
       " 'polynomialfeatures__degree': 2,\n",
       " 'polynomialfeatures__include_bias': True,\n",
       " 'polynomialfeatures__interaction_only': False,\n",
       " 'polynomialfeatures__order': 'C',\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'lgbmclassifier__boosting_type': 'gbdt',\n",
       " 'lgbmclassifier__class_weight': None,\n",
       " 'lgbmclassifier__colsample_bytree': 1.0,\n",
       " 'lgbmclassifier__importance_type': 'split',\n",
       " 'lgbmclassifier__learning_rate': 0.1,\n",
       " 'lgbmclassifier__max_depth': -1,\n",
       " 'lgbmclassifier__min_child_samples': 20,\n",
       " 'lgbmclassifier__min_child_weight': 0.001,\n",
       " 'lgbmclassifier__min_split_gain': 0.0,\n",
       " 'lgbmclassifier__n_estimators': 100,\n",
       " 'lgbmclassifier__n_jobs': -1,\n",
       " 'lgbmclassifier__num_leaves': 31,\n",
       " 'lgbmclassifier__objective': None,\n",
       " 'lgbmclassifier__random_state': None,\n",
       " 'lgbmclassifier__reg_alpha': 0.0,\n",
       " 'lgbmclassifier__reg_lambda': 0.0,\n",
       " 'lgbmclassifier__silent': 'warn',\n",
       " 'lgbmclassifier__subsample': 1.0,\n",
       " 'lgbmclassifier__subsample_for_bin': 200000,\n",
       " 'lgbmclassifier__subsample_freq': 0}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "61bbdfbc-7ce0-4da0-b6c1-4f5d19169048",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_pipe = {'polynomialfeatures__degree': [2, 3, 4], \n",
    "              'lgbmclassifier__n_estimators': [90, 100, 110]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8d7158f0-240d-4e58-a453-bd0567563bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_pipe_search = GridSearchCV(estimator=LGBM_pipe, param_grid=param_pipe, n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "59125199-a8e2-44de-ac56-31ccab52c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('polynomialfeatures',\n",
       "                                        PolynomialFeatures()),\n",
       "                                       ('standardscaler', StandardScaler()),\n",
       "                                       ('lgbmclassifier', LGBMClassifier())]),\n",
       "             n_jobs=10,\n",
       "             param_grid={'lgbmclassifier__n_estimators': [90, 100, 110],\n",
       "                         'polynomialfeatures__degree': [2, 3, 4]})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_pipe_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4fd418b9-e176-4c5e-9877-8d2ce9ef8823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgbmclassifier__n_estimators': 90, 'polynomialfeatures__degree': 3}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_pipe_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8d863d10-41e6-400b-911a-1820dda99f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_pipe_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9badb1-b3d2-451b-b20d-ea3b4baa6fc1",
   "metadata": {},
   "source": [
    "不难看出，此过程能够一步到位实现自动数据清洗和自动模型超参数优化。不过上述流程目前为止还是过于简单，在本课程最后的案例阶段，我们会有对此更进一步探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbf287-ed0e-4f8b-8592-dabc2551f382",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，通过调用sklearn API，我们还能够对LGBM评估器进行诸如模型本地保存、构造模型融合评估器等，这些内容我们也将在后续部分内容中逐步进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ca31e-d1d1-496e-bd6b-02e59e0ac361",
   "metadata": {},
   "source": [
    "- LGBM各模型的sklearn API简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea266064-6567-43fa-8ab8-177f0443084c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，除了分类模型外，LGBM的sklearn API还包括回归模型和排序模型两种，分别可以通过如下方式进行调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "732a774f-e039-406a-ae30-19e613f6d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM回归模型的sklearn API\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2da2d842-fd01-40e6-b722-ecb9bfae0ec5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLGBMRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gbdt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'split'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM regressor.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    group : array-like\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     DaskLGBMRegressor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LGBMRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e709d42-8e39-4f19-9c4b-461d24c7fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM排序模型的sklearn API\n",
    "from lightgbm import LGBMRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1fb3d120-e502-434e-85be-0a409fd20044",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLGBMRanker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gbdt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msilent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'split'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "LightGBM ranker.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "    scikit-learn doesn't support ranking applications yet,\n",
       "    therefore this class is not really compatible with the sklearn ecosystem.\n",
       "    Please use this class mainly for training and applying ranking models in common sklearnish way.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'goss', Gradient-based One-Side Sampling.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int, optional (default=-1)\n",
       "    Number of parallel threads.\n",
       "silent : bool, optional (default=True)\n",
       "    Whether to print messages while running boosting.\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess`` or\n",
       "``objective(y_true, y_pred, group) -> grad, hess``:\n",
       "\n",
       "    y_true : array-like of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    group : array-like\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, the y_pred is group by class_id first, then group by row_id.\n",
       "If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n",
       "and you should group grad and hess in this way as well.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     DaskLGBMRanker\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LGBMRanker?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52b8b1-e3bd-471a-8781-3b4cee36efef",
   "metadata": {},
   "source": [
    "其中排序模型更多的应用于时间序列和推荐系统等领域。相关API说明可以参考[LGBM官方API说明文档](https://lightgbm.readthedocs.io/en/v3.3.5/Python-API.html)，各API的具体调用方法和超参数说明我们会在实践环节具体讨论。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
